#!/usr/bin/env python3
"""
LJWX-Stock ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
æ•´åˆæ‰€æœ‰é…ç½®é¡¹åˆ°å•ä¸€ç®¡ç†ç³»ç»Ÿä¸­
"""

import os
import json
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime
import platform
import psutil
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = logging.getLogger(__name__)

@dataclass
class TuShareConfig:
    """TuShareé…ç½®"""
    token: str = ""
    rate_limit: int = 500  # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
    batch_size: int = 100
    thread_count: int = 5
    use_pro_api: bool = True
    fallback_to_free: bool = True

@dataclass  
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    type: str = "mysql"
    host: str = "127.0.0.1"
    port: int = 3306
    user: str = "root"
    password: str = ""
    database: str = "ljwx_stock"
    charset: str = "utf8mb4"
    # SQLiteé…ç½®ï¼ˆå¤‡ç”¨ï¼‰
    sqlite_path: str = "data/ljwx_stock.db"

@dataclass
class RedisConfig:
    """Redisé…ç½®"""
    enabled: bool = False
    host: str = "localhost"
    port: int = 6379
    db: int = 0
    password: Optional[str] = None
    default_ttl: int = 3600

@dataclass
class OllamaConfig:
    """Ollama LLMé…ç½®"""
    api_url: str = "http://14.127.218.229:11434"
    model: str = "lingjingwanxiang:70b"
    timeout: int = 30
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    num_predict: int = 1024
    # OpenAIå…¼å®¹APIé…ç½®
    openai_api_key: str = "sk-fake-key"
    openai_base_url: str = "http://14.127.218.229:11434/v1"

@dataclass
class APIConfig:
    """APIæœåŠ¡é…ç½®"""
    host: str = "0.0.0.0"
    port: int = 5005
    debug: bool = False
    cors_enabled: bool = True
    cors_origins: List[str] = field(default_factory=lambda: ["*"])
    max_connections: int = 100

@dataclass
class WebSocketConfig:
    """WebSocketé…ç½®"""
    host: str = "0.0.0.0"
    port: int = 8765
    realtime_update_interval: int = 10
    max_connections: int = 100
    ping_timeout: int = 30
    ping_interval: int = 10

@dataclass
class LoggingConfig:
    """æ—¥å¿—é…ç½®"""
    level: str = "INFO"
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_rotation: bool = True
    max_log_files: int = 10
    log_dir: str = "logs"

@dataclass
class SchedulerConfig:
    """è°ƒåº¦å™¨é…ç½®"""
    enabled: bool = True
    timezone: str = "Asia/Shanghai"
    daily_update_enabled: bool = True
    daily_update_times: List[str] = field(default_factory=lambda: ["09:35", "15:05", "21:00"])
    weekly_sync_enabled: bool = True
    weekly_sync_day: int = 6  # æ˜ŸæœŸå…­
    weekly_sync_time: str = "20:00"
    market_hours: Dict[str, str] = field(default_factory=lambda: {
        "morning_start": "09:30",
        "morning_end": "11:30", 
        "afternoon_start": "13:00",
        "afternoon_end": "15:00"
    })

@dataclass
class TrainingConfig:
    """æ¨¡å‹è®­ç»ƒé…ç½®"""
    base_model: str = "ljwx-stock"
    training_data_dir: str = "data/llm_training"
    models_dir: str = "data/models"
    max_training_examples_per_batch: int = 200
    stock_batch_size: int = 30
    days_back: int = 90
    incremental_days: int = 30
    max_models_to_keep: int = 5
    min_success_rate: float = 0.8
    daily_training_time: str = "02:00"
    weekly_training_days: List[str] = field(default_factory=lambda: ["monday", "friday"])

@dataclass
class HardwareConfig:
    """ç¡¬ä»¶ä¼˜åŒ–é…ç½®"""
    cpu_cores: int = 0
    memory_gb: float = 0.0
    is_m2_ultra: bool = False
    max_workers: int = 4
    chunk_size: int = 10000
    memory_limit_gb: float = 16.0
    enable_gpu_acceleration: bool = False
    gpu_device: str = "cpu"

@dataclass
class SecurityConfig:
    """å®‰å…¨é…ç½®"""
    secret_key: str = "ljwx-stock-unified-2025"
    jwt_secret_key: str = ""
    jwt_expiration_hours: int = 24
    api_rate_limit: int = 1000  # æ¯å°æ—¶è¯·æ±‚é™åˆ¶
    enable_api_key_auth: bool = False
    allowed_ips: List[str] = field(default_factory=list)

@dataclass  
class SystemConfig:
    """ç³»ç»Ÿé…ç½®"""
    max_concurrent_tasks: int = 5
    data_retention_days: int = 90
    cache_enabled: bool = True
    cache_size_gb: float = 4.0
    monitoring_enabled: bool = True
    max_history: int = 1000
    backup_enabled: bool = True
    backup_dir: str = "backups"

class ConfigManager:
    """ç»Ÿä¸€é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: Optional[str] = None):
        self.config_file = config_file or "unified_config.json"
        self.config_dir = Path("config")
        self.config_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ‰€æœ‰é…ç½®
        self.tushare = TuShareConfig()
        self.database = DatabaseConfig()
        self.redis = RedisConfig()
        self.ollama = OllamaConfig()
        self.api = APIConfig()
        self.websocket = WebSocketConfig()
        self.logging = LoggingConfig()
        self.scheduler = SchedulerConfig()
        self.training = TrainingConfig()
        self.hardware = HardwareConfig()
        self.security = SecurityConfig()
        self.system = SystemConfig()
        
        # åŠ è½½é…ç½®
        self._load_configurations()
        
        # æ£€æµ‹å¹¶è®¾ç½®ç¡¬ä»¶é…ç½®
        self._detect_hardware()
        
        logger.info("âœ… ç»Ÿä¸€é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_configurations(self):
        """åŠ è½½æ‰€æœ‰é…ç½®"""
        # 1. ä»ç¯å¢ƒå˜é‡åŠ è½½
        self._load_from_env()
        
        # 2. ä»JSONé…ç½®æ–‡ä»¶åŠ è½½
        self._load_from_json_files()
        
        # 3. ä»ç»Ÿä¸€é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._load_from_unified_config()
        
        # 4. éªŒè¯å¿…éœ€é…ç½®
        self._validate_config()
    
    def _load_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        # TuShareé…ç½®
        self.tushare.token = os.getenv('TS_TOKEN', self.tushare.token)
        
        # æ•°æ®åº“é…ç½®
        self.database.host = os.getenv('DB_HOST', self.database.host)
        self.database.port = int(os.getenv('DB_PORT', self.database.port))
        self.database.user = os.getenv('DB_USER', self.database.user)
        self.database.password = os.getenv('DB_PASSWORD', self.database.password)
        self.database.database = os.getenv('DB_NAME', self.database.database)
        
        # Redisé…ç½®
        self.redis.host = os.getenv('REDIS_HOST', self.redis.host)
        self.redis.port = int(os.getenv('REDIS_PORT', self.redis.port))
        
        # Ollamaé…ç½®
        self.ollama.api_url = os.getenv('LLM_API_URL', self.ollama.api_url)
        self.ollama.model = os.getenv('LLM_MODEL', self.ollama.model)
        self.ollama.openai_api_key = os.getenv('OPENAI_API_KEY', self.ollama.openai_api_key)
        self.ollama.openai_base_url = os.getenv('OPENAI_BASE_URL', self.ollama.openai_base_url)
        
        # APIé…ç½®
        self.api.host = os.getenv('API_HOST', self.api.host)
        self.api.port = int(os.getenv('API_PORT', self.api.port))
        self.api.debug = os.getenv('API_DEBUG', 'False').lower() == 'true'
        
        # WebSocketé…ç½®
        self.websocket.host = os.getenv('WS_HOST', self.websocket.host)
        self.websocket.port = int(os.getenv('WS_PORT', self.websocket.port))
        self.websocket.realtime_update_interval = int(os.getenv('REALTIME_UPDATE_INTERVAL', self.websocket.realtime_update_interval))
        
        # æ—¥å¿—é…ç½®
        self.logging.level = os.getenv('LOG_LEVEL', self.logging.level)
    
    def _load_from_json_files(self):
        """ä»ç°æœ‰JSONé…ç½®æ–‡ä»¶åŠ è½½"""
        try:
            # åŠ è½½æ•°æ®åº“é…ç½®
            db_config_file = Path("database_config.json")
            if db_config_file.exists():
                with open(db_config_file, 'r', encoding='utf-8') as f:
                    db_config = json.load(f)
                    if 'database' in db_config:
                        db = db_config['database']
                        self.database.type = db.get('type', self.database.type)
                        self.database.host = db.get('host', self.database.host)
                        self.database.port = db.get('port', self.database.port)
                        self.database.user = db.get('user', self.database.user)
                        self.database.password = db.get('password', self.database.password)
                        self.database.database = db.get('database', self.database.database)
            
            # åŠ è½½è°ƒåº¦å™¨é…ç½®
            scheduler_config_file = Path("config/scheduler_config.json")
            if scheduler_config_file.exists():
                with open(scheduler_config_file, 'r', encoding='utf-8') as f:
                    scheduler_config = json.load(f)
                    if 'daily_update' in scheduler_config:
                        daily = scheduler_config['daily_update']
                        self.scheduler.daily_update_enabled = daily.get('enabled', True)
                        self.scheduler.daily_update_times = daily.get('times', self.scheduler.daily_update_times)
                    
                    if 'weekly_full_sync' in scheduler_config:
                        weekly = scheduler_config['weekly_full_sync']
                        self.scheduler.weekly_sync_enabled = weekly.get('enabled', True)
                        self.scheduler.weekly_sync_day = weekly.get('day', self.scheduler.weekly_sync_day)
                        self.scheduler.weekly_sync_time = weekly.get('time', self.scheduler.weekly_sync_time)
                    
                    if 'market_hours' in scheduler_config:
                        self.scheduler.market_hours = scheduler_config['market_hours']
            
            # åŠ è½½è®­ç»ƒé…ç½®
            training_config_file = Path("training_config.json")
            if training_config_file.exists():
                with open(training_config_file, 'r', encoding='utf-8') as f:
                    training_config = json.load(f)
                    if 'continuous_training' in training_config:
                        ct = training_config['continuous_training']
                        self.training.base_model = ct.get('base_model', self.training.base_model)
                        self.training.training_data_dir = ct.get('training_data_dir', self.training.training_data_dir)
                        self.training.models_dir = ct.get('models_dir', self.training.models_dir)
                        self.training.max_training_examples_per_batch = ct.get('max_training_examples_per_batch', self.training.max_training_examples_per_batch)
                        self.training.stock_batch_size = ct.get('stock_batch_size', self.training.stock_batch_size)
                        
                    if 'model_settings' in training_config:
                        ms = training_config['model_settings']
                        self.ollama.temperature = ms.get('temperature', self.ollama.temperature)
                        self.ollama.top_p = ms.get('top_p', self.ollama.top_p)
                        self.ollama.top_k = ms.get('top_k', self.ollama.top_k)
                        self.ollama.repeat_penalty = ms.get('repeat_penalty', self.ollama.repeat_penalty)
                        self.ollama.num_predict = ms.get('num_predict', self.ollama.num_predict)
                        
        except Exception as e:
            logger.warning(f"åŠ è½½JSONé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def _load_from_unified_config(self):
        """ä»ç»Ÿä¸€é…ç½®æ–‡ä»¶åŠ è½½"""
        unified_config_path = Path(self.config_file)
        if unified_config_path.exists():
            try:
                with open(unified_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    self._update_from_dict(config)
                logger.info(f"âœ… ä»ç»Ÿä¸€é…ç½®æ–‡ä»¶åŠ è½½: {self.config_file}")
            except Exception as e:
                logger.error(f"åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_from_dict(self, config: Dict[str, Any]):
        """ä»å­—å…¸æ›´æ–°é…ç½®"""
        for section_name, section_config in config.items():
            if hasattr(self, section_name) and isinstance(section_config, dict):
                section = getattr(self, section_name)
                for key, value in section_config.items():
                    if hasattr(section, key):
                        setattr(section, key, value)
    
    def _detect_hardware(self):
        """æ£€æµ‹ç¡¬ä»¶é…ç½®"""
        try:
            # CPUä¿¡æ¯
            self.hardware.cpu_cores = os.cpu_count() or 4
            
            # å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            self.hardware.memory_gb = memory.total / (1024**3)
            
            # æ£€æµ‹M2 Ultra
            if platform.system() == "Darwin":
                try:
                    import subprocess
                    result = subprocess.run(
                        ["system_profiler", "SPHardwareDataType"], 
                        capture_output=True, text=True
                    )
                    output = result.stdout
                    self.hardware.is_m2_ultra = "M2 Ultra" in output and "192 GB" in output
                except Exception:
                    pass
            
            # è®¾ç½®ä¼˜åŒ–å‚æ•°
            if self.hardware.is_m2_ultra:
                self.hardware.max_workers = min(20, self.hardware.cpu_cores - 4)
                self.hardware.chunk_size = 50000
                self.hardware.memory_limit_gb = min(64, self.hardware.memory_gb * 0.4)
                self.hardware.enable_gpu_acceleration = True
                self.hardware.gpu_device = "mps"
            else:
                self.hardware.max_workers = min(8, self.hardware.cpu_cores - 2)
                self.hardware.chunk_size = 10000
                self.hardware.memory_limit_gb = min(16, self.hardware.memory_gb * 0.5)
            
            # æ£€æµ‹GPUæ”¯æŒ
            try:
                import torch
                if torch.backends.mps.is_available():
                    self.hardware.enable_gpu_acceleration = True
                    self.hardware.gpu_device = "mps"
            except ImportError:
                pass
                
        except Exception as e:
            logger.warning(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
    
    def _validate_config(self):
        """éªŒè¯å¿…éœ€é…ç½®"""
        errors = []
        
        # æ£€æŸ¥TuShare token
        if not self.tushare.token or self.tushare.token == "your_tushare_token":
            errors.append("TuShare tokenæœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
        
        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        if not self.database.password:
            logger.warning("æ•°æ®åº“å¯†ç ä¸ºç©º")
        
        # æ£€æŸ¥å¿…éœ€ç›®å½•
        required_dirs = [
            self.logging.log_dir,
            self.training.training_data_dir,
            self.training.models_dir,
            "data",
            "temp",
            "cache"
        ]
        
        for dir_path in required_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        if errors:
            logger.warning(f"é…ç½®éªŒè¯è­¦å‘Š: {', '.join(errors)}")
    
    def save_unified_config(self):
        """ä¿å­˜ç»Ÿä¸€é…ç½®åˆ°æ–‡ä»¶"""
        config_data = {}
        
        for attr_name in dir(self):
            if not attr_name.startswith('_') and not callable(getattr(self, attr_name)):
                attr_value = getattr(self, attr_name)
                if hasattr(attr_value, '__dict__'):
                    config_data[attr_name] = attr_value.__dict__
        
        unified_config_path = Path(self.config_file)
        try:
            with open(unified_config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"âœ… ç»Ÿä¸€é…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»Ÿä¸€é…ç½®å¤±è´¥: {e}")
    
    def get_database_url(self) -> str:
        """è·å–æ•°æ®åº“è¿æ¥URL"""
        if self.database.type == "mysql":
            return f"mysql+pymysql://{self.database.user}:{self.database.password}@{self.database.host}:{self.database.port}/{self.database.database}?charset={self.database.charset}"
        elif self.database.type == "sqlite":
            return f"sqlite:///{self.database.sqlite_path}"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {self.database.type}")
    
    def get_redis_url(self) -> str:
        """è·å–Redisè¿æ¥URL"""
        if self.redis.password:
            return f"redis://:{self.redis.password}@{self.redis.host}:{self.redis.port}/{self.redis.db}"
        else:
            return f"redis://{self.redis.host}:{self.redis.port}/{self.redis.db}"
    
    def get_ollama_config(self) -> Dict[str, Any]:
        """è·å–Ollamaé…ç½®å­—å…¸"""
        return {
            'api_url': self.ollama.api_url,
            'model': self.ollama.model,
            'timeout': self.ollama.timeout,
            'temperature': self.ollama.temperature,
            'top_p': self.ollama.top_p,
            'top_k': self.ollama.top_k,
            'repeat_penalty': self.ollama.repeat_penalty,
            'num_predict': self.ollama.num_predict,
            'openai_api_key': self.ollama.openai_api_key,
            'openai_base_url': self.ollama.openai_base_url
        }
    
    def get_hardware_config(self, task_type: str = "general") -> Dict[str, Any]:
        """è·å–ç‰¹å®šä»»åŠ¡çš„ç¡¬ä»¶é…ç½®"""
        base_config = {
            'cpu_cores': self.hardware.cpu_cores,
            'memory_gb': self.hardware.memory_gb,
            'is_m2_ultra': self.hardware.is_m2_ultra,
            'max_workers': self.hardware.max_workers,
            'chunk_size': self.hardware.chunk_size,
            'memory_limit_gb': self.hardware.memory_limit_gb,
            'enable_gpu_acceleration': self.hardware.enable_gpu_acceleration,
            'gpu_device': self.hardware.gpu_device
        }
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´é…ç½®
        if task_type == "data_processing" and self.hardware.is_m2_ultra:
            base_config.update({
                'max_workers': min(20, self.hardware.cpu_cores - 4),
                'chunk_size': 50000,
                'memory_limit_gb': min(64, self.hardware.memory_gb * 0.4)
            })
        elif task_type == "ml_training" and self.hardware.is_m2_ultra:
            base_config.update({
                'max_workers': min(16, self.hardware.cpu_cores - 8),
                'memory_limit_gb': min(96, self.hardware.memory_gb * 0.5)
            })
        
        return base_config
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ”§ LJWX-Stock ç»Ÿä¸€é…ç½®æ‘˜è¦")
        print("="*80)
        
        print(f"ğŸ“Š TuShare: {'âœ… å·²é…ç½®' if self.tushare.token else 'âŒ æœªé…ç½®'}")
        print(f"ğŸ—„ï¸ æ•°æ®åº“: {self.database.type}://{self.database.host}:{self.database.port}/{self.database.database}")
        print(f"ğŸ§  Ollama: {self.ollama.model} @ {self.ollama.api_url}")
        print(f"ğŸŒ APIæœåŠ¡: {self.api.host}:{self.api.port}")
        print(f"ğŸ”Œ WebSocket: {self.websocket.host}:{self.websocket.port}")
        print(f"ğŸ–¥ï¸ ç¡¬ä»¶: {'M2 Ultra' if self.hardware.is_m2_ultra else 'Standard'} - {self.hardware.cpu_cores} cores, {self.hardware.memory_gb:.1f}GB")
        print(f"ğŸš€ GPUåŠ é€Ÿ: {'âœ… MPS' if self.hardware.enable_gpu_acceleration else 'âŒ CPU only'}")
        print(f"ğŸ“ æ—¥å¿—çº§åˆ«: {self.logging.level}")
        print(f"â° è°ƒåº¦å™¨: {'âœ… å¯ç”¨' if self.scheduler.enabled else 'âŒ ç¦ç”¨'}")
        print(f"ğŸ”’ å®‰å…¨: {'âœ… å¯ç”¨' if self.security.enable_api_key_auth else 'âŒ ç¦ç”¨'}")
        
        print("="*80)

# å…¨å±€é…ç½®å®ä¾‹
config = ConfigManager()

def get_config() -> ConfigManager:
    """è·å–å…¨å±€é…ç½®å®ä¾‹"""
    return config

def reload_config():
    """é‡æ–°åŠ è½½é…ç½®"""
    global config
    config = ConfigManager()
    return config

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç®¡ç†å™¨
    config_mgr = ConfigManager()
    config_mgr.print_config_summary()
    
    # ä¿å­˜ç»Ÿä¸€é…ç½®
    config_mgr.save_unified_config()
    
    print(f"\nâœ… é…ç½®ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")
    print(f"ğŸ“ ç»Ÿä¸€é…ç½®å·²ä¿å­˜åˆ°: {config_mgr.config_file}")