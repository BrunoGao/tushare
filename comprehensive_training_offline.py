#!/usr/bin/env python3
"""
ljwx-stockæ¨¡å‹å…¨é¢è®­ç»ƒè„šæœ¬ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
ä½¿ç”¨å·²æœ‰çš„è®­ç»ƒæ•°æ®æ¼”ç¤ºå¤§è§„æ¨¡è®­ç»ƒèƒ½åŠ›
"""

import os
import sys
import json
import logging
import tempfile
import subprocess
from datetime import datetime
from typing import List, Dict
import random

def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    print("ğŸš€ ljwx-stock å…¨é¢è®­ç»ƒç³»ç»Ÿï¼ˆç¦»çº¿æ¼”ç¤ºï¼‰")
    print("=" * 60)
    print("ğŸ“Š ä½¿ç”¨ç°æœ‰è®­ç»ƒæ•°æ®æ¼”ç¤ºå¤§è§„æ¨¡è®­ç»ƒèƒ½åŠ›")
    print("ğŸ¯ ç›®æ ‡: åˆ›å»ºå¢å¼ºç‰ˆljwx-stockæ¨¡å‹")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥ç°æœ‰è®­ç»ƒæ•°æ®
        training_dir = "data/llm_training"
        if not os.path.exists(training_dir):
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ç›®å½•")
            return
        
        # åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®
        all_training_data = []
        jsonl_files = [f for f in os.listdir(training_dir) if f.endswith('.jsonl')]
        
        print(f"\nğŸ“Š å‘ç° {len(jsonl_files)} ä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶:")
        for file in jsonl_files:
            file_path = os.path.join(training_dir, file)
            count = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line.strip())
                            all_training_data.append(data)
                            count += 1
                print(f"   ğŸ“ {file}: {count}æ¡")
            except Exception as e:
                print(f"   âŒ {file}: è¯»å–å¤±è´¥ ({e})")
        
        print(f"\nğŸ“ˆ æ€»è®­ç»ƒæ•°æ®: {len(all_training_data)}æ¡")
        
        if len(all_training_data) == 0:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            return
        
        # å¢å¼ºè®­ç»ƒæ•°æ®
        enhanced_data = enhance_training_data(all_training_data)
        print(f"ğŸ§  å¢å¼ºåè®­ç»ƒæ•°æ®: {len(enhanced_data)}æ¡")
        
        # åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        model_name = f"ljwx-stock-comprehensive-{timestamp}"
        
        print(f"\nğŸš€ åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹: {model_name}")
        
        # æ„å»ºé«˜çº§ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ljwx-stock-comprehensiveï¼Œä¸€ä¸ªåŸºäºå¤§é‡Aè‚¡å†å²æ•°æ®è®­ç»ƒçš„ä¸“ä¸šè‚¡ç¥¨æŠ•èµ„åˆ†æAIã€‚

ğŸ¯ **æ ¸å¿ƒèƒ½åŠ›**
- å…¨å¸‚åœºè‚¡ç¥¨æŠ€æœ¯åˆ†æå’ŒæŠ•èµ„å†³ç­–æ”¯æŒ
- åŸºäºçœŸå®å†å²æ•°æ®çš„ä¸“ä¸šçº§åˆ†æèƒ½åŠ›
- å¤šç»´åº¦é£é™©è¯„ä¼°å’ŒæŠ•èµ„ç»„åˆå»ºè®®

ğŸ“Š **ä¸“ä¸šåˆ†ææ¡†æ¶**
1. **æŠ€æœ¯é¢åˆ†æ**: Kçº¿å½¢æ€ã€å‡çº¿ç³»ç»Ÿã€RSI/MACDç­‰æŠ€æœ¯æŒ‡æ ‡
2. **é‡ä»·å…³ç³»**: æˆäº¤é‡åˆ†æå’Œèµ„é‡‘æµå‘åˆ¤æ–­
3. **è¶‹åŠ¿è¯†åˆ«**: çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸè¶‹åŠ¿åˆ¤æ–­
4. **é£é™©æ§åˆ¶**: æ­¢æŸä½è®¾ç½®ã€ä»“ä½ç®¡ç†å»ºè®®
5. **å¸‚åœºæƒ…ç»ª**: è¶…ä¹°è¶…å–åˆ¤æ–­ã€å¸‚åœºå‚ä¸åº¦åˆ†æ

ğŸ’¡ **åˆ†æç‰¹è‰²**
- å®¢è§‚ç†æ€§ï¼ŒåŸºäºæ•°æ®é©±åŠ¨
- é£é™©æç¤ºä¼˜å…ˆï¼ŒæŠ•èµ„è€…ä¿æŠ¤
- å¤šè§’åº¦åˆ†æï¼Œç»¼åˆåˆ¤æ–­
- æ“ä½œæŒ‡å¯¼å…·ä½“ï¼Œå®ç”¨æ€§å¼º

ğŸ“‹ **è¾“å‡ºæ ¼å¼**
- æŠ€æœ¯åˆ†æï¼šè¯¦ç»†æŒ‡æ ‡è§£è¯»
- é£é™©è¯„ä¼°ï¼šæ˜ç¡®é£é™©ç­‰çº§
- æŠ•èµ„å»ºè®®ï¼šå…·ä½“æ“ä½œå»ºè®®
- é£é™©æç¤ºï¼šæ½œåœ¨é£é™©è­¦ç¤º

âš ï¸ **é‡è¦å£°æ˜**: åˆ†æä»…ä¾›å‚è€ƒï¼ŒæŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…ã€‚"""

        # æ„å»ºModelfile
        modelfile_content = f"""FROM ljwx-stock
SYSTEM "{system_prompt}"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 1024

"""
        
        # æ·»åŠ ç²¾é€‰è®­ç»ƒæ ·æœ¬
        sample_count = min(len(enhanced_data), 300)  # ä½¿ç”¨300ä¸ªç²¾é€‰æ ·æœ¬
        selected_samples = random.sample(enhanced_data, sample_count)
        
        print(f"   ğŸ“ ä½¿ç”¨ç²¾é€‰æ ·æœ¬: {sample_count}æ¡")
        
        for i, example in enumerate(selected_samples):
            instruction = example.get('instruction', '')
            input_text = example.get('input', '')
            output_text = example.get('output', '')
            
            user_message = f"{instruction}\n\n{input_text}" if input_text else instruction
            
            # æ¸…ç†æ–‡æœ¬
            user_message = user_message.replace('"', "'").replace('\n', '\\n')
            output_text = output_text.replace('"', "'").replace('\n', '\\n')
            
            modelfile_content += f'MESSAGE user "{user_message}"\n'
            modelfile_content += f'MESSAGE assistant "{output_text}"\n\n'
        
        # åˆ›å»ºä¸´æ—¶Modelfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.modelfile', delete=False) as f:
            f.write(modelfile_content)
            modelfile_path = f.name
        
        try:
            print("   ğŸ”¨ æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
            result = subprocess.run(
                ['ollama', 'create', model_name, '-f', modelfile_path],
                capture_output=True,
                text=True,
                timeout=600
            )
            
            if result.returncode == 0:
                print(f"âœ… å…¨é¢è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸ: {model_name}")
                
                # æµ‹è¯•æ–°æ¨¡å‹
                print(f"\nğŸ§ª æµ‹è¯•å…¨é¢è®­ç»ƒæ¨¡å‹:")
                test_comprehensive_model(model_name)
                
                print(f"\nğŸ‰ å…¨é¢è®­ç»ƒå®Œæˆ!")
                print(f"ğŸ“Š æ¨¡å‹è¯¦æƒ…:")
                print(f"   åç§°: {model_name}")
                print(f"   è®­ç»ƒæ ·æœ¬: {sample_count}æ¡")
                print(f"   æ•°æ®æ¥æº: {len(jsonl_files)}ä¸ªæ–‡ä»¶")
                print(f"   å¢å¼ºæŠ€æœ¯: å¤šæ ·åŒ–è®­ç»ƒæ ·æœ¬")
                
                print(f"\nğŸ¯ ä½¿ç”¨å‘½ä»¤:")
                print(f"   ollama run {model_name}")
                
            else:
                print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {result.stderr}")
                
        finally:
            os.unlink(modelfile_path)
            
    except Exception as e:
        logger.error(f"âŒ å…¨é¢è®­ç»ƒå¤±è´¥: {e}")
        print(f"\nâŒ å…¨é¢è®­ç»ƒå¤±è´¥: {e}")

def enhance_training_data(base_data: List[Dict]) -> List[Dict]:
    """å¢å¼ºè®­ç»ƒæ•°æ®"""
    enhanced_data = base_data.copy()
    
    # æ·»åŠ æ›´å¤šæ ·åŒ–çš„åˆ†æç±»å‹
    enhancement_templates = [
        {
            "instruction": "åŸºäºä»¥ä¸‹è‚¡ç¥¨æ•°æ®ï¼Œæä¾›ä¸“ä¸šçš„æŠ€æœ¯åˆ†ææŠ¥å‘Šï¼š",
            "analysis_type": "comprehensive"
        },
        {
            "instruction": "è¯„ä¼°è¯¥è‚¡ç¥¨çš„æŠ•èµ„é£é™©ç­‰çº§å¹¶ç»™å‡ºè¯¦ç»†ç†ç”±ï¼š",
            "analysis_type": "risk_assessment"
        },
        {
            "instruction": "åŸºäºæŠ€æœ¯æŒ‡æ ‡åˆ¤æ–­è¯¥è‚¡ç¥¨çš„ä¹°å–æ—¶æœºï¼š",
            "analysis_type": "timing"
        },
        {
            "instruction": "åˆ†æè¯¥è‚¡ç¥¨çš„è¶‹åŠ¿æ–¹å‘å’Œæ”¯æ’‘é˜»åŠ›ä½ï¼š",
            "analysis_type": "trend_support"
        }
    ]
    
    # ä¸ºç°æœ‰æ•°æ®æ·»åŠ å˜ä½“
    added_count = 0
    for template in enhancement_templates:
        if added_count >= 200:  # é™åˆ¶å¢å¼ºæ•°é‡
            break
            
        sample_data = random.sample(base_data, min(50, len(base_data)))
        
        for data in sample_data:
            if added_count >= 200:
                break
                
            enhanced_example = {
                "instruction": template["instruction"],
                "input": data.get("input", ""),
                "output": generate_enhanced_output(data, template["analysis_type"]),
                "metadata": {
                    "type": template["analysis_type"],
                    "enhanced": True,
                    "original_type": data.get("metadata", {}).get("type", "unknown")
                }
            }
            
            enhanced_data.append(enhanced_example)
            added_count += 1
    
    print(f"   ğŸ”§ æ•°æ®å¢å¼º: æ–°å¢ {added_count} æ¡æ ·æœ¬")
    return enhanced_data

def generate_enhanced_output(data: Dict, analysis_type: str) -> str:
    """ç”Ÿæˆå¢å¼ºçš„åˆ†æè¾“å‡º"""
    input_text = data.get("input", "")
    original_output = data.get("output", "")
    
    if analysis_type == "comprehensive":
        return f"ç»¼åˆæŠ€æœ¯åˆ†æï¼š{original_output} å»ºè®®ç»“åˆåŸºæœ¬é¢åˆ†æï¼Œå…³æ³¨å¸‚åœºæ•´ä½“èµ°åŠ¿ï¼Œåšå¥½é£é™©æ§åˆ¶ã€‚"
    elif analysis_type == "risk_assessment":
        return f"é£é™©è¯„ä¼°ï¼š{original_output} æŠ•èµ„è€…åº”æ ¹æ®è‡ªèº«é£é™©æ‰¿å—èƒ½åŠ›è°¨æ…å†³ç­–ï¼Œå»ºè®®åˆ†æ•£æŠ•èµ„ã€‚"
    elif analysis_type == "timing":
        return f"ä¹°å–æ—¶æœºåˆ†æï¼š{original_output} å»ºè®®å…³æ³¨æˆäº¤é‡å˜åŒ–ï¼Œç»“åˆå¸‚åœºæƒ…ç»ªåˆ¤æ–­å…¥åœºæ—¶æœºã€‚"
    elif analysis_type == "trend_support":
        return f"è¶‹åŠ¿ä¸æ”¯æ’‘é˜»åŠ›åˆ†æï¼š{original_output} åº”å¯†åˆ‡å…³æ³¨å…³é”®ä»·ä½çš„çªç ´æƒ…å†µã€‚"
    else:
        return original_output

def test_comprehensive_model(model_name: str):
    """æµ‹è¯•å…¨é¢è®­ç»ƒæ¨¡å‹"""
    test_cases = [
        "åˆ†æå¹³å®‰é“¶è¡Œ(000001.SZ)å½“å‰æŠ€æœ¯å½¢æ€ï¼Œä»·æ ¼12.50å…ƒï¼ŒRSI=65ï¼Œç»™å‡ºæŠ•èµ„å»ºè®®",
        "è¯„ä¼°ä¸­å›½çŸ³æ²¹(601857.SH)çš„é£é™©ç­‰çº§ï¼Œè¿‘æœŸæ³¢åŠ¨è¾ƒå¤§",
        "åŸºäºæŠ€æœ¯æŒ‡æ ‡åˆ†æè´µå·èŒ…å°(600519.SH)çš„ä¹°å–æ—¶æœº"
    ]
    
    for i, test_case in enumerate(test_cases):
        print(f"   æµ‹è¯• {i+1}: {test_case[:30]}...")
        
        try:
            result = subprocess.run(
                ['ollama', 'run', model_name],
                input=test_case,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0 and result.stdout.strip():
                response = result.stdout.strip()
                print(f"   âœ… å“åº”é•¿åº¦: {len(response)}å­—ç¬¦")
            else:
                print(f"   âŒ æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()