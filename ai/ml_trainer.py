"""
AIè‚¡ç¥¨åˆ†ææ¨¡å‹è®­ç»ƒå™¨
ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒè‚¡ç¥¨é¢„æµ‹æ¨¡å‹
"""
import pandas as pd
import numpy as np
import logging
import joblib
import json
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
import sys, os

# å¯é€‰ä¾èµ– - åŠ¨æ€å¯¼å…¥ä»¥é¿å…æ¶æ„é—®é¢˜
XGBOOST_AVAILABLE = False
LIGHTGBM_AVAILABLE = False

def check_xgboost():
    global XGBOOST_AVAILABLE
    try:
        import xgboost as xgb
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•å®ä¾‹æ¥æµ‹è¯•æ˜¯å¦çœŸçš„å¯ç”¨
        test = xgb.XGBClassifier(n_estimators=1)
        XGBOOST_AVAILABLE = True
        return xgb
    except Exception as e:
        print(f"âš ï¸ XGBoostä¸å¯ç”¨: {e}")
        return None

def check_lightgbm():
    global LIGHTGBM_AVAILABLE
    try:
        import lightgbm as lgb
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•å®ä¾‹æ¥æµ‹è¯•æ˜¯å¦çœŸçš„å¯ç”¨
        test = lgb.LGBMClassifier(n_estimators=1, verbose=-1)
        LIGHTGBM_AVAILABLE = True
        return lgb
    except Exception as e:
        print(f"âš ï¸ LightGBMä¸å¯ç”¨: {e}")
        return None

# ç§»é™¤ TA-Lib ä¾èµ–æ£€æŸ¥ï¼Œå·²åœ¨ utils/technical_indicators.py ä¸­å¤„ç†
print("âœ… ä½¿ç”¨å†…ç½®æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å™¨")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.db_manager import DatabaseManager
from llm.feature_engineering import feature_engineer

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockMLTrainer:
    """è‚¡ç¥¨æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.feature_engineer = feature_engineer
        self.models = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_names = []
        self.model_performance = {}
        
        # æ”¯æŒçš„æ¨¡å‹ - æ ¹æ®å¯ç”¨ä¾èµ–åŠ¨æ€æ„å»º
        self.available_models = {
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),
            'svm': SVC(random_state=42, probability=True),
            'neural_network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)
        }
        
        # åªæœ‰åœ¨XGBoostå¯ç”¨æ—¶æ‰æ·»åŠ 
        xgb = check_xgboost()
        if xgb is not None:
            self.available_models['xgboost'] = xgb.XGBClassifier(n_estimators=100, random_state=42)
        
        # åªæœ‰åœ¨LightGBMå¯ç”¨æ—¶æ‰æ·»åŠ   
        lgb = check_lightgbm()
        if lgb is not None:
            self.available_models['lightgbm'] = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
        
        logger.info(f"âœ… MLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ¨¡å‹: {list(self.available_models.keys())}")
    
    def prepare_training_data(self, days_back: int = 180, stock_limit: int = 500) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info(f"ğŸ”„ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®: {days_back}å¤©, {stock_limit}åªè‚¡ç¥¨")
        
        # è·å–è®­ç»ƒè‚¡ç¥¨åˆ—è¡¨
        stock_list = self.feature_engineer.get_stock_list_for_training(stock_limit)
        
        if not stock_list:
            raise ValueError("æ— æ³•è·å–è®­ç»ƒè‚¡ç¥¨åˆ—è¡¨")
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
        
        # é¢„å¤„ç†æ•°æ®
        training_data, selected_features = self.feature_engineer.preprocess_training_data(
            stock_list, start_date, end_date
        )
        
        if training_data.empty:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©º")
        
        self.feature_names = selected_features
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = training_data[selected_features].fillna(0)
        y = training_data['price_direction_5d'].fillna(0)
        
        # ç§»é™¤ç¼ºå¤±ç›®æ ‡å˜é‡çš„è¡Œ
        valid_mask = ~y.isnull()
        X = X[valid_mask]
        y = y[valid_mask]
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.scalers['feature_scaler'] = StandardScaler()
        X_scaled = self.scalers['feature_scaler'].fit_transform(X)
        
        # ç¼–ç ç›®æ ‡å˜é‡ (-1, 0, 1) -> (0, 1, 2)
        self.label_encoders['target'] = LabelEncoder()
        y_encoded = self.label_encoders['target'].fit_transform(y)
        
        logger.info(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {X_scaled.shape[0]}æ¡è®°å½•, {X_scaled.shape[1]}ä¸ªç‰¹å¾")
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {np.bincount(y_encoded)}")
        
        return training_data, X_scaled, y_encoded
    
    def train_single_model(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray, 
                          X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        if model_name not in self.available_models:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        logger.info(f"ğŸ”§ å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}")
        
        model = self.available_models[model_name]
        
        # è®­ç»ƒæ¨¡å‹
        start_time = datetime.now()
        model.fit(X_train, y_train)
        training_time = (datetime.now() - start_time).total_seconds()
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # è¯„ä¼°æ€§èƒ½
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        performance = {
            'model_name': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'training_time': training_time,
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'feature_count': X_train.shape[1]
        }
        
        # ä¿å­˜æ¨¡å‹
        self.models[model_name] = model
        self.model_performance[model_name] = performance
        
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_name}, å‡†ç¡®ç‡: {accuracy:.4f}")
        
        return performance
    
    def train_all_models(self, X: np.ndarray, y: np.ndarray, test_size: float = 0.2) -> Dict[str, Dict[str, Any]]:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        all_performance = {}
        
        for model_name in self.available_models.keys():
            try:
                performance = self.train_single_model(
                    model_name, X_train, y_train, X_test, y_test
                )
                all_performance[model_name] = performance
            except Exception as e:
                logger.error(f"è®­ç»ƒæ¨¡å‹{model_name}å¤±è´¥: {e}")
                continue
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        if all_performance:
            best_model = max(all_performance.keys(), key=lambda x: all_performance[x]['accuracy'])
            logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model}, å‡†ç¡®ç‡: {all_performance[best_model]['accuracy']:.4f}")
        
        return all_performance
    
    def hyperparameter_tuning(self, model_name: str, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """è¶…å‚æ•°è°ƒä¼˜"""
        if model_name not in self.available_models:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        logger.info(f"ğŸ” å¼€å§‹è¶…å‚æ•°è°ƒä¼˜: {model_name}")
        
        # å®šä¹‰å‚æ•°ç½‘æ ¼ - æ ¹æ®å¯ç”¨æ¨¡å‹åŠ¨æ€æ„å»º
        param_grids = {
            'random_forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            },
            'gradient_boosting': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            },
            'logistic_regression': {
                'C': [0.1, 1, 10],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            }
        }
        
        # åªæœ‰åœ¨XGBoostå¯ç”¨æ—¶æ‰æ·»åŠ å‚æ•°ç½‘æ ¼
        if XGBOOST_AVAILABLE:
            param_grids['xgboost'] = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            }
        
        # åªæœ‰åœ¨LightGBMå¯ç”¨æ—¶æ‰æ·»åŠ å‚æ•°ç½‘æ ¼
        if LIGHTGBM_AVAILABLE:
            param_grids['lightgbm'] = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2], 
                'max_depth': [3, 5, 7],
                'num_leaves': [31, 50, 100]
            }
        
        if model_name not in param_grids:
            logger.warning(f"æ¨¡å‹{model_name}æ²¡æœ‰å®šä¹‰å‚æ•°ç½‘æ ¼ï¼Œè·³è¿‡è°ƒä¼˜")
            return {}
        
        # ç½‘æ ¼æœç´¢
        model = self.available_models[model_name]
        grid_search = GridSearchCV(
            model, param_grids[model_name], 
            cv=5, scoring='accuracy', n_jobs=-1
        )
        
        grid_search.fit(X, y)
        
        # æ›´æ–°æ¨¡å‹
        self.models[model_name] = grid_search.best_estimator_
        
        result = {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }
        
        logger.info(f"âœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ: {model_name}, æœ€ä½³å¾—åˆ†: {result['best_score']:.4f}")
        
        return result
    
    def get_feature_importance(self, model_name: str) -> List[Dict[str, Any]]:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if model_name not in self.models:
            return []
        
        model = self.models[model_name]
        
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_[0])
        else:
            return []
        
        feature_importance = []
        for i, importance in enumerate(importances):
            if i < len(self.feature_names):
                feature_importance.append({
                    'feature': self.feature_names[i],
                    'importance': float(importance)
                })
        
        # æŒ‰é‡è¦æ€§æ’åº
        feature_importance.sort(key=lambda x: x['importance'], reverse=True)
        
        return feature_importance
    
    def predict_single_stock(self, ts_code: str, model_name: str = None) -> Dict[str, Any]:
        """é¢„æµ‹å•åªè‚¡ç¥¨"""
        if not self.models:
            raise ValueError("æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹")
        
        # é€‰æ‹©æ¨¡å‹
        if model_name is None:
            model_name = max(self.model_performance.keys(), 
                           key=lambda x: self.model_performance[x]['accuracy'])
        
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹{model_name}ä¸å­˜åœ¨")
        
        # è·å–è‚¡ç¥¨æ•°æ®
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=90)).strftime('%Y%m%d')
        
        df = self.feature_engineer.fetch_raw_stock_data(ts_code, start_date, end_date)
        if df.empty:
            raise ValueError(f"æ— æ³•è·å–è‚¡ç¥¨{ts_code}çš„æ•°æ®")
        
        # ç‰¹å¾å·¥ç¨‹
        df = self.feature_engineer.calculate_technical_features(df)
        df = self.feature_engineer.calculate_fundamental_features(df)
        df = self.feature_engineer.calculate_market_features(df)
        
        # è·å–æœ€æ–°æ•°æ®
        latest_data = df.iloc[-1:][self.feature_names].fillna(0)
        
        # æ ‡å‡†åŒ–
        if 'feature_scaler' in self.scalers:
            X_scaled = self.scalers['feature_scaler'].transform(latest_data)
        else:
            X_scaled = latest_data.values
        
        # é¢„æµ‹
        model = self.models[model_name]
        prediction = model.predict(X_scaled)[0]
        probability = model.predict_proba(X_scaled)[0] if hasattr(model, 'predict_proba') else None
        
        # è§£ç é¢„æµ‹ç»“æœ
        if 'target' in self.label_encoders:
            prediction_label = self.label_encoders['target'].inverse_transform([prediction])[0]
        else:
            prediction_label = prediction
        
        result = {
            'ts_code': ts_code,
            'model_name': model_name,
            'prediction': int(prediction_label),
            'prediction_text': self._prediction_to_text(prediction_label),
            'probability': probability.tolist() if probability is not None else None,
            'confidence': float(np.max(probability)) if probability is not None else 0.5,
            'timestamp': datetime.now().isoformat()
        }
        
        return result
    
    def _prediction_to_text(self, prediction: int) -> str:
        """å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºæ–‡æœ¬"""
        mapping = {
            -1: "å–å‡º",
            0: "æŒæœ‰", 
            1: "ä¹°å…¥"
        }
        return mapping.get(prediction, "æœªçŸ¥")
    
    def save_models(self, model_dir: str = "models") -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            os.makedirs(model_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            for model_name, model in self.models.items():
                model_path = os.path.join(model_dir, f"{model_name}.joblib")
                joblib.dump(model, model_path)
            
            # ä¿å­˜é¢„å¤„ç†å™¨
            preprocessor_path = os.path.join(model_dir, "preprocessors.joblib")
            preprocessors = {
                'scalers': self.scalers,
                'label_encoders': self.label_encoders,
                'feature_names': self.feature_names
            }
            joblib.dump(preprocessors, preprocessor_path)
            
            # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            performance_path = os.path.join(model_dir, "performance.json")
            with open(performance_path, 'w', encoding='utf-8') as f:
                json.dump(self.model_performance, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_models(self, model_dir: str = "models") -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            if not os.path.exists(model_dir):
                logger.warning(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
                return False
            
            # åŠ è½½é¢„å¤„ç†å™¨
            preprocessor_path = os.path.join(model_dir, "preprocessors.joblib")
            if os.path.exists(preprocessor_path):
                preprocessors = joblib.load(preprocessor_path)
                self.scalers = preprocessors.get('scalers', {})
                self.label_encoders = preprocessors.get('label_encoders', {})
                self.feature_names = preprocessors.get('feature_names', [])
            
            # åŠ è½½æ¨¡å‹
            for model_name in self.available_models.keys():
                model_path = os.path.join(model_dir, f"{model_name}.joblib")
                if os.path.exists(model_path):
                    self.models[model_name] = joblib.load(model_path)
            
            # åŠ è½½æ€§èƒ½æŒ‡æ ‡
            performance_path = os.path.join(model_dir, "performance.json")
            if os.path.exists(performance_path):
                with open(performance_path, 'r', encoding='utf-8') as f:
                    self.model_performance = json.load(f)
            
            logger.info(f"âœ… ä»{model_dir}åŠ è½½äº†{len(self.models)}ä¸ªæ¨¡å‹")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

# å…¨å±€å®ä¾‹
ml_trainer = StockMLTrainer()

def main():
    """ä¸»å‡½æ•° - ç”¨äºè®­ç»ƒå’Œæµ‹è¯•"""
    try:
        logger.info("ğŸš€ å¼€å§‹AIè‚¡ç¥¨åˆ†ææ¨¡å‹è®­ç»ƒ...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data, X, y = ml_trainer.prepare_training_data(days_back=180, stock_limit=100)
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        performance = ml_trainer.train_all_models(X, y)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print("=" * 80)
        for model_name, perf in performance.items():
            print(f"{model_name:20} | å‡†ç¡®ç‡: {perf['accuracy']:.4f} | F1: {perf['f1_score']:.4f} | CV: {perf['cv_mean']:.4f}Â±{perf['cv_std']:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        success = ml_trainer.save_models()
        if success:
            print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜!")
        
        # æµ‹è¯•é¢„æµ‹
        test_stock = "000001.SZ"
        try:
            prediction = ml_trainer.predict_single_stock(test_stock)
            print(f"\nğŸ¯ æµ‹è¯•é¢„æµ‹ {test_stock}:")
            print(f"é¢„æµ‹ç»“æœ: {prediction['prediction_text']}")
            print(f"ç½®ä¿¡åº¦: {prediction['confidence']:.4f}")
        except Exception as e:
            print(f"æµ‹è¯•é¢„æµ‹å¤±è´¥: {e}")
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºè¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()