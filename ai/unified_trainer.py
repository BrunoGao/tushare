"""
ç»¼åˆæ¨¡å‹è®­ç»ƒæ¡†æ¶
æ”¯æŒä¼ ç»Ÿæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œå¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒæ¥å£
"""
import pandas as pd
import numpy as np
import logging
import joblib
import json
import os
import pickle
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# å¯é€‰ä¾èµ–æ£€æŸ¥
TORCH_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False
XGBOOST_AVAILABLE = False
LIGHTGBM_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    pass

try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    pass

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    xgb = None
except Exception as e:
    XGBOOST_AVAILABLE = False
    xgb = None
    logging.warning(f"XGBooståŠ è½½å¤±è´¥: {e}")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    lgb = None
except Exception as e:
    LIGHTGBM_AVAILABLE = False
    lgb = None
    logging.warning(f"LightGBMåŠ è½½å¤±è´¥: {e}")

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.db_manager import DatabaseManager
from llm.enhanced_feature_engineering import enhanced_feature_engineer
from ai.ml_trainer import ml_trainer
from utils.m2_ultra_config import m2_ultra_config, get_optimal_config
from utils.performance_monitor import performance_monitor

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# PyTorchç›¸å…³ç±»ï¼ˆä»…åœ¨PyTorchå¯ç”¨æ—¶å®šä¹‰ï¼‰
if TORCH_AVAILABLE:
    class StockDataset(Dataset):
        """PyTorchæ•°æ®é›†"""
        def __init__(self, features: np.ndarray, targets: np.ndarray, sequence_features: np.ndarray = None):
            self.features = torch.FloatTensor(features)
            self.targets = torch.LongTensor(targets)
            self.sequence_features = torch.FloatTensor(sequence_features) if sequence_features is not None else None
        
        def __len__(self):
            return len(self.features)
        
        def __getitem__(self, idx):
            if self.sequence_features is not None:
                return {
                    'tabular': self.features[idx],
                    'sequence': self.sequence_features[idx],
                    'target': self.targets[idx]
                }
            else:
                return {
                    'tabular': self.features[idx],
                    'target': self.targets[idx]
                }

    class LSTMModel(nn.Module):
        """LSTMæ¨¡å‹"""
        def __init__(self, sequence_length: int, feature_dim: int, hidden_dim: int = 64, num_layers: int = 2, num_classes: int = 3):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(feature_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
            self.classifier = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim // 2, num_classes)
            )
        
        def forward(self, x):
            lstm_out, (hidden, _) = self.lstm(x)
            # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            output = self.classifier(hidden[-1])
            return output

    class MultiModalModel(nn.Module):
        """å¤šæ¨¡æ€èåˆæ¨¡å‹"""
        def __init__(self, tabular_dim: int, sequence_length: int, sequence_feature_dim: int, 
                     hidden_dim: int = 128, num_classes: int = 3):
            super(MultiModalModel, self).__init__()
            
            # è¡¨æ ¼ç‰¹å¾å¤„ç†
            self.tabular_net = nn.Sequential(
                nn.Linear(tabular_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, hidden_dim // 2)
            )
            
            # æ—¶åºç‰¹å¾å¤„ç†
            self.lstm = nn.LSTM(sequence_feature_dim, hidden_dim // 2, 2, batch_first=True, dropout=0.2)
            
            # èåˆå±‚
            self.fusion = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, num_classes)
            )
        
        def forward(self, tabular, sequence):
            # å¤„ç†è¡¨æ ¼ç‰¹å¾
            tabular_features = self.tabular_net(tabular)
            
            # å¤„ç†æ—¶åºç‰¹å¾
            lstm_out, (hidden, _) = self.lstm(sequence)
            sequence_features = hidden[-1]
            
            # èåˆç‰¹å¾
            combined = torch.cat([tabular_features, sequence_features], dim=1)
            output = self.fusion(combined)
            
            return output
else:
    # å½“PyTorchä¸å¯ç”¨æ—¶çš„å ä½ç±»
    class StockDataset:
        def __init__(self, *args, **kwargs):
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨StockDataset")
    
    class LSTMModel:
        def __init__(self, *args, **kwargs):
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨LSTMModel")
    
    class MultiModalModel:
        def __init__(self, *args, **kwargs):
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨MultiModalModel")

class UnifiedModelTrainer:
    """ç»Ÿä¸€æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.feature_engineer = enhanced_feature_engineer
        self.ml_trainer = ml_trainer
        
        # M2 Ultraä¼˜åŒ–é…ç½®
        self.m2_config = m2_ultra_config
        self.is_m2_ultra = self.m2_config.is_m2_ultra
        
        # æ¨¡å‹å­˜å‚¨
        self.models = {}
        self.model_configs = {}
        self.performance_history = {}
        
        # æ ¹æ®ç¡¬ä»¶é…ç½®è®¾å¤‡
        if TORCH_AVAILABLE:
            pytorch_config = self.m2_config.get_pytorch_config()
            device_type = pytorch_config.get('device', 'cpu')
            
            if device_type == 'mps' and torch.backends.mps.is_available():
                self.device = torch.device('mps')
                # é…ç½®MPSä¼˜åŒ– (å®‰å…¨çš„APIè°ƒç”¨)
                try:
                    if hasattr(torch.backends.mps, 'use_sync_allocator'):
                        torch.backends.mps.use_sync_allocator(pytorch_config.get('mps_sync_allocator', True))
                    if hasattr(torch.backends.mps, 'enable_metal_sync') and pytorch_config.get('mps_metal_sync', True):
                        torch.backends.mps.enable_metal_sync(True)
                except Exception as e:
                    logger.warning(f"MPSé…ç½®è­¦å‘Š: {e}")
                logger.info("âœ… ä½¿ç”¨Apple Silicon GPU (MPS)")
            else:
                self.device = torch.device('cpu')
                logger.info("ä½¿ç”¨CPUè®¡ç®—")
        else:
            self.device = None
            
        self.model_save_path = "models"
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # æ‰“å°ç³»ç»Ÿä¿¡æ¯
        if self.is_m2_ultra:
            logger.info("ğŸš€ M2 Ultraä¼˜åŒ–æ¨¡å¼å·²å¯ç”¨")
            self.m2_config.print_system_info()
        
        logger.info(f"ç»Ÿä¸€è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device if self.device else 'CPU only'}")
    
    def prepare_training_data(self, stock_limit: int = 500, days_back: int = 180, 
                             include_sequences: bool = True, include_text: bool = False) -> Dict[str, Any]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info(f"å‡†å¤‡è®­ç»ƒæ•°æ®: {stock_limit}åªè‚¡ç¥¨, {days_back}å¤©")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = self.feature_engineer.base_engineer.get_stock_list_for_training(stock_limit)
        if not stock_list:
            raise ValueError("æ— æ³•è·å–è®­ç»ƒè‚¡ç¥¨åˆ—è¡¨")
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
        
        if include_sequences or include_text:
            # ä½¿ç”¨å¢å¼ºç‰¹å¾å·¥ç¨‹
            multimodal_data = self.feature_engineer.prepare_multimodal_features(
                stock_list, start_date, end_date
            )
            
            if not multimodal_data or multimodal_data['tabular_features'].empty:
                raise ValueError("å¤šæ¨¡æ€æ•°æ®å‡†å¤‡å¤±è´¥")
            
            return multimodal_data
        else:
            # ä½¿ç”¨ä¼ ç»Ÿç‰¹å¾å·¥ç¨‹
            training_data, X, y = self.ml_trainer.prepare_training_data(days_back, stock_limit)
            return {
                'tabular_features': training_data,
                'X': X,
                'y': y,
                'feature_names': self.ml_trainer.feature_names
            }
    
    def train_traditional_models(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        performance_monitor.start_monitoring()
        
        try:
            # è·å–M2 Ultraä¼˜åŒ–é…ç½®
            ml_config = get_optimal_config('traditional_ml')
            logger.info(f"ä½¿ç”¨M2 Ultraä¼˜åŒ–é…ç½®: {ml_config}")
            
            if 'X' in data and 'y' in data:
                # ä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
                X, y = data['X'], data['y']
            else:
                # ä»è¡¨æ ¼ç‰¹å¾å‡†å¤‡æ•°æ®
                tabular_features = data['tabular_features']
                target_col = 'price_direction_5d'
                
                if target_col not in tabular_features.columns:
                    target_col = 'composite_direction'
                
                if target_col not in tabular_features.columns:
                    raise ValueError("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç›®æ ‡å˜é‡")
                
                # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
                feature_cols = [col for col in tabular_features.columns 
                               if not col.startswith('future_') and 
                               col not in ['price_direction_1d', 'price_direction_5d', 'price_direction_10d', 'composite_direction']]
                
                X = tabular_features[feature_cols].fillna(0).values
                y = tabular_features[target_col].fillna(0).values
                
                # ç¼–ç ç›®æ ‡å˜é‡
                le = LabelEncoder()
                y = le.fit_transform(y)
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                scaler = StandardScaler()
                X = scaler.fit_transform(X)
                
                # ä¿å­˜é¢„å¤„ç†å™¨
                self.models['scaler'] = scaler
                self.models['label_encoder'] = le
                self.models['feature_names'] = feature_cols
            
            # ä½¿ç”¨M2 Ultraä¼˜åŒ–çš„é…ç½®è®­ç»ƒæ¨¡å‹
            self._configure_ml_trainer_for_m2(ml_config)
            performance = self.ml_trainer.train_all_models(X, y)
            
            # ä¿å­˜æ¨¡å‹
            for model_name, model in self.ml_trainer.models.items():
                self.models[f'traditional_{model_name}'] = model
            
            self.performance_history['traditional'] = performance
            
            logger.info(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå®Œæˆ: {len(performance)}ä¸ªæ¨¡å‹")
            return performance
            
        finally:
            performance_monitor.stop_monitoring()
    
    def _configure_ml_trainer_for_m2(self, ml_config: Dict[str, Any]):
        """ä¸ºM2 Ultraé…ç½®MLè®­ç»ƒå™¨"""
        try:
            # é…ç½®XGBoost
            if XGBOOST_AVAILABLE:
                xgb_config = self.m2_config.get_xgboost_config()
                # è¿™é‡Œå¯ä»¥æ›´æ–°ml_trainerçš„XGBoosté…ç½®
                logger.info(f"XGBoosté…ç½®: {xgb_config}")
            
            # é…ç½®LightGBM
            if LIGHTGBM_AVAILABLE:
                lgb_config = self.m2_config.get_lightgbm_config()
                # è¿™é‡Œå¯ä»¥æ›´æ–°ml_trainerçš„LightGBMé…ç½®
                logger.info(f"LightGBMé…ç½®: {lgb_config}")
            
            # é…ç½®sklearnå¹¶è¡Œåº¦
            sklearn_jobs = ml_config.get('sklearn_jobs', -1)
            logger.info(f"scikit-learnå¹¶è¡Œåº¦: {sklearn_jobs}")
            
        except Exception as e:
            logger.error(f"é…ç½®MLè®­ç»ƒå™¨å¤±è´¥: {e}")
    
    def train_deep_learning_models(self, data: Dict[str, Any], epochs: int = 50, batch_size: int = 64) -> Dict[str, Any]:
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        if not TORCH_AVAILABLE:
            logger.warning("PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
            return {}
        
        logger.info("å¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        tabular_features = data['tabular_features']
        target_col = 'price_direction_5d'
        
        if target_col not in tabular_features.columns:
            target_col = 'composite_direction'
        
        if target_col not in tabular_features.columns:
            logger.error("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç›®æ ‡å˜é‡")
            return {}
        
        # å‡†å¤‡ç‰¹å¾
        feature_cols = [col for col in tabular_features.columns 
                       if not col.startswith('future_') and 
                       col not in ['price_direction_1d', 'price_direction_5d', 'price_direction_10d', 'composite_direction']]
        
        X_tabular = tabular_features[feature_cols].fillna(0).values
        y = tabular_features[target_col].fillna(0).values
        
        # ç¼–ç ç›®æ ‡å˜é‡
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_tabular_scaled = scaler.fit_transform(X_tabular)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_tabular_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        performance = {}
        
        # è®­ç»ƒç®€å•çš„å…¨è¿æ¥ç½‘ç»œ
        if self._train_mlp_model(X_train, X_test, y_train, y_test, epochs, batch_size):
            performance['mlp'] = self._evaluate_pytorch_model('mlp', X_test, y_test)
        
        # å¦‚æœæœ‰æ—¶åºæ•°æ®ï¼Œè®­ç»ƒLSTM
        if 'sequence_features' in data and data['sequence_features'].size > 0:
            if self._train_lstm_model(data['sequence_features'], y_encoded, epochs, batch_size):
                # é‡æ–°åˆ†å‰²ä»¥åŒ¹é…æ—¶åºæ•°æ®
                seq_len = data['sequence_features'].shape[0]
                y_seq = y_encoded[-seq_len:]  # åŒ¹é…åºåˆ—é•¿åº¦
                X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(
                    data['sequence_features'], y_seq, test_size=0.2, random_state=42
                )
                performance['lstm'] = self._evaluate_pytorch_model('lstm', X_seq_test, y_seq_test)
            
            # è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹
            if self._train_multimodal_model(X_tabular_scaled, data['sequence_features'], y_encoded, epochs, batch_size):
                performance['multimodal'] = self._evaluate_multimodal_model(data, y_encoded)
        
        self.performance_history['deep_learning'] = performance
        
        logger.info(f"æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ: {len(performance)}ä¸ªæ¨¡å‹")
        return performance
    
    def _train_mlp_model(self, X_train: np.ndarray, X_test: np.ndarray, 
                        y_train: np.ndarray, y_test: np.ndarray, 
                        epochs: int, batch_size: int) -> bool:
        """è®­ç»ƒMLPæ¨¡å‹"""
        try:
            input_dim = X_train.shape[1]
            num_classes = len(np.unique(y_train))
            
            model = nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(64, num_classes)
            ).to(self.device)
            
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # è®­ç»ƒ
            dataset = StockDataset(X_train, y_train)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            model.train()
            for epoch in range(epochs):
                total_loss = 0
                for batch in dataloader:
                    optimizer.zero_grad()
                    outputs = model(batch['tabular'].to(self.device))
                    loss = criterion(outputs, batch['target'].to(self.device))
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                
                if (epoch + 1) % 10 == 0:
                    logger.info(f"MLP Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")
            
            self.models['mlp'] = model
            return True
            
        except Exception as e:
            logger.error(f"MLPæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _train_lstm_model(self, sequence_data: np.ndarray, y: np.ndarray, 
                         epochs: int, batch_size: int) -> bool:
        """è®­ç»ƒLSTMæ¨¡å‹"""
        try:
            seq_len, feature_dim = sequence_data.shape[1], sequence_data.shape[2]
            num_classes = len(np.unique(y))
            
            # åŒ¹é…åºåˆ—æ•°æ®é•¿åº¦
            y_seq = y[-len(sequence_data):]
            
            model = LSTMModel(seq_len, feature_dim, num_classes=num_classes).to(self.device)
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                sequence_data, y_seq, test_size=0.2, random_state=42
            )
            
            # è®­ç»ƒ
            dataset = StockDataset(np.zeros((len(X_train), 1)), y_train, X_train)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            model.train()
            for epoch in range(epochs):
                total_loss = 0
                for batch in dataloader:
                    optimizer.zero_grad()
                    outputs = model(batch['sequence'].to(self.device))
                    loss = criterion(outputs, batch['target'].to(self.device))
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                
                if (epoch + 1) % 10 == 0:
                    logger.info(f"LSTM Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")
            
            self.models['lstm'] = model
            return True
            
        except Exception as e:
            logger.error(f"LSTMæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _train_multimodal_model(self, tabular_data: np.ndarray, sequence_data: np.ndarray, 
                               y: np.ndarray, epochs: int, batch_size: int) -> bool:
        """è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹"""
        try:
            # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
            min_len = min(len(tabular_data), len(sequence_data))
            X_tabular = tabular_data[-min_len:]
            X_sequence = sequence_data[-min_len:]
            y_matched = y[-min_len:]
            
            tabular_dim = X_tabular.shape[1]
            seq_len, seq_feature_dim = X_sequence.shape[1], X_sequence.shape[2]
            num_classes = len(np.unique(y_matched))
            
            model = MultiModalModel(tabular_dim, seq_len, seq_feature_dim, num_classes=num_classes).to(self.device)
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # æ•°æ®åˆ†å‰²
            indices = np.arange(len(X_tabular))
            train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
            
            X_tab_train, X_seq_train, y_train = X_tabular[train_idx], X_sequence[train_idx], y_matched[train_idx]
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = StockDataset(X_tab_train, y_train, X_seq_train)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            model.train()
            for epoch in range(epochs):
                total_loss = 0
                for batch in dataloader:
                    optimizer.zero_grad()
                    outputs = model(batch['tabular'].to(self.device), batch['sequence'].to(self.device))
                    loss = criterion(outputs, batch['target'].to(self.device))
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                
                if (epoch + 1) % 10 == 0:
                    logger.info(f"MultiModal Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")
            
            self.models['multimodal'] = model
            return True
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _evaluate_pytorch_model(self, model_name: str, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°PyTorchæ¨¡å‹"""
        try:
            model = self.models[model_name]
            model.eval()
            
            with torch.no_grad():
                if model_name == 'mlp':
                    X_tensor = torch.FloatTensor(X_test).to(self.device)
                    outputs = model(X_tensor)
                elif model_name == 'lstm':
                    X_tensor = torch.FloatTensor(X_test).to(self.device)
                    outputs = model(X_tensor)
                else:
                    return {}
                
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
            
            accuracy = accuracy_score(y_test, predictions)
            precision = precision_score(y_test, predictions, average='weighted')
            recall = recall_score(y_test, predictions, average='weighted')
            f1 = f1_score(y_test, predictions, average='weighted')
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
            
        except Exception as e:
            logger.error(f"è¯„ä¼°{model_name}æ¨¡å‹å¤±è´¥: {e}")
            return {}
    
    def _evaluate_multimodal_model(self, data: Dict[str, Any], y: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹"""
        try:
            model = self.models['multimodal']
            model.eval()
            
            tabular_features = data['tabular_features']
            sequence_features = data['sequence_features']
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            feature_cols = [col for col in tabular_features.columns 
                           if not col.startswith('future_') and 
                           col not in ['price_direction_1d', 'price_direction_5d', 'price_direction_10d', 'composite_direction']]
            
            X_tabular = tabular_features[feature_cols].fillna(0).values
            
            # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
            min_len = min(len(X_tabular), len(sequence_features))
            X_tabular = X_tabular[-min_len:]
            X_sequence = sequence_features[-min_len:]
            y_matched = y[-min_len:]
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_tabular_scaled = scaler.fit_transform(X_tabular)
            
            # æ•°æ®åˆ†å‰²
            indices = np.arange(len(X_tabular_scaled))
            _, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
            
            X_tab_test = X_tabular_scaled[test_idx]
            X_seq_test = X_sequence[test_idx]
            y_test = y_matched[test_idx]
            
            with torch.no_grad():
                X_tab_tensor = torch.FloatTensor(X_tab_test).to(self.device)
                X_seq_tensor = torch.FloatTensor(X_seq_test).to(self.device)
                outputs = model(X_tab_tensor, X_seq_tensor)
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
            
            accuracy = accuracy_score(y_test, predictions)
            precision = precision_score(y_test, predictions, average='weighted')
            recall = recall_score(y_test, predictions, average='weighted')
            f1 = f1_score(y_test, predictions, average='weighted')
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
            
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹å¤±è´¥: {e}")
            return {}
    
    def train_all_models(self, stock_limit: int = 500, days_back: int = 180, 
                        train_traditional: bool = True, train_deep: bool = True,
                        epochs: int = 50) -> Dict[str, Any]:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_training_data(stock_limit, days_back, 
                                         include_sequences=train_deep, 
                                         include_text=False)
        
        all_performance = {}
        
        # è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        if train_traditional:
            try:
                traditional_perf = self.train_traditional_models(data)
                all_performance.update(traditional_perf)
            except Exception as e:
                logger.error(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        if train_deep and TORCH_AVAILABLE:
            try:
                deep_perf = self.train_deep_learning_models(data, epochs)
                all_performance.update(deep_perf)
            except Exception as e:
                logger.error(f"æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        # ä¿å­˜æ‰€æœ‰æ¨¡å‹
        self.save_all_models()
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        if all_performance:
            best_model = max(all_performance.keys(), key=lambda x: all_performance[x].get('accuracy', 0))
            logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model}, å‡†ç¡®ç‡: {all_performance[best_model].get('accuracy', 0):.4f}")
        
        return all_performance
    
    def save_all_models(self) -> bool:
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        try:
            # ä¿å­˜ä¼ ç»Ÿæ¨¡å‹
            for name, model in self.models.items():
                if name.startswith('traditional_'):
                    model_path = os.path.join(self.model_save_path, f"{name}.joblib")
                    joblib.dump(model, model_path)
                elif name in ['scaler', 'label_encoder', 'feature_names']:
                    model_path = os.path.join(self.model_save_path, f"{name}.joblib")
                    joblib.dump(model, model_path)
                elif TORCH_AVAILABLE and isinstance(model, nn.Module):
                    model_path = os.path.join(self.model_save_path, f"{name}.pth")
                    torch.save(model.state_dict(), model_path)
            
            # ä¿å­˜æ€§èƒ½å†å²
            performance_path = os.path.join(self.model_save_path, "performance_history.json")
            with open(performance_path, 'w', encoding='utf-8') as f:
                json.dump(self.performance_history, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_save_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def predict_stock(self, ts_code: str, model_name: str = None) -> Dict[str, Any]:
        """é¢„æµ‹å•åªè‚¡ç¥¨"""
        try:
            if not self.models:
                raise ValueError("æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            
            # é€‰æ‹©æœ€ä½³æ¨¡å‹
            if model_name is None:
                if self.performance_history:
                    all_models = {}
                    for category, models in self.performance_history.items():
                        all_models.update(models)
                    model_name = max(all_models.keys(), key=lambda x: all_models[x].get('accuracy', 0))
                else:
                    model_name = list(self.models.keys())[0]
            
            # è·å–è‚¡ç¥¨æ•°æ®å¹¶è¿›è¡Œé¢„æµ‹
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=90)).strftime('%Y%m%d')
            
            df = self.feature_engineer.base_engineer.fetch_raw_stock_data(ts_code, start_date, end_date)
            if df.empty:
                raise ValueError(f"æ— æ³•è·å–è‚¡ç¥¨{ts_code}çš„æ•°æ®")
            
            # ç‰¹å¾å·¥ç¨‹
            df = self.feature_engineer.base_engineer.calculate_technical_features(df)
            df = self.feature_engineer.base_engineer.calculate_fundamental_features(df)
            df = self.feature_engineer.extract_statistical_features(df)
            df = self.feature_engineer.create_momentum_features(df)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹
            if model_name.startswith('traditional_'):
                return self._predict_traditional(df, model_name, ts_code)
            elif model_name in ['mlp', 'lstm', 'multimodal'] and TORCH_AVAILABLE:
                return self._predict_deep_learning(df, model_name, ts_code)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}")
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _predict_traditional(self, df: pd.DataFrame, model_name: str, ts_code: str) -> Dict[str, Any]:
        """ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹"""
        feature_names = self.models.get('feature_names', [])
        if not feature_names:
            raise ValueError("ç¼ºå°‘ç‰¹å¾åç§°")
        
        # å‡†å¤‡ç‰¹å¾
        latest_data = df.iloc[-1:][feature_names].fillna(0)
        
        # æ ‡å‡†åŒ–
        if 'scaler' in self.models:
            X_scaled = self.models['scaler'].transform(latest_data)
        else:
            X_scaled = latest_data.values
        
        # é¢„æµ‹
        model = self.models[model_name]
        prediction = model.predict(X_scaled)[0]
        probability = model.predict_proba(X_scaled)[0] if hasattr(model, 'predict_proba') else None
        
        # è§£ç 
        if 'label_encoder' in self.models:
            prediction_label = self.models['label_encoder'].inverse_transform([prediction])[0]
        else:
            prediction_label = prediction
        
        return {
            'ts_code': ts_code,
            'model_name': model_name,
            'prediction': int(prediction_label),
            'prediction_text': self._prediction_to_text(prediction_label),
            'probability': probability.tolist() if probability is not None else None,
            'confidence': float(np.max(probability)) if probability is not None else 0.5,
            'timestamp': datetime.now().isoformat()
        }
    
    def _predict_deep_learning(self, df: pd.DataFrame, model_name: str, ts_code: str) -> Dict[str, Any]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹"""
        model = self.models[model_name]
        model.eval()
        
        with torch.no_grad():
            if model_name == 'mlp':
                # å‡†å¤‡è¡¨æ ¼ç‰¹å¾
                feature_cols = [col for col in df.columns if col in self.models.get('feature_names', [])]
                X = df.iloc[-1:][feature_cols].fillna(0).values
                
                # æ ‡å‡†åŒ–
                if 'scaler' in self.models:
                    X = self.models['scaler'].transform(X)
                
                X_tensor = torch.FloatTensor(X).to(self.device)
                outputs = model(X_tensor)
                
            elif model_name == 'lstm':
                # å‡†å¤‡åºåˆ—ç‰¹å¾
                sequence_data = self.feature_engineer.create_sequence_features(df)
                if sequence_data.size == 0:
                    raise ValueError("æ— æ³•åˆ›å»ºåºåˆ—ç‰¹å¾")
                
                X_tensor = torch.FloatTensor(sequence_data[-1:]).to(self.device)
                outputs = model(X_tensor)
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹: {model_name}")
            
            probabilities = torch.softmax(outputs, dim=1)
            prediction = torch.argmax(outputs, dim=1).cpu().item()
            confidence = torch.max(probabilities).cpu().item()
        
        # è§£ç é¢„æµ‹ç»“æœ
        if 'label_encoder' in self.models:
            prediction_label = self.models['label_encoder'].inverse_transform([prediction])[0]
        else:
            prediction_label = prediction - 1  # å‡è®¾ 0,1,2 å¯¹åº” -1,0,1
        
        return {
            'ts_code': ts_code,
            'model_name': model_name,
            'prediction': int(prediction_label),
            'prediction_text': self._prediction_to_text(prediction_label),
            'probability': probabilities.cpu().numpy().tolist()[0],
            'confidence': float(confidence),
            'timestamp': datetime.now().isoformat()
        }
    
    def _prediction_to_text(self, prediction: int) -> str:
        """é¢„æµ‹ç»“æœè½¬æ–‡æœ¬"""
        mapping = {-1: "å–å‡º", 0: "æŒæœ‰", 1: "ä¹°å…¥"}
        return mapping.get(prediction, "æœªçŸ¥")

# å…¨å±€å®ä¾‹
unified_trainer = UnifiedModelTrainer()

def main():
    """ä¸»å‡½æ•° - è®­ç»ƒå’Œæµ‹è¯•æ‰€æœ‰æ¨¡å‹"""
    try:
        logger.info("ğŸš€ å¼€å§‹ç»¼åˆæ¨¡å‹è®­ç»ƒç³»ç»Ÿæµ‹è¯•...")
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        performance = unified_trainer.train_all_models(
            stock_limit=50,  # æµ‹è¯•ç”¨å°‘é‡è‚¡ç¥¨
            days_back=120,
            train_traditional=True,
            train_deep=TORCH_AVAILABLE,
            epochs=20  # æµ‹è¯•ç”¨å°‘é‡epoch
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š ç»¼åˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print("=" * 80)
        for model_name, perf in performance.items():
            if isinstance(perf, dict) and 'accuracy' in perf:
                print(f"{model_name:20} | å‡†ç¡®ç‡: {perf['accuracy']:.4f} | F1: {perf.get('f1_score', 0):.4f}")
        
        # æµ‹è¯•é¢„æµ‹
        test_stock = "000001.SZ"
        try:
            prediction = unified_trainer.predict_stock(test_stock)
            print(f"\nğŸ¯ æµ‹è¯•é¢„æµ‹ {test_stock}:")
            print(f"æ¨¡å‹: {prediction.get('model_name', 'N/A')}")
            print(f"é¢„æµ‹: {prediction.get('prediction_text', 'N/A')}")
            print(f"ç½®ä¿¡åº¦: {prediction.get('confidence', 0):.4f}")
        except Exception as e:
            print(f"æµ‹è¯•é¢„æµ‹å¤±è´¥: {e}")
        
        print("\nâœ… ç»¼åˆæ¨¡å‹è®­ç»ƒç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºè¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()