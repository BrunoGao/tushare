#!/usr/bin/env python3
"""
ljwx-stockæ¨¡å‹å…¨é¢è®­ç»ƒè„šæœ¬
ä½¿ç”¨è¿‡å»ä¸€å¹´çš„å®é™…å†å²æ•°æ®å’Œæ‰€æœ‰è‚¡ç¥¨ç”Ÿæˆå¤§è§„æ¨¡è®­ç»ƒé›†
"""

import os
import sys
import json
import logging
import tempfile
import subprocess
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd
import time
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm.tushare_data_extractor import TuShareDataExtractor
from llm.llm_training_data_generator import LLMTrainingDataGenerator

class ComprehensiveTrainer:
    """å…¨é¢è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, tushare_token: str = None):
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_extractor = TuShareDataExtractor(tushare_token)
        self.data_generator = LLMTrainingDataGenerator()
        
        # é…ç½®
        self.config = {
            'base_model': 'ljwx-stock',
            'training_data_dir': 'data/llm_training',
            'models_dir': 'data/models',
            'cache_dir': 'data/cache',  # æ•°æ®ç¼“å­˜ç›®å½•
            'max_training_examples': 50000,  # å¤§å¹…å¢åŠ åˆ°50000æ¡è®­ç»ƒæ ·æœ¬
            'batch_size': 100,   # æ¯æ‰¹å¤„ç†100åªè‚¡ç¥¨ï¼ˆæé«˜æ•ˆç‡ï¼‰
            'days_back': 1825,   # è¿‡å»5å¹´æ•°æ®ï¼ˆ5*365=1825å¤©ï¼‰
            'request_delay': 0.3,  # å‡å°‘APIè¯·æ±‚å»¶è¿Ÿ
            'max_retries': 5,   # å¢åŠ æœ€å¤§é‡è¯•æ¬¡æ•°
            'test_mode': False,  # å…³é—­æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰è‚¡ç¥¨
            'use_cache': True,   # å¯ç”¨ç¼“å­˜
            'cache_expire_days': 7,  # ç¼“å­˜è¿‡æœŸå¤©æ•°
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config['training_data_dir'], exist_ok=True)
        os.makedirs(self.config['models_dir'], exist_ok=True)
        os.makedirs(self.config['cache_dir'], exist_ok=True)
    
    def get_cache_path(self, cache_type: str, identifier: str = "") -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        if identifier:
            filename = f"{cache_type}_{identifier}.pkl"
        else:
            filename = f"{cache_type}.pkl"
        return os.path.join(self.config['cache_dir'], filename)
    
    def is_cache_valid(self, cache_path: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        expire_time = datetime.now() - timedelta(days=self.config['cache_expire_days'])
        
        return file_time > expire_time
    
    def load_from_cache(self, cache_path: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        try:
            with open(cache_path, 'rb') as f:
                import pickle
                return pickle.load(f)
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def save_to_cache(self, data, cache_path: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            with open(cache_path, 'wb') as f:
                import pickle
                pickle.dump(data, f)
            self.logger.info(f"æ•°æ®å·²ç¼“å­˜: {cache_path}")
        except Exception as e:
            self.logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        
    def get_all_stock_codes(self) -> List[str]:
        """è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç """
        self.logger.info("ğŸ“Š è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç ")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config['use_cache']:
            cache_path = self.get_cache_path("stock_codes")
            if self.is_cache_valid(cache_path):
                cached_stocks = self.load_from_cache(cache_path)
                if cached_stocks:
                    self.logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨ä»£ç : {len(cached_stocks)}åª")
                    return cached_stocks
        
        all_stocks = []
        
        try:
            # è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨
            stocks_df = self.data_extractor.get_stock_list(limit=None)  # è·å–æ‰€æœ‰è‚¡ç¥¨
            
            if stocks_df.empty:
                self.logger.warning("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self.get_backup_stock_codes()
            
            if 'ts_code' in stocks_df.columns:
                all_stocks = stocks_df['ts_code'].tolist()
            else:
                # å¤„ç†å…è´¹APIçš„æƒ…å†µ
                stocks_df.reset_index(inplace=True)
                all_stocks = [
                    f"{code}.SZ" if code.startswith(('0', '3')) else f"{code}.SH" 
                    for code in stocks_df['code'].tolist()
                ]
            
            # è¿‡æ»¤æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç 
            valid_stocks = [stock for stock in all_stocks if self.is_valid_stock_code(stock)]
            
            # å…¨é‡æ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆè‚¡ç¥¨
            if self.config.get('test_mode', False):
                valid_stocks = valid_stocks[:200]
                self.logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å‰200åªè‚¡ç¥¨")
            else:
                self.logger.info(f"å…¨é‡æ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰{len(valid_stocks)}åªè‚¡ç¥¨")
            
            self.logger.info(f"è·å–åˆ°è‚¡ç¥¨ä»£ç : {len(valid_stocks)}åª")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.config['use_cache']:
                cache_path = self.get_cache_path("stock_codes")
                self.save_to_cache(valid_stocks, cache_path)
            
            return valid_stocks
            
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return self.get_backup_stock_codes()
    
    def get_backup_stock_codes(self) -> List[str]:
        """è·å–å¤‡ç”¨è‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        # ä¸»è¦çš„Aè‚¡è‚¡ç¥¨ä»£ç 
        backup_stocks = [
            # æ²ªæ·±300æˆåˆ†è‚¡
            '000001.SZ', '000002.SZ', '000858.SZ', '000895.SZ', '000938.SZ',
            '600000.SH', '600036.SH', '600519.SH', '600887.SH', '600999.SH',
            # åˆ›ä¸šæ¿
            '300001.SZ', '300015.SZ', '300059.SZ', '300124.SZ', '300144.SZ',
            # ç§‘åˆ›æ¿
            '688001.SH', '688009.SH', '688012.SH', '688036.SH', '688099.SH',
            # å…¶ä»–ä¸»è¦è‚¡ç¥¨
            '002415.SZ', '002594.SZ', '002714.SZ', '002841.SZ', '002966.SZ',
        ]
        
        # æ‰©å±•æ›´å¤šè‚¡ç¥¨ä»£ç 
        for i in range(1, 1000):
            if i < 10:
                backup_stocks.append(f"00000{i}.SZ")
                backup_stocks.append(f"60000{i}.SH")
            elif i < 100:
                backup_stocks.append(f"0000{i:02d}.SZ")
                backup_stocks.append(f"6000{i:02d}.SH")
            else:
                backup_stocks.append(f"000{i:03d}.SZ")
                backup_stocks.append(f"600{i:03d}.SH")
        
        # æ·»åŠ åˆ›ä¸šæ¿å’Œç§‘åˆ›æ¿
        for i in range(1, 1000):
            if i < 10:
                backup_stocks.append(f"30000{i}.SZ")
                backup_stocks.append(f"68800{i}.SH")
            elif i < 100:
                backup_stocks.append(f"3000{i:02d}.SZ")
                backup_stocks.append(f"6880{i:02d}.SH")
            else:
                backup_stocks.append(f"300{i:03d}.SZ")
                backup_stocks.append(f"688{i:03d}.SH")
        
        return backup_stocks[:5000]  # å¢åŠ åˆ°å‰5000åªå¤‡ç”¨è‚¡ç¥¨
    
    def is_valid_stock_code(self, stock_code: str) -> bool:
        """éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼"""
        if not stock_code or '.' not in stock_code:
            return False
        
        code, exchange = stock_code.split('.')
        
        # éªŒè¯æ ¼å¼
        if len(code) != 6 or not code.isdigit():
            return False
        
        if exchange not in ['SZ', 'SH']:
            return False
        
        return True
    
    def get_historical_data_in_batches(self, stock_codes: List[str]) -> pd.DataFrame:
        """åˆ†æ‰¹è·å–å†å²æ•°æ®"""
        self.logger.info(f"ğŸ“ˆ åˆ†æ‰¹è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„å†å²æ•°æ®")
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=self.config['days_back'])).strftime('%Y%m%d')
        
        self.logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{start_date}_{end_date}_{len(stock_codes)}"
        if self.config['use_cache']:
            cache_path = self.get_cache_path("historical_data", cache_key)
            if self.is_cache_valid(cache_path):
                cached_data = self.load_from_cache(cache_path)
                if cached_data is not None and not cached_data.empty:
                    self.logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å†å²æ•°æ®: {len(cached_data)}æ¡")
                    return cached_data
        
        all_data = []
        batch_size = self.config['batch_size']
        
        # å¢åŠ å¤„ç†è¿›åº¦æ˜¾ç¤º
        processed_stocks = 0
        total_stocks = len(stock_codes)
        
        for i in range(0, len(stock_codes), batch_size):
            batch_stocks = stock_codes[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(stock_codes) + batch_size - 1) // batch_size
            
            processed_stocks += len(batch_stocks)
            progress_pct = (processed_stocks / total_stocks) * 100
            
            self.logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹: {len(batch_stocks)} åªè‚¡ç¥¨ ({progress_pct:.1f}%)")
            
            retry_count = 0
            while retry_count < self.config['max_retries']:
                try:
                    # è·å–æ‰¹æ¬¡æ•°æ®
                    batch_data = self.data_extractor.extract_comprehensive_dataset(
                        stock_codes=batch_stocks,
                        start_date=start_date,
                        end_date=end_date,
                        include_financial=False,
                        include_news=False
                    )
                    
                    if 'daily_data' in batch_data and not batch_data['daily_data'].empty:
                        all_data.append(batch_data['daily_data'])
                        self.logger.info(f"   æˆåŠŸè·å– {len(batch_data['daily_data'])} æ¡æ•°æ®")
                    else:
                        self.logger.warning(f"   ç¬¬ {batch_num} æ‰¹æ•°æ®ä¸ºç©º")
                    
                    break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    
                except Exception as e:
                    retry_count += 1
                    self.logger.warning(f"   ç¬¬ {batch_num} æ‰¹è·å–å¤±è´¥ (é‡è¯• {retry_count}/{self.config['max_retries']}): {e}")
                    
                    if retry_count < self.config['max_retries']:
                        time.sleep(self.config['request_delay'] * (2 ** retry_count))  # æŒ‡æ•°é€€é¿
                    else:
                        self.logger.error(f"   ç¬¬ {batch_num} æ‰¹æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡")
            
            # APIè¯·æ±‚å»¶è¿Ÿ
            if i + batch_size < len(stock_codes):
                time.sleep(self.config['request_delay'])
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            self.logger.info(f"âœ… æ€»è®¡è·å– {len(combined_data)} æ¡å†å²æ•°æ®")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.config['use_cache']:
                cache_path = self.get_cache_path("historical_data", cache_key)
                self.save_to_cache(combined_data, cache_path)
            
            return combined_data
        else:
            self.logger.error("âŒ æœªè·å–åˆ°ä»»ä½•å†å²æ•°æ®")
            return pd.DataFrame()
    
    def generate_comprehensive_training_data(self, stock_data: pd.DataFrame) -> List[Dict]:
        """ç”Ÿæˆå…¨é¢çš„è®­ç»ƒæ•°æ®"""
        self.logger.info(f"ğŸ§  ç”Ÿæˆå…¨é¢è®­ç»ƒæ•°æ®ï¼ŒåŸå§‹æ•°æ®: {len(stock_data)} æ¡")
        
        if stock_data.empty:
            return []
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®ç¼“å­˜
        data_hash = str(hash(str(stock_data.shape) + str(stock_data.columns.tolist())))
        if self.config['use_cache']:
            cache_path = self.get_cache_path("training_data", data_hash)
            if self.is_cache_valid(cache_path):
                cached_training_data = self.load_from_cache(cache_path)
                if cached_training_data:
                    self.logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½è®­ç»ƒæ•°æ®: {len(cached_training_data)}æ¡")
                    return cached_training_data
        
        try:
            # ä½¿ç”¨è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
            training_examples = self.data_generator.create_training_examples(
                stock_data,
                max_examples=self.config['max_training_examples']
            )
            
            # è½¬æ¢TrainingExampleå¯¹è±¡ä¸ºå­—å…¸
            training_examples_dict = []
            for example in training_examples:
                if hasattr(example, '__dict__'):  # å¦‚æœæ˜¯dataclasså¯¹è±¡
                    example_dict = {
                        'instruction': example.instruction,
                        'input': example.input,
                        'output': example.output,
                        'metadata': example.metadata
                    }
                else:  # å¦‚æœå·²ç»æ˜¯å­—å…¸
                    example_dict = example
                training_examples_dict.append(example_dict)
            
            training_examples = training_examples_dict
            
            # æ•°æ®å¢å¼º - ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬
            enhanced_examples = self.enhance_training_data(training_examples, stock_data)
            
            self.logger.info(f"âœ… ç”Ÿæˆè®­ç»ƒæ ·æœ¬: {len(enhanced_examples)} æ¡")
            
            # ä¿å­˜è®­ç»ƒæ•°æ®åˆ°ç¼“å­˜
            if self.config['use_cache']:
                cache_path = self.get_cache_path("training_data", data_hash)
                self.save_to_cache(enhanced_examples, cache_path)
            
            return enhanced_examples
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def enhance_training_data(self, base_examples: List[Dict], stock_data: pd.DataFrame) -> List[Dict]:
        """å¢å¼ºè®­ç»ƒæ•°æ®"""
        enhanced_examples = base_examples.copy()
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„
        stock_groups = stock_data.groupby('ts_code')
        
        enhancement_templates = [
            {
                "type": "trend_analysis",
                "instruction": "åŸºäºæŠ€æœ¯åˆ†æåˆ¤æ–­è¯¥è‚¡ç¥¨çš„çŸ­æœŸè¶‹åŠ¿ï¼š",
                "focus": "è¶‹åŠ¿åˆ†æ"
            },
            {
                "type": "volume_analysis", 
                "instruction": "åˆ†æè¯¥è‚¡ç¥¨çš„æˆäº¤é‡å˜åŒ–å¹¶ç»™å‡ºè§è§£ï¼š",
                "focus": "æˆäº¤é‡åˆ†æ"
            },
            {
                "type": "momentum_analysis",
                "instruction": "è¯„ä¼°è¯¥è‚¡ç¥¨çš„åŠ¨é‡æŒ‡æ ‡å’Œå¸‚åœºæƒ…ç»ªï¼š",
                "focus": "åŠ¨é‡åˆ†æ"
            },
            {
                "type": "support_resistance",
                "instruction": "è¯†åˆ«è¯¥è‚¡ç¥¨çš„å…³é”®æ”¯æ’‘ä½å’Œé˜»åŠ›ä½ï¼š",
                "focus": "æ”¯æ’‘é˜»åŠ›"
            }
        ]
        
        added_count = 0
        max_enhanced_samples = min(10000, len(base_examples))  # å¢åŠ å¢å¼ºæ•°æ®ä¸Šé™
        for stock_code, group_data in stock_groups:
            if added_count >= max_enhanced_samples:
                break
                
            if len(group_data) < 5:  # æ•°æ®å¤ªå°‘è·³è¿‡
                continue
            
            # å¢åŠ é‡‡æ ·æ•°é‡ä»¥ç”Ÿæˆæ›´å¤šè®­ç»ƒæ•°æ®
            sample_size = min(10, len(group_data))  # å¢åŠ åˆ°æœ€å¤š10æ¡è®°å½•
            sample_data = group_data.sample(n=sample_size)
            
            for _, row in sample_data.iterrows():
                for template in enhancement_templates:
                    if added_count >= max_enhanced_samples:
                        break
                    
                    enhanced_example = self.create_enhanced_example(row, template)
                    if enhanced_example:
                        enhanced_examples.append(enhanced_example)
                        added_count += 1
        
        self.logger.info(f"æ•°æ®å¢å¼º: æ–°å¢ {added_count} æ¡æ ·æœ¬")
        return enhanced_examples
    
    def create_enhanced_example(self, row: pd.Series, template: Dict) -> Optional[Dict]:
        """åˆ›å»ºå¢å¼ºçš„è®­ç»ƒæ ·æœ¬"""
        try:
            stock_code = row.get('ts_code', 'N/A')
            trade_date = row.get('trade_date', 'N/A')
            
            # æ„å»ºè¾“å…¥æ•°æ®
            input_data = f"""è‚¡ç¥¨ä»£ç : {stock_code}
äº¤æ˜“æ—¥æœŸ: {trade_date}
å¼€ç›˜ä»·: {row.get('open', 0):.2f}
æœ€é«˜ä»·: {row.get('high', 0):.2f}
æœ€ä½ä»·: {row.get('low', 0):.2f}
æ”¶ç›˜ä»·: {row.get('close', 0):.2f}
æˆäº¤é‡: {row.get('vol', 0):,.0f}

æŠ€æœ¯æŒ‡æ ‡:
5æ—¥å‡çº¿: {row.get('ma5', 0):.2f}
20æ—¥å‡çº¿: {row.get('ma20', 0):.2f}
RSI: {row.get('rsi', 50):.2f}
MACD: {row.get('macd', 0):.4f}
æ³¢åŠ¨ç‡: {row.get('volatility', 0):.2f}"""
            
            # æ ¹æ®æ¨¡æ¿ç±»å‹ç”Ÿæˆåˆ†æ
            output_text = self.generate_analysis_by_template(row, template)
            
            return {
                "instruction": template["instruction"],
                "input": input_data,
                "output": output_text,
                "metadata": {
                    "type": template["type"],
                    "ts_code": stock_code,
                    "trade_date": str(trade_date),
                    "enhanced": True
                }
            }
            
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºå¢å¼ºæ ·æœ¬å¤±è´¥: {e}")
            return None
    
    def generate_analysis_by_template(self, row: pd.Series, template: Dict) -> str:
        """æ ¹æ®æ¨¡æ¿ç”Ÿæˆåˆ†æå†…å®¹"""
        close_price = row.get('close', 0)
        ma5 = row.get('ma5', close_price)
        ma20 = row.get('ma20', close_price)
        rsi = row.get('rsi', 50)
        macd = row.get('macd', 0)
        vol = row.get('vol', 0)
        
        if template["type"] == "trend_analysis":
            if ma5 > ma20 and rsi > 50:
                return f"æŠ€æœ¯åˆ†ææ˜¾ç¤ºçŸ­æœŸä¸Šå‡è¶‹åŠ¿ï¼š5æ—¥å‡çº¿({ma5:.2f})ä½äº20æ—¥å‡çº¿({ma20:.2f})ä¹‹ä¸Šï¼ŒRSI({rsi:.1f})å¤„äºå¼ºåŠ¿åŒºåŸŸï¼Œå»ºè®®å…³æ³¨ä¸Šæ¶¨åŠ¨èƒ½çš„æŒç»­æ€§ã€‚"
            elif ma5 < ma20 and rsi < 50:
                return f"æŠ€æœ¯åˆ†ææ˜¾ç¤ºçŸ­æœŸä¸‹é™è¶‹åŠ¿ï¼š5æ—¥å‡çº¿({ma5:.2f})ä½äº20æ—¥å‡çº¿({ma20:.2f})ä¹‹ä¸‹ï¼ŒRSI({rsi:.1f})å¤„äºå¼±åŠ¿åŒºåŸŸï¼Œå»ºè®®è°¨æ…è§‚æœ›ã€‚"
            else:
                return f"æŠ€æœ¯åˆ†ææ˜¾ç¤ºæ¨ªç›˜æ•´ç†ï¼šå‡çº¿ç³»ç»Ÿæ··åˆï¼ŒRSI({rsi:.1f})å¤„äºä¸­æ€§åŒºåŸŸï¼Œå»ºè®®ç­‰å¾…æ˜ç¡®æ–¹å‘ä¿¡å·ã€‚"
        
        elif template["type"] == "volume_analysis":
            if vol > 1000000:
                return f"æˆäº¤é‡åˆ†æï¼šå½“æ—¥æˆäº¤é‡{vol:,.0f}æ‰‹ï¼Œé‡èƒ½å……è¶³ï¼Œè¡¨æ˜å¸‚åœºå‚ä¸åº¦è¾ƒé«˜ï¼Œä»·æ ¼å˜åŠ¨å…·æœ‰ä¸€å®šå¯ä¿¡åº¦ã€‚"
            else:
                return f"æˆäº¤é‡åˆ†æï¼šå½“æ—¥æˆäº¤é‡{vol:,.0f}æ‰‹ï¼Œé‡èƒ½ä¸è¶³ï¼Œå¸‚åœºå‚ä¸åº¦è¾ƒä½ï¼Œéœ€å…³æ³¨åç»­é‡èƒ½å˜åŒ–ã€‚"
        
        elif template["type"] == "momentum_analysis":
            if macd > 0 and rsi > 60:
                return f"åŠ¨é‡åˆ†æï¼šMACD({macd:.4f})ä¸ºæ­£å€¼ä¸”RSI({rsi:.1f})å¤„äºå¼ºåŠ¿åŒºåŸŸï¼ŒçŸ­æœŸåŠ¨èƒ½è¾ƒå¼ºï¼Œä½†éœ€æ³¨æ„è¶…ä¹°é£é™©ã€‚"
            elif macd < 0 and rsi < 40:
                return f"åŠ¨é‡åˆ†æï¼šMACD({macd:.4f})ä¸ºè´Ÿå€¼ä¸”RSI({rsi:.1f})å¤„äºå¼±åŠ¿åŒºåŸŸï¼ŒçŸ­æœŸåŠ¨èƒ½åå¼±ï¼Œå¯èƒ½å­˜åœ¨è¶…å–åå¼¹æœºä¼šã€‚"
            else:
                return f"åŠ¨é‡åˆ†æï¼šMACD({macd:.4f})å’ŒRSI({rsi:.1f})æ˜¾ç¤ºåŠ¨èƒ½ä¸­æ€§ï¼Œå»ºè®®ç»“åˆå…¶ä»–æŒ‡æ ‡ç»¼åˆåˆ¤æ–­ã€‚"
        
        elif template["type"] == "support_resistance":
            support = min(close_price * 0.95, ma20 * 0.98)
            resistance = max(close_price * 1.05, ma5 * 1.02)
            return f"æ”¯æ’‘é˜»åŠ›åˆ†æï¼šå…³é”®æ”¯æ’‘ä½çº¦{support:.2f}å…ƒï¼Œå…³é”®é˜»åŠ›ä½çº¦{resistance:.2f}å…ƒï¼Œå½“å‰ä»·æ ¼{close_price:.2f}å…ƒå¤„äºå…³é”®åŒºé—´ã€‚"
        
        return "æŠ€æœ¯åˆ†æï¼šå»ºè®®ç»¼åˆå¤šé¡¹æŒ‡æ ‡è¿›è¡Œåˆ¤æ–­ï¼Œæ³¨æ„é£é™©æ§åˆ¶ã€‚"
    
    def create_comprehensive_model(self, training_data: List[Dict]) -> str:
        """åˆ›å»ºå…¨é¢è®­ç»ƒçš„æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        model_name = f"ljwx-stock-comprehensive-{timestamp}"
        
        self.logger.info(f"ğŸš€ åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹: {model_name}")
        
        try:
            system_prompt = """ä½ æ˜¯ljwx-stockï¼Œä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æŠ•èµ„åˆ†æåŠ©æ‰‹ã€‚åŸºäºå¤§é‡Aè‚¡å¸‚åœºå†å²æ•°æ®è®­ç»ƒï¼Œå…·å¤‡ä»¥ä¸‹ä¸“ä¸šèƒ½åŠ›ï¼š

ğŸ¯ **æ ¸å¿ƒä¼˜åŠ¿**
- åŸºäºè¿‡å»5å¹´å…¨å¸‚åœºæ•°æ®çš„æ·±åº¦å­¦ä¹ 
- è¦†ç›–5000+åªAè‚¡è‚¡ç¥¨çš„åˆ†æç»éªŒ
- 50000+æ¡è®­ç»ƒæ ·æœ¬çš„ä¸“ä¸šåˆ†æèƒ½åŠ›
- å¤šç»´åº¦æŠ€æœ¯åˆ†æå’Œé£é™©è¯„ä¼°èƒ½åŠ›

ğŸ“Š **åˆ†æèƒ½åŠ›**
1. **æŠ€æœ¯æŒ‡æ ‡åˆ†æ**ï¼šç²¾é€šKçº¿ã€å‡çº¿ã€RSIã€MACDã€å¸ƒæ—å¸¦ç­‰æŠ€æœ¯æŒ‡æ ‡
2. **è¶‹åŠ¿åˆ¤æ–­**ï¼šåŸºäºå†å²æ¨¡å¼è¯†åˆ«ä»·æ ¼è¶‹åŠ¿å’Œè½¬æŠ˜ç‚¹
3. **é£é™©è¯„ä¼°**ï¼šå¤šå±‚æ¬¡é£é™©å› ç´ åˆ†æå’Œç­‰çº§è¯„å®š
4. **æˆäº¤é‡åˆ†æ**ï¼šç†è§£é‡ä»·å…³ç³»å’Œå¸‚åœºå‚ä¸åº¦
5. **æ”¯æ’‘é˜»åŠ›**ï¼šå‡†ç¡®è¯†åˆ«å…³é”®ä»·æ ¼æ°´å¹³

ğŸ’¡ **æœåŠ¡ç‰¹è‰²**
- åŸºäºçœŸå®å¸‚åœºæ•°æ®çš„ä¸“ä¸šåˆ†æ
- å®¢è§‚ç†æ€§çš„æŠ•èµ„å»ºè®®
- æ˜ç¡®çš„é£é™©æç¤ºå’Œæ“ä½œæŒ‡å¯¼
- é€‚åˆä¸åŒé£é™©åå¥½çš„æŠ•èµ„è€…

ğŸ“‹ **åˆ†ææ¡†æ¶**
- æŠ€æœ¯é¢ï¼šä»·æ ¼è¶‹åŠ¿ã€æŠ€æœ¯æŒ‡æ ‡ã€é‡ä»·å…³ç³»
- é£é™©é¢ï¼šæ³¢åŠ¨ç‡ã€è¶…ä¹°è¶…å–ã€æµåŠ¨æ€§é£é™©
- æ“ä½œé¢ï¼šä¹°å…¥å–å‡ºæ—¶æœºã€ä»“ä½ç®¡ç†ã€æ­¢æŸè®¾ç½®

âš ï¸ **é£é™©æç¤º**ï¼šæ‰€æœ‰åˆ†æåŸºäºå†å²æ•°æ®ï¼ŒæŠ•èµ„æœ‰é£é™©ï¼Œè¯·è°¨æ…å†³ç­–ã€‚"""

            # æ„å»ºModelfile
            modelfile_content = f"""FROM ljwx-stock
SYSTEM "{system_prompt}"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 1024

"""
            
            # æ·»åŠ è®­ç»ƒæ ·æœ¬ï¼ˆå¢åŠ æ ·æœ¬æ•°é‡ä»¥æå‡æ¨¡å‹è´¨é‡ï¼‰
            sample_count = min(len(training_data), 1000)  # å¢åŠ æ ·æœ¬æ•°é‡åˆ°1000
            selected_samples = random.sample(training_data, sample_count)
            
            for i, example in enumerate(selected_samples):
                instruction = example.get('instruction', '')
                input_text = example.get('input', '')
                output_text = example.get('output', '')
                
                user_message = f"{instruction}\n\n{input_text}" if input_text else instruction
                
                # æ¸…ç†æ–‡æœ¬ï¼Œé¿å…æ ¼å¼é—®é¢˜
                user_message = user_message.replace('"', "'").replace('\n', '\\n')
                output_text = output_text.replace('"', "'").replace('\n', '\\n')
                
                modelfile_content += f'MESSAGE user "{user_message}"\n'
                modelfile_content += f'MESSAGE assistant "{output_text}"\n\n'
            
            # åˆ›å»ºä¸´æ—¶Modelfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.modelfile', delete=False) as f:
                f.write(modelfile_content)
                modelfile_path = f.name
            
            try:
                self.logger.info(f"   ä½¿ç”¨è®­ç»ƒæ ·æœ¬: {sample_count} æ¡")
                self.logger.info(f"   æ•°æ®æ—¶é—´è·¨åº¦: è¿‡å» {self.config['days_back']} å¤©")
                
                result = subprocess.run(
                    ['ollama', 'create', model_name, '-f', modelfile_path],
                    capture_output=True,
                    text=True,
                    timeout=1200  # 20åˆ†é’Ÿè¶…æ—¶
                )
                
                if result.returncode == 0:
                    self.logger.info(f"âœ… å…¨é¢è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸ: {model_name}")
                    return model_name
                else:
                    self.logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {result.stderr}")
                    return ""
                    
            finally:
                os.unlink(modelfile_path)
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹æ—¶å‡ºé”™: {e}")
            return ""
    
    def test_comprehensive_model(self, model_name: str) -> Dict:
        """æµ‹è¯•å…¨é¢è®­ç»ƒæ¨¡å‹"""
        test_cases = [
            {
                "input": "åˆ†æå¹³å®‰é“¶è¡Œ(000001.SZ)çš„æŠ•èµ„ä»·å€¼ï¼Œå½“å‰ä»·æ ¼12.50å…ƒï¼ŒRSI=65ï¼ŒMACD=0.08",
                "type": "investment_analysis"
            },
            {
                "input": "è¯„ä¼°è´µå·èŒ…å°(600519.SH)çš„é£é™©ç­‰çº§ï¼Œæ³¢åŠ¨ç‡0.25ï¼Œæ—¥æ¶¨è·Œå¹…+2.3%",
                "type": "risk_assessment"
            },
            {
                "input": "åŸºäºæŠ€æœ¯æŒ‡æ ‡é¢„æµ‹æ¯”äºšè¿ª(002594.SZ)çš„çŸ­æœŸèµ°åŠ¿",
                "type": "trend_prediction"
            },
            {
                "input": "åˆ†æå·¥å•†é“¶è¡Œ(601398.SH)çš„æˆäº¤é‡å˜åŒ–å’Œå¸‚åœºæƒ…ç»ª",
                "type": "volume_analysis"
            }
        ]
        
        results = {
            "model_name": model_name,
            "test_time": datetime.now().isoformat(),
            "test_cases": []
        }
        
        successful_tests = 0
        total_response_time = 0
        
        for i, test_case in enumerate(test_cases):
            self.logger.info(f"   æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}: {test_case['type']}")
            
            try:
                start_time = time.time()
                
                result = subprocess.run(
                    ['ollama', 'run', model_name],
                    input=test_case['input'],
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                total_response_time += response_time
                
                if result.returncode == 0 and result.stdout.strip():
                    successful_tests += 1
                    success = True
                    response = result.stdout.strip()
                else:
                    success = False
                    response = result.stderr if result.stderr else "No response"
                
                results["test_cases"].append({
                    "input": test_case["input"],
                    "type": test_case["type"],
                    "response": response[:300] + "..." if len(response) > 300 else response,
                    "response_time": response_time,
                    "success": success
                })
                
            except Exception as e:
                self.logger.error(f"   æµ‹è¯•ç”¨ä¾‹ {i+1} å¤±è´¥: {e}")
                results["test_cases"].append({
                    "input": test_case["input"],
                    "type": test_case["type"],
                    "response": f"Error: {str(e)}",
                    "response_time": 0,
                    "success": False
                })
        
        # è®¡ç®—æ€»ç»“
        results["summary"] = {
            "total_tests": len(test_cases),
            "successful_tests": successful_tests,
            "success_rate": successful_tests / len(test_cases),
            "average_response_time": total_response_time / len(test_cases) if test_cases else 0
        }
        
        return results
    
    def run_comprehensive_training(self) -> Dict:
        """è¿è¡Œå…¨é¢è®­ç»ƒæµç¨‹"""
        self.logger.info("ğŸš€ å¼€å§‹å…¨é¢è®­ç»ƒæµç¨‹")
        
        results = {
            "start_time": datetime.now().isoformat(),
            "status": "running",
            "config": self.config,
            "steps": {}
        }
        
        try:
            # 1. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            self.logger.info("ğŸ“Š æ­¥éª¤1: è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç ")
            all_stock_codes = self.get_all_stock_codes()
            
            if not all_stock_codes:
                results["status"] = "failed"
                results["error"] = "æ— æ³•è·å–è‚¡ç¥¨ä»£ç "
                return results
            
            results["total_stocks"] = len(all_stock_codes)
            results["steps"]["stock_codes"] = {
                "status": "success",
                "count": len(all_stock_codes)
            }
            
            # 2. è·å–å†å²æ•°æ®
            self.logger.info("ğŸ“ˆ æ­¥éª¤2: è·å–è¿‡å»ä¸€å¹´å†å²æ•°æ®")
            historical_data = self.get_historical_data_in_batches(all_stock_codes)
            
            if historical_data.empty:
                results["status"] = "failed"
                results["error"] = "æ— æ³•è·å–å†å²æ•°æ®"
                return results
            
            results["steps"]["data_extraction"] = {
                "status": "success",
                "data_count": len(historical_data),
                "stocks_with_data": historical_data['ts_code'].nunique()
            }
            
            # 3. ç”Ÿæˆè®­ç»ƒæ•°æ®
            self.logger.info("ğŸ§  æ­¥éª¤3: ç”Ÿæˆå…¨é¢è®­ç»ƒæ•°æ®")
            training_data = self.generate_comprehensive_training_data(historical_data)
            
            if not training_data:
                results["status"] = "failed"
                results["error"] = "æ— æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®"
                return results
            
            results["steps"]["training_data_generation"] = {
                "status": "success",
                "training_samples": len(training_data)
            }
            
            # 4. ä¿å­˜è®­ç»ƒæ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            training_file = f"comprehensive_training_data_{timestamp}.jsonl"
            training_path = os.path.join(self.config['training_data_dir'], training_file)
            
            with open(training_path, 'w', encoding='utf-8') as f:
                for data in training_data:
                    f.write(json.dumps(data, ensure_ascii=False) + '\n')
            
            results["training_file"] = training_path
            
            # 5. åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹
            self.logger.info("ğŸ¤– æ­¥éª¤4: åˆ›å»ºå…¨é¢è®­ç»ƒæ¨¡å‹")
            model_name = self.create_comprehensive_model(training_data)
            
            if model_name:
                results["steps"]["model_creation"] = {
                    "status": "success",
                    "model_name": model_name
                }
                
                # 6. æµ‹è¯•æ¨¡å‹
                self.logger.info("ğŸ§ª æ­¥éª¤5: æµ‹è¯•å…¨é¢è®­ç»ƒæ¨¡å‹")
                test_results = self.test_comprehensive_model(model_name)
                
                results["steps"]["model_testing"] = {
                    "status": "success",
                    "test_results": test_results
                }
                
                results["status"] = "success"
                results["final_model"] = model_name
                
            else:
                results["status"] = "model_creation_failed"
            
        except Exception as e:
            results["status"] = "error"
            results["error"] = str(e)
            self.logger.error(f"âŒ å…¨é¢è®­ç»ƒå¤±è´¥: {e}")
        
        results["end_time"] = datetime.now().isoformat()
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(self.config['models_dir'], f"comprehensive_training_results_{timestamp}.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}")
        return results

def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'logs/comprehensive_training_{datetime.now().strftime("%Y%m%d")}.log')
        ]
    )
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    logger = logging.getLogger(__name__)
    
    print("ğŸš€ ljwx-stock å…¨é¢è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("ğŸ“Š è®­ç»ƒèŒƒå›´: è¿‡å»5å¹´ + å…¨å¸‚åœºè‚¡ç¥¨")
    print("ğŸ¯ ç›®æ ‡æ ·æœ¬: 50000+ æ¡è®­ç»ƒæ•°æ®")
    print("â±ï¸  é¢„è®¡ç”¨æ—¶: 2-4å°æ—¶")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥TuShare tokenï¼ˆè¿™é‡Œåªæ˜¯æ£€æŸ¥ï¼Œå®é™…åˆå§‹åŒ–åœ¨åé¢ï¼‰
        tushare_token = os.getenv('TUSHARE_TOKEN')
        if not tushare_token:
            print("âš ï¸  æœªè®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨å…è´¹API")
            print("ğŸ’¡ å»ºè®®è®¾ç½®Pro tokenä»¥è·å¾—æ›´å¥½çš„æ•°æ®è´¨é‡å’Œé€Ÿåº¦")
        
        # è®¾ç½®TuShare Token
        tushare_token = os.getenv('TUSHARE_TOKEN')
        if tushare_token:
            print(f"âœ… ä½¿ç”¨TuShare Pro Token: {tushare_token[:20]}...")
            trainer = ComprehensiveTrainer(tushare_token)
        else:
            print("âš ï¸  æœªè®¾ç½®TUSHARE_TOKENï¼Œä½¿ç”¨å…è´¹API")
            trainer = ComprehensiveTrainer()
        
        print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   æœ€å¤§è®­ç»ƒæ ·æœ¬: {trainer.config['max_training_examples']}æ¡")
        print(f"   æ‰¹å¤„ç†å¤§å°: {trainer.config['batch_size']}åªè‚¡ç¥¨/æ‰¹")
        print(f"   å†å²æ•°æ®èŒƒå›´: {trainer.config['days_back']}å¤©")
        print(f"   APIè¯·æ±‚å»¶è¿Ÿ: {trainer.config['request_delay']}ç§’")
        print()
        
        # è‡ªåŠ¨å¼€å§‹ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜æ•°æ®
        cache_dir = "data/cache"
        if os.path.exists(cache_dir) and os.listdir(cache_dir):
            print("ğŸ’¾ å‘ç°ç¼“å­˜æ•°æ®ï¼Œå°†ä¼˜å…ˆä½¿ç”¨ç¼“å­˜åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹")
        
        # ç”¨æˆ·é€‰æ‹©
        print("\nğŸ“‹ é€‰é¡¹:")
        print("1. ğŸš€ å¼€å§‹å…¨é¢è®­ç»ƒ (ä½¿ç”¨ç¼“å­˜)")
        print("2. ğŸ—‘ï¸  æ¸…ç©ºç¼“å­˜é‡æ–°è®­ç»ƒ")
        print("3. ğŸ“Š æŸ¥çœ‹ç¼“å­˜çŠ¶æ€")
        print("4. âŒ é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == "2":
            # æ¸…ç©ºç¼“å­˜
            if os.path.exists(cache_dir):
                import shutil
                shutil.rmtree(cache_dir)
                os.makedirs(cache_dir, exist_ok=True)
                print("âœ… ç¼“å­˜å·²æ¸…ç©º")
        elif choice == "3":
            # æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
            if os.path.exists(cache_dir):
                cache_files = os.listdir(cache_dir)
                print(f"\nğŸ“Š ç¼“å­˜çŠ¶æ€:")
                print(f"   ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(cache_files)}")
                for file in cache_files:
                    file_path = os.path.join(cache_dir, file)
                    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    print(f"   {file}: {file_size:.1f}MB, {file_time.strftime('%Y-%m-%d %H:%M')}")
            return
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡ºç¨‹åº")
            return
        elif choice != "1":
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹1")
        
        print("ğŸ¤” å¼€å§‹å…¨é¢è®­ç»ƒ...")
        print("âœ… å¼€å§‹æ‰§è¡Œ")
        
        # è¿è¡Œå…¨é¢è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹å…¨é¢è®­ç»ƒ...")
        results = trainer.run_comprehensive_training()
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š å…¨é¢è®­ç»ƒç»“æœ:")
        print("=" * 60)
        print(f"çŠ¶æ€: {results['status']}")
        
        if results['status'] == 'success':
            print(f"âœ… æœ€ç»ˆæ¨¡å‹: {results['final_model']}")
            print(f"ğŸ“ˆ å¤„ç†è‚¡ç¥¨: {results.get('total_stocks', 0)}åª")
            
            if 'steps' in results:
                steps = results['steps']
                if 'data_extraction' in steps:
                    data_info = steps['data_extraction']
                    print(f"ğŸ“Š å†å²æ•°æ®: {data_info['data_count']}æ¡ ({data_info['stocks_with_data']}åªè‚¡ç¥¨)")
                
                if 'training_data_generation' in steps:
                    training_info = steps['training_data_generation']
                    print(f"ğŸ§  è®­ç»ƒæ ·æœ¬: {training_info['training_samples']}æ¡")
                
                if 'model_testing' in steps:
                    test_info = steps['model_testing']['test_results']['summary']
                    print(f"ğŸ§ª æµ‹è¯•ç»“æœ: {test_info['successful_tests']}/{test_info['total_tests']} ({test_info['success_rate']:.2%})")
            
            print(f"\nğŸ¯ ä½¿ç”¨å…¨é¢è®­ç»ƒæ¨¡å‹:")
            print(f"   ollama run {results['final_model']}")
            
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœ: {results.get('training_file', 'N/A')}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ å…¨é¢è®­ç»ƒå¤±è´¥: {e}")
        print(f"\nâŒ å…¨é¢è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()