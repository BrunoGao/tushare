#!/usr/bin/env python3
"""
å¤§æ¨¡å‹è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - ä¸»è®­ç»ƒè„šæœ¬
ç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯ï¼Œè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹ï¼Œç”Ÿæˆå®Œæ•´çš„æ¨¡å‹å’Œè¯„ä¼°æŠ¥å‘Š
"""
import os
import sys
import logging
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
import argparse
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# æ ¸å¿ƒæ¨¡å—å¯¼å…¥
from database.db_manager import DatabaseManager
from llm.enhanced_feature_engineering import enhanced_feature_engineer

# AIè®­ç»ƒæ¨¡å—
try:
    from ai.unified_trainer import unified_trainer
    UNIFIED_TRAINER_AVAILABLE = True
except ImportError:
    UNIFIED_TRAINER_AVAILABLE = False

try:
    from ai.deep_learning_trainer import deep_learning_trainer
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    DEEP_LEARNING_AVAILABLE = False

try:
    from ai.advanced_evaluation_system import evaluation_system, model_optimizer
    EVALUATION_AVAILABLE = True
except ImportError:
    EVALUATION_AVAILABLE = False

try:
    from ai.continuous_learning_system import continuous_learning_system
    CONTINUOUS_LEARNING_AVAILABLE = True
except ImportError:
    CONTINUOUS_LEARNING_AVAILABLE = False

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/ai_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AITrainingPipeline:
    """AIè®­ç»ƒç®¡é“"""
    
    def __init__(self, config: dict = None):
        self.db_manager = DatabaseManager()
        self.config = self._load_config(config)
        self.results = {}
        self.training_start_time = None
        self.training_end_time = None
        
        # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
    def _load_config(self, config: dict = None) -> dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            # æ•°æ®é…ç½®
            'stock_limit': 200,
            'days_back': 150,
            'target_columns': ['price_direction_5d', 'price_direction_10d'],
            
            # è®­ç»ƒé…ç½®
            'train_traditional': True,
            'train_deep_learning': True,
            'train_multimodal': True,
            
            # æ·±åº¦å­¦ä¹ é…ç½®
            'dl_epochs': 50,
            'dl_batch_size': 64,
            'dl_learning_rate': 0.001,
            
            # è¯„ä¼°é…ç½®
            'enable_evaluation': True,
            'evaluation_days': 5,
            
            # ä¼˜åŒ–é…ç½®
            'enable_optimization': True,
            'optimization_trials': 30,
            
            # æŒç»­å­¦ä¹ é…ç½®
            'enable_continuous_learning': True,
            'start_continuous_learning': False,
            
            # è¾“å‡ºé…ç½®
            'save_models': True,
            'generate_report': True,
            'report_format': 'html',
            
            # å…¶ä»–é…ç½®
            'random_seed': 42,
            'verbose': True
        }
        
        if config:
            default_config.update(config)
            
        return default_config
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨"""
        directories = [
            'models',
            'models/traditional',
            'models/deep_learning', 
            'models/incremental',
            'models/evaluation',
            'logs',
            'reports',
            'reports/training'
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def run_full_pipeline(self) -> dict:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒç®¡é“"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒAIå¤§æ¨¡å‹è®­ç»ƒç®¡é“")
        self.training_start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1: ç¯å¢ƒæ£€æŸ¥å’Œæ•°æ®å‡†å¤‡
            logger.info("ğŸ“‹ é˜¶æ®µ1: ç¯å¢ƒæ£€æŸ¥å’Œæ•°æ®å‡†å¤‡")
            env_check = self._check_environment()
            self.results['environment_check'] = env_check
            
            if not env_check['success']:
                return self._finalize_results(success=False, error="ç¯å¢ƒæ£€æŸ¥å¤±è´¥")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            data_prep = self._prepare_training_data()
            self.results['data_preparation'] = data_prep
            
            if not data_prep['success']:
                return self._finalize_results(success=False, error="æ•°æ®å‡†å¤‡å¤±è´¥")
            
            # é˜¶æ®µ2: ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
            if self.config['train_traditional'] and UNIFIED_TRAINER_AVAILABLE:
                logger.info("ğŸ¯ é˜¶æ®µ2: ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
                traditional_results = self._train_traditional_models()
                self.results['traditional_models'] = traditional_results
            
            # é˜¶æ®µ3: æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ
            if self.config['train_deep_learning'] and DEEP_LEARNING_AVAILABLE:
                logger.info("ğŸ§  é˜¶æ®µ3: æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
                dl_results = self._train_deep_learning_models()
                self.results['deep_learning_models'] = dl_results
            
            # é˜¶æ®µ4: æ¨¡å‹è¯„ä¼°
            if self.config['enable_evaluation'] and EVALUATION_AVAILABLE:
                logger.info("ğŸ“Š é˜¶æ®µ4: æ¨¡å‹è¯„ä¼°")
                eval_results = self._evaluate_models()
                self.results['evaluation'] = eval_results
            
            # é˜¶æ®µ5: æ¨¡å‹ä¼˜åŒ–
            if self.config['enable_optimization'] and EVALUATION_AVAILABLE:
                logger.info("âš¡ é˜¶æ®µ5: æ¨¡å‹ä¼˜åŒ–")
                opt_results = self._optimize_models()
                self.results['optimization'] = opt_results
            
            # é˜¶æ®µ6: æŒç»­å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–
            if self.config['enable_continuous_learning'] and CONTINUOUS_LEARNING_AVAILABLE:
                logger.info("ğŸ”„ é˜¶æ®µ6: æŒç»­å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–")
                cl_results = self._setup_continuous_learning()
                self.results['continuous_learning'] = cl_results
            
            # é˜¶æ®µ7: ç”ŸæˆæŠ¥å‘Š
            if self.config['generate_report']:
                logger.info("ğŸ“ é˜¶æ®µ7: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š")
                report_results = self._generate_comprehensive_report()
                self.results['report'] = report_results
            
            return self._finalize_results(success=True)
            
        except Exception as e:
            logger.error(f"è®­ç»ƒç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            return self._finalize_results(success=False, error=str(e))
    
    def _check_environment(self) -> dict:
        """æ£€æŸ¥ç¯å¢ƒå’Œä¾èµ–"""
        logger.info("æ£€æŸ¥ç¯å¢ƒå’Œä¾èµ–...")
        
        try:
            env_status = {
                'unified_trainer': UNIFIED_TRAINER_AVAILABLE,
                'deep_learning': DEEP_LEARNING_AVAILABLE,
                'evaluation_system': EVALUATION_AVAILABLE,
                'continuous_learning': CONTINUOUS_LEARNING_AVAILABLE,
                'database_connection': False,
                'feature_engineering': False
            }
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            try:
                test_query = self.db_manager.fetch_data("SELECT COUNT(*) as count FROM stock_basic LIMIT 1")
                env_status['database_connection'] = not test_query.empty
                stock_count = int(test_query.iloc[0]['count']) if not test_query.empty else 0
            except Exception as e:
                logger.error(f"æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
                stock_count = 0
            
            # æ£€æŸ¥ç‰¹å¾å·¥ç¨‹
            try:
                test_stocks = enhanced_feature_engineer.base_engineer.get_stock_list_for_training(5)
                env_status['feature_engineering'] = len(test_stocks) > 0
            except Exception as e:
                logger.error(f"ç‰¹å¾å·¥ç¨‹æ£€æŸ¥å¤±è´¥: {e}")
            
            # è¯„ä¼°æ•´ä½“çŠ¶æ€
            critical_components = ['database_connection', 'feature_engineering']
            success = all(env_status[comp] for comp in critical_components)
            
            available_features = sum(1 for available in env_status.values() if available)
            
            result = {
                'success': success,
                'environment_status': env_status,
                'stock_count': stock_count,
                'available_features': available_features,
                'total_features': len(env_status),
                'check_time': datetime.now().isoformat()
            }
            
            if success:
                logger.info(f"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ ({available_features}/{len(env_status)} åŠŸèƒ½å¯ç”¨)")
            else:
                logger.error(f"âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œå…³é”®ç»„ä»¶ä¸å¯ç”¨")
            
            return result
            
        except Exception as e:
            logger.error(f"ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _prepare_training_data(self) -> dict:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info(f"å‡†å¤‡è®­ç»ƒæ•°æ®: {self.config['stock_limit']}åªè‚¡ç¥¨, {self.config['days_back']}å¤©å†å²æ•°æ®")
        
        try:
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = enhanced_feature_engineer.base_engineer.get_stock_list_for_training(
                self.config['stock_limit']
            )
            
            if not stock_list:
                return {'success': False, 'error': 'æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨'}
            
            # è®¾ç½®æ—¥æœŸèŒƒå›´
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=self.config['days_back'])).strftime('%Y%m%d')
            
            # å‡†å¤‡å¤šæ¨¡æ€ç‰¹å¾
            logger.info("æ­£åœ¨æ„å»ºå¤šæ¨¡æ€ç‰¹å¾...")
            multimodal_data = enhanced_feature_engineer.prepare_multimodal_features(
                stock_list, start_date, end_date
            )
            
            if not multimodal_data:
                return {'success': False, 'error': 'å¤šæ¨¡æ€ç‰¹å¾å‡†å¤‡å¤±è´¥'}
            
            # ç»Ÿè®¡ä¿¡æ¯
            tabular_features = multimodal_data.get('tabular_features', pd.DataFrame())
            sequence_features = multimodal_data.get('sequence_features', np.array([]))
            text_features = multimodal_data.get('text_features', [])
            
            data_stats = {
                'stock_count': len(stock_list),
                'date_range': f"{start_date} - {end_date}",
                'tabular_samples': len(tabular_features) if not tabular_features.empty else 0,
                'tabular_features': len(tabular_features.columns) if not tabular_features.empty else 0,
                'sequence_samples': sequence_features.shape[0] if sequence_features.size > 0 else 0,
                'sequence_length': sequence_features.shape[1] if sequence_features.size > 0 else 0,
                'sequence_features': sequence_features.shape[2] if sequence_features.size > 0 else 0,
                'text_samples': len(text_features),
                'feature_categories': multimodal_data.get('feature_categories', {}),
                'valid_targets': 0
            }
            
            # æ£€æŸ¥ç›®æ ‡å˜é‡
            valid_targets = 0
            for target_col in self.config['target_columns']:
                if target_col in tabular_features.columns:
                    valid_count = tabular_features[target_col].notna().sum()
                    valid_targets += valid_count
                    data_stats[f'{target_col}_valid_samples'] = valid_count
            
            data_stats['valid_targets'] = valid_targets
            
            result = {
                'success': valid_targets > 100,  # è‡³å°‘éœ€è¦100ä¸ªæœ‰æ•ˆç›®æ ‡
                'stock_list': stock_list,
                'multimodal_data': multimodal_data,
                'data_statistics': data_stats,
                'preparation_time': datetime.now().isoformat()
            }
            
            if result['success']:
                logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {data_stats['tabular_samples']}ä¸ªæ ·æœ¬, "
                           f"{data_stats['tabular_features']}ç»´ç‰¹å¾")
            else:
                logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: æœ‰æ•ˆç›®æ ‡æ ·æœ¬ä¸è¶³({valid_targets})")
            
            return result
            
        except Exception as e:
            logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _train_traditional_models(self) -> dict:
        """è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        try:
            training_results = unified_trainer.train_all_models(
                stock_limit=self.config['stock_limit'],
                days_back=self.config['days_back'],
                train_traditional=True,
                train_deep=False,
                epochs=30
            )
            
            if not training_results:
                return {'success': False, 'error': 'ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥'}
            
            # ç»Ÿè®¡è®­ç»ƒç»“æœ
            trained_models = []
            performance_summary = {}
            
            for model_name, performance in training_results.items():
                if isinstance(performance, dict) and 'accuracy' in performance:
                    trained_models.append(model_name)
                    performance_summary[model_name] = {
                        'accuracy': performance.get('accuracy', 0),
                        'precision': performance.get('precision', 0),
                        'recall': performance.get('recall', 0),
                        'f1_score': performance.get('f1_score', 0)
                    }
            
            result = {
                'success': len(trained_models) > 0,
                'trained_models': trained_models,
                'model_count': len(trained_models),
                'performance_summary': performance_summary,
                'best_model': max(performance_summary.keys(), 
                                key=lambda x: performance_summary[x]['accuracy']) if performance_summary else None,
                'training_time': datetime.now().isoformat()
            }
            
            if result['success']:
                best_acc = max(p['accuracy'] for p in performance_summary.values()) if performance_summary else 0
                logger.info(f"âœ… ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå®Œæˆ: {len(trained_models)}ä¸ªæ¨¡å‹, æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
            else:
                logger.error("âŒ ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥")
            
            return result
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _train_deep_learning_models(self) -> dict:
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        try:
            # è·å–æ•°æ®
            data_prep = self.results.get('data_preparation', {})
            if not data_prep.get('success', False):
                return {'success': False, 'error': 'ç¼ºå°‘è®­ç»ƒæ•°æ®'}
            
            stock_list = data_prep['stock_list']
            
            # è®¾ç½®æ—¥æœŸèŒƒå›´
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=self.config['days_back'])).strftime('%Y%m%d')
            
            # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
            dl_results = deep_learning_trainer.train_all_models(
                stock_list, start_date, end_date, 'price_direction_5d'
            )
            
            if 'error' in dl_results:
                return {'success': False, 'error': dl_results['error']}
            
            # å¤„ç†ç»“æœ
            trained_models = []
            performance_summary = {}
            
            for model_name, result in dl_results.get('results', {}).items():
                performance = result.get('performance', {})
                if performance:
                    trained_models.append(model_name)
                    performance_summary[model_name] = performance
            
            result = {
                'success': len(trained_models) > 0,
                'trained_models': trained_models,
                'model_count': len(trained_models),
                'performance_summary': performance_summary,
                'training_info': dl_results.get('training_info', {}),
                'best_model': max(performance_summary.keys(),
                                key=lambda x: performance_summary[x].get('accuracy', 0)) if performance_summary else None,
                'training_time': datetime.now().isoformat()
            }
            
            if result['success']:
                best_acc = max(p.get('accuracy', 0) for p in performance_summary.values()) if performance_summary else 0
                logger.info(f"âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ: {len(trained_models)}ä¸ªæ¨¡å‹, æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
            else:
                logger.error("âŒ æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå¤±è´¥")
            
            return result
            
        except Exception as e:
            logger.error(f"æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _evaluate_models(self) -> dict:
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("å¼€å§‹è¯„ä¼°æ¨¡å‹...")
        
        try:
            # è¿è¡Œé¢„æµ‹è¯„ä¼°
            evaluation_result = evaluation_system.evaluate_predictions(
                days_back=self.config['evaluation_days']
            )
            
            if 'error' in evaluation_result:
                return {'success': False, 'error': evaluation_result['error']}
            
            # è·å–æ€§èƒ½æŠ¥å‘Š
            performance_report = evaluation_system.get_model_performance_report(days_back=30)
            
            # è·å–åé¦ˆç»Ÿè®¡
            feedback_stats = evaluation_system.get_feedback_statistics(days_back=30)
            
            result = {
                'success': True,
                'evaluation_result': evaluation_result,
                'performance_report': performance_report,
                'feedback_statistics': feedback_stats,
                'evaluated_predictions': evaluation_result.get('evaluated_count', 0),
                'models_evaluated': len(evaluation_result.get('results', {})),
                'evaluation_time': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ: è¯„ä¼°äº†{result['evaluated_predictions']}ä¸ªé¢„æµ‹, "
                       f"{result['models_evaluated']}ä¸ªæ¨¡å‹")
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _optimize_models(self) -> dict:
        """ä¼˜åŒ–æ¨¡å‹"""
        logger.info("å¼€å§‹ä¼˜åŒ–æ¨¡å‹...")
        
        try:
            # åˆ†ææ¨¡å‹æ€§èƒ½
            analysis_result = model_optimizer.analyze_model_performance(days_back=30)
            
            if 'error' in analysis_result:
                return {'success': False, 'error': analysis_result['error']}
            
            optimization_actions = []
            optimization_results = {}
            
            # è·å–éœ€è¦ä¼˜åŒ–çš„æ¨¡å‹
            models_analyzed = analysis_result.get('models_analyzed', {})
            high_priority_models = [
                model_name for model_name, data in models_analyzed.items()
                if data.get('priority') == 'high'
            ]
            
            # å¯¹é«˜ä¼˜å…ˆçº§æ¨¡å‹è¿›è¡Œä¼˜åŒ–
            for model_name in high_priority_models[:3]:  # é™åˆ¶ä¼˜åŒ–æ•°é‡
                try:
                    opt_result = model_optimizer.optimize_model_parameters(
                        model_name, 'hyperparameter'
                    )
                    
                    if opt_result.get('success', False):
                        optimization_actions.append(f"ä¼˜åŒ–äº†æ¨¡å‹ {model_name} çš„è¶…å‚æ•°")
                        optimization_results[model_name] = opt_result
                    
                except Exception as e:
                    logger.warning(f"ä¼˜åŒ–æ¨¡å‹ {model_name} å¤±è´¥: {e}")
            
            # æ‰§è¡Œè‡ªåŠ¨ä¼˜åŒ–æ£€æŸ¥
            auto_opt_result = model_optimizer.auto_optimization_check()
            optimization_actions.extend(auto_opt_result.get('actions_taken', []))
            
            result = {
                'success': True,
                'analysis_result': analysis_result,
                'optimization_actions': optimization_actions,
                'optimization_results': optimization_results,
                'auto_optimization': auto_opt_result,
                'models_optimized': len(optimization_results),
                'optimization_time': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ: æ‰§è¡Œäº†{len(optimization_actions)}ä¸ªä¼˜åŒ–åŠ¨ä½œ")
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _setup_continuous_learning(self) -> dict:
        """è®¾ç½®æŒç»­å­¦ä¹ ç³»ç»Ÿ"""
        logger.info("åˆå§‹åŒ–æŒç»­å­¦ä¹ ç³»ç»Ÿ...")
        
        try:
            # åˆå§‹åŒ–å¢é‡å­¦ä¹ æ¨¡å‹
            setup_results = []
            
            # ä¸ºä¸»è¦æ¨¡å‹ç±»å‹è®¾ç½®å¢é‡å­¦ä¹ 
            model_types = ['sgd', 'random_forest']
            for i, model_type in enumerate(model_types):
                model_name = f"incremental_{model_type}"
                success = continuous_learning_system.incremental_learner.initialize_incremental_model(
                    model_name, model_type
                )
                
                if success:
                    setup_results.append(f"åˆå§‹åŒ–å¢é‡å­¦ä¹ æ¨¡å‹: {model_name}")
                
            # è·å–æŒç»­å­¦ä¹ çŠ¶æ€
            learning_status = continuous_learning_system.get_learning_status()
            
            # å¦‚æœé…ç½®è¦æ±‚ï¼Œå¯åŠ¨æŒç»­å­¦ä¹ 
            if self.config.get('start_continuous_learning', False):
                continuous_learning_system.start_continuous_learning()
                setup_results.append("å¯åŠ¨äº†æŒç»­å­¦ä¹ è°ƒåº¦å™¨")
            
            result = {
                'success': len(setup_results) > 0,
                'setup_actions': setup_results,
                'learning_status': learning_status,
                'continuous_learning_active': continuous_learning_system.learning_scheduler_running,
                'setup_time': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æŒç»­å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: {len(setup_results)}ä¸ªåŠ¨ä½œ")
            
            return result
            
        except Exception as e:
            logger.error(f"æŒç»­å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_comprehensive_report(self) -> dict:
        """ç”Ÿæˆç»¼åˆè®­ç»ƒæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆè®­ç»ƒæŠ¥å‘Š...")
        
        try:
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            report_data = {
                'training_summary': self._create_training_summary(),
                'environment_info': self.results.get('environment_check', {}),
                'data_preparation': self.results.get('data_preparation', {}),
                'traditional_models': self.results.get('traditional_models', {}),
                'deep_learning_models': self.results.get('deep_learning_models', {}),
                'evaluation_results': self.results.get('evaluation', {}),
                'optimization_results': self.results.get('optimization', {}),
                'continuous_learning': self.results.get('continuous_learning', {}),
                'generation_time': datetime.now().isoformat()
            }
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
            report_filename = f"ai_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # JSONæ ¼å¼æŠ¥å‘Š
            json_path = Path(f"reports/training/{report_filename}.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
            
            # HTMLæ ¼å¼æŠ¥å‘Š
            if self.config['report_format'] == 'html':
                html_path = Path(f"reports/training/{report_filename}.html")
                html_content = self._generate_html_report(report_data)
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
            
            result = {
                'success': True,
                'report_files': {
                    'json': str(json_path),
                    'html': str(html_path) if self.config['report_format'] == 'html' else None
                },
                'report_data': report_data,
                'generation_time': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {json_path}")
            
            return result
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _create_training_summary(self) -> dict:
        """åˆ›å»ºè®­ç»ƒæ‘˜è¦"""
        try:
            summary = {
                'training_duration': None,
                'total_models_trained': 0,
                'best_overall_accuracy': 0,
                'best_model': None,
                'data_samples': 0,
                'feature_dimensions': 0,
                'evaluation_performed': False,
                'optimization_performed': False,
                'continuous_learning_enabled': False
            }
            
            if self.training_start_time and self.training_end_time:
                duration = self.training_end_time - self.training_start_time
                summary['training_duration'] = str(duration)
            
            # ç»Ÿè®¡ä¼ ç»Ÿæ¨¡å‹
            traditional = self.results.get('traditional_models', {})
            if traditional.get('success'):
                summary['total_models_trained'] += traditional.get('model_count', 0)
                
                # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                for model_name, perf in traditional.get('performance_summary', {}).items():
                    accuracy = perf.get('accuracy', 0)
                    if accuracy > summary['best_overall_accuracy']:
                        summary['best_overall_accuracy'] = accuracy
                        summary['best_model'] = f"traditional_{model_name}"
            
            # ç»Ÿè®¡æ·±åº¦å­¦ä¹ æ¨¡å‹
            deep_learning = self.results.get('deep_learning_models', {})
            if deep_learning.get('success'):
                summary['total_models_trained'] += deep_learning.get('model_count', 0)
                
                # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                for model_name, perf in deep_learning.get('performance_summary', {}).items():
                    accuracy = perf.get('accuracy', 0)
                    if accuracy > summary['best_overall_accuracy']:
                        summary['best_overall_accuracy'] = accuracy
                        summary['best_model'] = f"deep_learning_{model_name}"
            
            # æ•°æ®ç»Ÿè®¡
            data_prep = self.results.get('data_preparation', {})
            if data_prep.get('success'):
                data_stats = data_prep.get('data_statistics', {})
                summary['data_samples'] = data_stats.get('tabular_samples', 0)
                summary['feature_dimensions'] = data_stats.get('tabular_features', 0)
            
            # å…¶ä»–çŠ¶æ€
            summary['evaluation_performed'] = self.results.get('evaluation', {}).get('success', False)
            summary['optimization_performed'] = self.results.get('optimization', {}).get('success', False)
            summary['continuous_learning_enabled'] = self.results.get('continuous_learning', {}).get('success', False)
            
            return summary
            
        except Exception as e:
            logger.error(f"åˆ›å»ºè®­ç»ƒæ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    def _generate_html_report(self, report_data: dict) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        try:
            training_summary = report_data.get('training_summary', {})
            
            html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIå¤§æ¨¡å‹è®­ç»ƒæŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 40px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; border-bottom: 3px solid #007bff; padding-bottom: 20px; margin-bottom: 30px; }}
        .section {{ margin: 30px 0; padding: 20px; border-left: 4px solid #007bff; background: #f8f9fa; }}
        .metrics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric {{ background: white; padding: 20px; border-radius: 8px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
        .metric-value {{ font-size: 2em; font-weight: bold; color: #007bff; }}
        .metric-label {{ color: #666; margin-top: 5px; }}
        .success {{ color: #28a745; }}
        .warning {{ color: #ffc107; }}
        .error {{ color: #dc3545; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #007bff; color: white; }}
        .status-badge {{ padding: 4px 8px; border-radius: 4px; color: white; font-size: 0.8em; }}
        .status-success {{ background-color: #28a745; }}
        .status-error {{ background-color: #dc3545; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸš€ AIå¤§æ¨¡å‹è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ</h1>
            <h2>è®­ç»ƒæŠ¥å‘Š</h2>
            <p>ç”Ÿæˆæ—¶é—´: {report_data['generation_time']}</p>
        </div>
        
        <div class="section">
            <h3>ğŸ“Š è®­ç»ƒæ‘˜è¦</h3>
            <div class="metrics">
                <div class="metric">
                    <div class="metric-value">{training_summary.get('total_models_trained', 0)}</div>
                    <div class="metric-label">è®­ç»ƒæ¨¡å‹æ•°é‡</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{training_summary.get('best_overall_accuracy', 0):.4f}</div>
                    <div class="metric-label">æœ€ä½³å‡†ç¡®ç‡</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{training_summary.get('data_samples', 0):,}</div>
                    <div class="metric-label">è®­ç»ƒæ ·æœ¬æ•°</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{training_summary.get('feature_dimensions', 0)}</div>
                    <div class="metric-label">ç‰¹å¾ç»´åº¦</div>
                </div>
            </div>
            <p><strong>æœ€ä½³æ¨¡å‹:</strong> {training_summary.get('best_model', 'N/A')}</p>
            <p><strong>è®­ç»ƒæ—¶é•¿:</strong> {training_summary.get('training_duration', 'N/A')}</p>
        </div>
        
        <div class="section">
            <h3>ğŸ”§ ç¯å¢ƒçŠ¶æ€</h3>
            {self._format_environment_status(report_data.get('environment_info', {}))}
        </div>
        
        <div class="section">
            <h3>ğŸ¯ ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹</h3>
            {self._format_model_results(report_data.get('traditional_models', {}))}
        </div>
        
        <div class="section">
            <h3>ğŸ§  æ·±åº¦å­¦ä¹ æ¨¡å‹</h3>
            {self._format_model_results(report_data.get('deep_learning_models', {}))}
        </div>
        
        <div class="section">
            <h3>ğŸ“ˆ æ¨¡å‹è¯„ä¼°ç»“æœ</h3>
            {self._format_evaluation_results(report_data.get('evaluation_results', {}))}
        </div>
        
        <div class="section">
            <h3>âš¡ æ¨¡å‹ä¼˜åŒ–ç»“æœ</h3>
            {self._format_optimization_results(report_data.get('optimization_results', {}))}
        </div>
        
        <div class="section">
            <h3>ğŸ”„ æŒç»­å­¦ä¹ ç³»ç»Ÿ</h3>
            {self._format_continuous_learning(report_data.get('continuous_learning', {}))}
        </div>
        
        <div class="section">
            <h3>ğŸ’¡ å»ºè®®å’Œä¸‹ä¸€æ­¥</h3>
            {self._format_recommendations(report_data)}
        </div>
    </div>
</body>
</html>
"""
            return html_content
            
        except Exception as e:
            logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")
            return f"<html><body><h1>æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}</h1></body></html>"
    
    def _format_environment_status(self, env_info: dict) -> str:
        """æ ¼å¼åŒ–ç¯å¢ƒçŠ¶æ€"""
        if not env_info.get('success'):
            return '<p class="error">ç¯å¢ƒæ£€æŸ¥å¤±è´¥</p>'
        
        env_status = env_info.get('environment_status', {})
        status_items = []
        
        for component, available in env_status.items():
            status_class = 'status-success' if available else 'status-error'
            status_text = 'å¯ç”¨' if available else 'ä¸å¯ç”¨'
            status_items.append(f'<span class="status-badge {status_class}">{component}: {status_text}</span>')
        
        return f'<p>{" ".join(status_items)}</p>'
    
    def _format_model_results(self, model_results: dict) -> str:
        """æ ¼å¼åŒ–æ¨¡å‹ç»“æœ"""
        if not model_results.get('success'):
            return '<p class="error">æ¨¡å‹è®­ç»ƒå¤±è´¥</p>'
        
        performance = model_results.get('performance_summary', {})
        if not performance:
            return '<p>æ— æ€§èƒ½æ•°æ®</p>'
        
        table_rows = []
        for model_name, metrics in performance.items():
            accuracy = metrics.get('accuracy', 0)
            precision = metrics.get('precision', 0)
            recall = metrics.get('recall', 0)
            f1_score = metrics.get('f1_score', 0)
            
            table_rows.append(f"""
                <tr>
                    <td>{model_name}</td>
                    <td>{accuracy:.4f}</td>
                    <td>{precision:.4f}</td>
                    <td>{recall:.4f}</td>
                    <td>{f1_score:.4f}</td>
                </tr>
            """)
        
        return f"""
            <table>
                <thead>
                    <tr><th>æ¨¡å‹</th><th>å‡†ç¡®ç‡</th><th>ç²¾ç¡®ç‡</th><th>å¬å›ç‡</th><th>F1åˆ†æ•°</th></tr>
                </thead>
                <tbody>{''.join(table_rows)}</tbody>
            </table>
        """
    
    def _format_evaluation_results(self, eval_results: dict) -> str:
        """æ ¼å¼åŒ–è¯„ä¼°ç»“æœ"""
        if not eval_results.get('success'):
            return '<p class="error">æ¨¡å‹è¯„ä¼°å¤±è´¥</p>'
        
        evaluated_count = eval_results.get('evaluated_predictions', 0)
        models_count = eval_results.get('models_evaluated', 0)
        
        return f"""
            <p>è¯„ä¼°äº† <strong>{evaluated_count}</strong> ä¸ªé¢„æµ‹ç»“æœ</p>
            <p>æ¶‰åŠ <strong>{models_count}</strong> ä¸ªæ¨¡å‹</p>
        """
    
    def _format_optimization_results(self, opt_results: dict) -> str:
        """æ ¼å¼åŒ–ä¼˜åŒ–ç»“æœ"""
        if not opt_results.get('success'):
            return '<p class="error">æ¨¡å‹ä¼˜åŒ–å¤±è´¥</p>'
        
        actions = opt_results.get('optimization_actions', [])
        if not actions:
            return '<p>æœªæ‰§è¡Œä¼˜åŒ–åŠ¨ä½œ</p>'
        
        action_list = '</li><li>'.join(actions)
        return f'<ul><li>{action_list}</li></ul>'
    
    def _format_continuous_learning(self, cl_results: dict) -> str:
        """æ ¼å¼åŒ–æŒç»­å­¦ä¹ ç»“æœ"""
        if not cl_results.get('success'):
            return '<p class="error">æŒç»­å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥</p>'
        
        setup_actions = cl_results.get('setup_actions', [])
        is_active = cl_results.get('continuous_learning_active', False)
        
        status_text = 'è¿è¡Œä¸­' if is_active else 'å·²åˆå§‹åŒ–ä½†æœªå¯åŠ¨'
        status_class = 'success' if is_active else 'warning'
        
        action_list = '</li><li>'.join(setup_actions) if setup_actions else 'æ— '
        
        return f"""
            <p class="{status_class}">çŠ¶æ€: {status_text}</p>
            <p>åˆå§‹åŒ–åŠ¨ä½œ:</p>
            <ul><li>{action_list}</li></ul>
        """
    
    def _format_recommendations(self, report_data: dict) -> str:
        """æ ¼å¼åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºè®­ç»ƒç»“æœçš„å»ºè®®
        training_summary = report_data.get('training_summary', {})
        best_accuracy = training_summary.get('best_overall_accuracy', 0)
        
        if best_accuracy < 0.6:
            recommendations.append("æ¨¡å‹å‡†ç¡®ç‡åä½ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®æˆ–ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹")
        
        if training_summary.get('total_models_trained', 0) < 3:
            recommendations.append("è®­ç»ƒçš„æ¨¡å‹æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¯ç”¨æ›´å¤šæ¨¡å‹ç±»å‹")
        
        if not training_summary.get('continuous_learning_enabled', False):
            recommendations.append("å»ºè®®å¯ç”¨æŒç»­å­¦ä¹ ç³»ç»Ÿä»¥ä¿æŒæ¨¡å‹æ€§èƒ½")
        
        # åŸºäºè¯„ä¼°ç»“æœçš„å»ºè®®
        eval_results = report_data.get('evaluation_results', {})
        if eval_results.get('evaluated_predictions', 0) < 100:
            recommendations.append("è¯„ä¼°æ ·æœ¬è¾ƒå°‘ï¼Œå»ºè®®ç§¯ç´¯æ›´å¤šé¢„æµ‹ç»“æœåå†è¯„ä¼°")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®å®šæœŸç›‘æ§æ¨¡å‹æ€§èƒ½")
        
        rec_list = '</li><li>'.join(recommendations)
        return f'<ul><li>{rec_list}</li></ul>'
    
    def _finalize_results(self, success: bool, error: str = None) -> dict:
        """å®Œæˆå¹¶è¿”å›æœ€ç»ˆç»“æœ"""
        self.training_end_time = datetime.now()
        
        if self.training_start_time:
            duration = self.training_end_time - self.training_start_time
        else:
            duration = timedelta(0)
        
        final_results = {
            'success': success,
            'error': error,
            'training_duration': str(duration),
            'start_time': self.training_start_time.isoformat() if self.training_start_time else None,
            'end_time': self.training_end_time.isoformat(),
            'config_used': self.config,
            'detailed_results': self.results
        }
        
        if success:
            logger.info(f"ğŸ‰ AIè®­ç»ƒç®¡é“æ‰§è¡Œå®Œæˆï¼æ€»è€—æ—¶: {duration}")
        else:
            logger.error(f"âŒ AIè®­ç»ƒç®¡é“æ‰§è¡Œå¤±è´¥: {error}")
        
        return final_results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='AIå¤§æ¨¡å‹è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿè®­ç»ƒ')
    parser.add_argument('--stock-limit', type=int, default=200, help='è®­ç»ƒè‚¡ç¥¨æ•°é‡')
    parser.add_argument('--days-back', type=int, default=150, help='å†å²æ•°æ®å¤©æ•°')
    parser.add_argument('--no-traditional', action='store_true', help='è·³è¿‡ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--no-deep', action='store_true', help='è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--no-eval', action='store_true', help='è·³è¿‡æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--no-opt', action='store_true', help='è·³è¿‡æ¨¡å‹ä¼˜åŒ–')
    parser.add_argument('--start-continuous', action='store_true', help='å¯åŠ¨æŒç»­å­¦ä¹ ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºé…ç½®
    config = {
        'stock_limit': args.stock_limit,
        'days_back': args.days_back,
        'train_traditional': not args.no_traditional,
        'train_deep_learning': not args.no_deep,
        'enable_evaluation': not args.no_eval,
        'enable_optimization': not args.no_opt,
        'start_continuous_learning': args.start_continuous,
    }
    
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å¹¶åˆå¹¶
    if args.config and os.path.exists(args.config):
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                file_config = json.load(f)
            config.update(file_config)
            logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒç®¡é“
    logger.info("å¯åŠ¨AIå¤§æ¨¡å‹è®­ç»ƒç³»ç»Ÿ...")
    pipeline = AITrainingPipeline(config)
    
    try:
        results = pipeline.run_full_pipeline()
        
        # è¾“å‡ºç®€è¦ç»“æœ
        print("\n" + "="*60)
        print("ğŸ¯ AIå¤§æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*60)
        
        if results['success']:
            print("âœ… è®­ç»ƒçŠ¶æ€: æˆåŠŸ")
            print(f"â±ï¸  æ€»è€—æ—¶: {results['training_duration']}")
            
            # æ˜¾ç¤ºè®­ç»ƒæ‘˜è¦
            detailed_results = results.get('detailed_results', {})
            
            # ä¼ ç»Ÿæ¨¡å‹
            traditional = detailed_results.get('traditional_models', {})
            if traditional.get('success'):
                print(f"ğŸ¯ ä¼ ç»Ÿæ¨¡å‹: {traditional.get('model_count', 0)}ä¸ª")
            
            # æ·±åº¦å­¦ä¹ æ¨¡å‹
            deep_learning = detailed_results.get('deep_learning_models', {})
            if deep_learning.get('success'):
                print(f"ğŸ§  æ·±åº¦å­¦ä¹ æ¨¡å‹: {deep_learning.get('model_count', 0)}ä¸ª")
            
            # è¯„ä¼°ç»“æœ
            evaluation = detailed_results.get('evaluation', {})
            if evaluation.get('success'):
                print(f"ğŸ“Š è¯„ä¼°é¢„æµ‹: {evaluation.get('evaluated_predictions', 0)}ä¸ª")
            
            # ä¼˜åŒ–ç»“æœ
            optimization = detailed_results.get('optimization', {})
            if optimization.get('success'):
                opt_actions = len(optimization.get('optimization_actions', []))
                print(f"âš¡ ä¼˜åŒ–åŠ¨ä½œ: {opt_actions}ä¸ª")
            
            # æŠ¥å‘Šæ–‡ä»¶
            report = detailed_results.get('report', {})
            if report.get('success'):
                json_file = report.get('report_files', {}).get('json')
                html_file = report.get('report_files', {}).get('html')
                print(f"ğŸ“ JSONæŠ¥å‘Š: {json_file}")
                if html_file:
                    print(f"ğŸ“„ HTMLæŠ¥å‘Š: {html_file}")
        else:
            print("âŒ è®­ç»ƒçŠ¶æ€: å¤±è´¥")
            print(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {results['error']}")
        
        print("="*60)
        
        return 0 if results['success'] else 1
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        return 130
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    exit(main())