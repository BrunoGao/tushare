#!/usr/bin/env python3
"""
å¼‚æ­¥è·å–è‚¡ç¥¨å…¨é‡æ•°æ®è„šæœ¬
æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ã€æ–­ç‚¹ç»­ä¼ ã€è¿›åº¦ç›‘æ§
"""
import asyncio
import aiohttp
import logging
import tushare as ts
import pandas as pd
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from database.db_manager import DatabaseManager
import config
import json
import os
import time
from typing import List, Dict, Tuple
from tqdm import tqdm
import threading
from queue import Queue

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/fetch_full_data.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AsyncStockDataFetcher:
    def __init__(self, max_workers=10, batch_size=50):
        """
        åˆå§‹åŒ–å¼‚æ­¥æ•°æ®è·å–å™¨
        :param max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        :param batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        """
        ts.set_token(config.TS_TOKEN)
        self.pro = ts.pro_api()
        self.db_manager = DatabaseManager()
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress_file = 'logs/fetch_progress.json'
        self.completed_stocks = set()
        self.failed_stocks = set()
        self.total_stocks = 0
        
        # é¢‘ç‡æ§åˆ¶
        self.request_lock = threading.Lock()
        self.request_count = 0
        self.last_minute = datetime.now().minute
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_records': 0,
            'success_stocks': 0,
            'failed_stocks': 0,
            'start_time': None,
            'tables_created': set()
        }
        
        self.load_progress()

    def load_progress(self):
        """åŠ è½½è¿›åº¦æ–‡ä»¶"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r') as f:
                    progress = json.load(f)
                    self.completed_stocks = set(progress.get('completed', []))
                    self.failed_stocks = set(progress.get('failed', []))
                    self.stats.update(progress.get('stats', {}))
                logger.info(f"ğŸ“ åŠ è½½è¿›åº¦: å·²å®Œæˆ {len(self.completed_stocks)} åªè‚¡ç¥¨")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½è¿›åº¦æ–‡ä»¶: {e}")

    def save_progress(self):
        """ä¿å­˜è¿›åº¦æ–‡ä»¶"""
        try:
            progress = {
                'completed': list(self.completed_stocks),
                'failed': list(self.failed_stocks),
                'stats': self.stats,
                'last_update': datetime.now().isoformat()
            }
            os.makedirs('logs', exist_ok=True)
            with open(self.progress_file, 'w') as f:
                json.dump(progress, f, indent=2)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def rate_limit_control(self):
        """é¢‘ç‡æ§åˆ¶ - æ¯åˆ†é’Ÿæœ€å¤š500æ¬¡è¯·æ±‚"""
        with self.request_lock:
            current_minute = datetime.now().minute
            if current_minute != self.last_minute:
                self.request_count = 0
                self.last_minute = current_minute
                
            if self.request_count >= config.TS_RATE_LIMIT:
                sleep_time = 61 - datetime.now().second
                logger.info(f"â±ï¸ è¾¾åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time} ç§’...")
                time.sleep(sleep_time)
                self.request_count = 0
                self.last_minute = datetime.now().minute
                
            self.request_count += 1

    def create_monthly_table(self, year: int, month: int) -> str:
        """åˆ›å»ºæœˆåº¦åˆ†è¡¨"""
        table_name = f"stock_daily_{year:04d}{month:02d}"
        
        if table_name in self.stats['tables_created']:
            return table_name
            
        try:
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                id BIGINT AUTO_INCREMENT PRIMARY KEY,
                ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                trade_date DATE NOT NULL COMMENT 'äº¤æ˜“æ—¥æœŸ',
                open DECIMAL(10,3) COMMENT 'å¼€ç›˜ä»·',
                high DECIMAL(10,3) COMMENT 'æœ€é«˜ä»·',
                low DECIMAL(10,3) COMMENT 'æœ€ä½ä»·',
                close DECIMAL(10,3) COMMENT 'æ”¶ç›˜ä»·',
                pre_close DECIMAL(10,3) COMMENT 'æ˜¨æ”¶ä»·',
                `change` DECIMAL(10,3) COMMENT 'æ¶¨è·Œé¢',
                pct_chg DECIMAL(8,4) COMMENT 'æ¶¨è·Œå¹…',
                vol BIGINT COMMENT 'æˆäº¤é‡(æ‰‹)',
                amount DECIMAL(20,3) COMMENT 'æˆäº¤é¢(åƒå…ƒ)',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE KEY uk_code_date (ts_code, trade_date),
                INDEX idx_code (ts_code),
                INDEX idx_date (trade_date),
                INDEX idx_code_date (ts_code, trade_date)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='è‚¡ç¥¨æ—¥çº¿æ•°æ®è¡¨-{year:04d}{month:02d}'
            """
            
            self.db_manager.execute_sql(create_sql)
            self.stats['tables_created'].add(table_name)
            logger.debug(f"âœ… åˆ›å»ºè¡¨ {table_name}")
            return table_name
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {e}")
            return None

    def fetch_stock_daily_data(self, ts_code: str, start_date: str, end_date: str) -> Dict:
        """è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®"""
        try:
            self.rate_limit_control()
            
            # è·å–æ•°æ®
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                return {'success': False, 'ts_code': ts_code, 'error': 'æ— æ•°æ®', 'records': 0}
                
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df = df.sort_values('trade_date')
            
            # æŒ‰æœˆåˆ†ç»„æ’å…¥
            total_records = 0
            for (year, month), group in df.groupby([df['trade_date'].dt.year, df['trade_date'].dt.month]):
                table_name = self.create_monthly_table(year, month)
                if table_name:
                    success = self.db_manager.insert_dataframe(table_name, group, replace=False)
                    if success:
                        total_records += len(group)
                    
            return {
                'success': True, 
                'ts_code': ts_code, 
                'records': total_records,
                'date_range': f"{df['trade_date'].min().date()} ~ {df['trade_date'].max().date()}"
            }
            
        except Exception as e:
            return {'success': False, 'ts_code': ts_code, 'error': str(e), 'records': 0}

    def fetch_stock_batch(self, stock_batch: List[Tuple[str, str]]) -> List[Dict]:
        """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®"""
        results = []
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘3å¹´ï¼‰
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y%m%d')
        
        for ts_code, name in stock_batch:
            if ts_code in self.completed_stocks:
                continue
                
            result = self.fetch_stock_daily_data(ts_code, start_date, end_date)
            result['name'] = name
            results.append(result)
            
            if result['success']:
                self.completed_stocks.add(ts_code)
                self.stats['success_stocks'] += 1
                self.stats['total_records'] += result['records']
                logger.info(f"âœ… {ts_code} ({name}): {result['records']} æ¡è®°å½•")
            else:
                self.failed_stocks.add(ts_code)
                self.stats['failed_stocks'] += 1
                logger.error(f"âŒ {ts_code} ({name}): {result['error']}")
                
            # å°å»¶æ—¶é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(0.05)
            
        return results

    async def fetch_all_stocks_async(self):
        """å¼‚æ­¥è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹å¼‚æ­¥è·å–å…¨é‡è‚¡ç¥¨æ•°æ®...")
        self.stats['start_time'] = datetime.now().isoformat()
        
        try:
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            logger.info("ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨...")
            stock_df = self.db_manager.fetch_data("""
                SELECT ts_code, name FROM stock_basic 
                ORDER BY ts_code
            """)
            
            if stock_df.empty:
                logger.error("âŒ æœªæ‰¾åˆ°è‚¡ç¥¨æ•°æ®")
                return False
                
            # è¿‡æ»¤å·²å®Œæˆçš„è‚¡ç¥¨
            remaining_stocks = [
                (row['ts_code'], row['name']) 
                for _, row in stock_df.iterrows() 
                if row['ts_code'] not in self.completed_stocks
            ]
            
            self.total_stocks = len(stock_df)
            remaining_count = len(remaining_stocks)
            
            logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {self.total_stocks}")
            logger.info(f"ğŸ“Š å·²å®Œæˆ: {len(self.completed_stocks)}")
            logger.info(f"ğŸ“Š å¾…å¤„ç†: {remaining_count}")
            
            if remaining_count == 0:
                logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²è·å–å®Œæˆï¼")
                return True
                
            # åˆ†æ‰¹å¤„ç†
            batches = [
                remaining_stocks[i:i + self.batch_size] 
                for i in range(0, remaining_count, self.batch_size)
            ]
            
            logger.info(f"ğŸ”„ å°†åˆ† {len(batches)} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {self.batch_size} åªè‚¡ç¥¨")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
                future_to_batch = {
                    executor.submit(self.fetch_stock_batch, batch): i 
                    for i, batch in enumerate(batches)
                }
                
                # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                with tqdm(total=len(batches), desc="å¤„ç†æ‰¹æ¬¡") as pbar:
                    for future in as_completed(future_to_batch):
                        batch_idx = future_to_batch[future]
                        try:
                            results = future.result()
                            pbar.set_postfix({
                                'æˆåŠŸ': self.stats['success_stocks'],
                                'å¤±è´¥': self.stats['failed_stocks'],
                                'è®°å½•æ•°': self.stats['total_records']
                            })
                            pbar.update(1)
                            
                            # å®šæœŸä¿å­˜è¿›åº¦
                            if batch_idx % 10 == 0:
                                self.save_progress()
                                
                        except Exception as e:
                            logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                            pbar.update(1)
            
            # æœ€ç»ˆä¿å­˜è¿›åº¦
            self.save_progress()
            
            # ç»Ÿè®¡ç»“æœ
            end_time = datetime.now()
            duration = (end_time - datetime.fromisoformat(self.stats['start_time'])).total_seconds()
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ å…¨é‡æ•°æ®è·å–å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {self.total_stocks}")
            logger.info(f"âœ… æˆåŠŸè·å–: {self.stats['success_stocks']}")
            logger.info(f"âŒ è·å–å¤±è´¥: {self.stats['failed_stocks']}")
            logger.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {self.stats['total_records']:,}")
            logger.info(f"ğŸ•’ æ€»è€—æ—¶: {duration/60:.1f} åˆ†é’Ÿ")
            logger.info(f"ğŸ“‹ åˆ›å»ºè¡¨æ•°: {len(self.stats['tables_created'])}")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥è·å–å¤±è´¥: {e}")
            self.save_progress()
            return False

    def get_progress_stats(self) -> Dict:
        """è·å–è¿›åº¦ç»Ÿè®¡"""
        return {
            'total_stocks': self.total_stocks,
            'completed_stocks': len(self.completed_stocks),
            'failed_stocks': len(self.failed_stocks),
            'completion_rate': len(self.completed_stocks) / max(self.total_stocks, 1) * 100,
            'total_records': self.stats['total_records'],
            'tables_created': len(self.stats['tables_created'])
        }

async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¼‚æ­¥è‚¡ç¥¨å…¨é‡æ•°æ®è·å–ç¨‹åº")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®è·å–å™¨
    fetcher = AsyncStockDataFetcher(
        max_workers=8,  # å¹¶å‘çº¿ç¨‹æ•°
        batch_size=30   # æ¯æ‰¹è‚¡ç¥¨æ•°
    )
    
    # æ˜¾ç¤ºå½“å‰è¿›åº¦
    stats = fetcher.get_progress_stats()
    if stats['completed_stocks'] > 0:
        print(f"ğŸ“Š å½“å‰è¿›åº¦: {stats['completed_stocks']}/{stats['total_stocks']} "
              f"({stats['completion_rate']:.1f}%)")
        print(f"ğŸ“ˆ å·²è·å–è®°å½•: {stats['total_records']:,} æ¡")
        print(f"ğŸ“‹ å·²åˆ›å»ºè¡¨: {stats['tables_created']} ä¸ª")
        print("-" * 60)
    
    # å¼€å§‹è·å–æ•°æ®
    success = await fetcher.fetch_all_stocks_async()
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = fetcher.get_progress_stats()
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ å…¨é‡æ•°æ®è·å–æˆåŠŸå®Œæˆï¼")
    else:
        print("âš ï¸ æ•°æ®è·å–è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜")
    
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»è‚¡ç¥¨æ•°: {final_stats['total_stocks']}")
    print(f"  æˆåŠŸè·å–: {final_stats['completed_stocks']}")
    print(f"  è·å–å¤±è´¥: {final_stats['failed_stocks']}")
    print(f"  å®Œæˆç‡: {final_stats['completion_rate']:.1f}%")
    print(f"  æ€»è®°å½•æ•°: {final_stats['total_records']:,}")
    print("=" * 60)
    
    return 0 if success else 1

if __name__ == "__main__":
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    # è¿è¡Œå¼‚æ­¥ç¨‹åº
    exit_code = asyncio.run(main())
    exit(exit_code)