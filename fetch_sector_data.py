#!/usr/bin/env python3
"""
æ¿å—æ•°æ®æŠ“å–è„šæœ¬
åŒ…æ‹¬è¡Œä¸šåˆ†ç±»ã€æ¦‚å¿µæ¿å—ã€åœ°åŸŸåˆ†ç±»ç­‰æ•°æ®
"""

import os
import sys
import time
import json
import pandas as pd
import pymysql
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import tushare as ts

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import Config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/sector_data_fetch.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SectorDataFetcher:
    """æ¿å—æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.ts_pro = ts.pro_api(Config.TS_TOKEN)
        self.db_config = {
            'host': Config.DB_HOST,
            'port': Config.DB_PORT,
            'user': Config.DB_USER,
            'password': Config.DB_PASSWORD,
            'database': Config.DB_NAME,
            'charset': 'utf8mb4'
        }
        self.progress_file = 'logs/sector_fetch_progress.json'
        self.stats = {
            'start_time': datetime.now().isoformat(),
            'processed_tables': 0,
            'total_records': 0,
            'failed_operations': []
        }
        
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return pymysql.connect(**self.db_config)
    
    def create_tables(self):
        """åˆ›å»ºæ‰€éœ€çš„æ•°æ®è¡¨"""
        tables_sql = {
            'industry_classify': """
                CREATE TABLE IF NOT EXISTS industry_classify (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                    symbol VARCHAR(20) COMMENT 'è‚¡ç¥¨ç®€ç§°',
                    name VARCHAR(100) COMMENT 'è‚¡ç¥¨åç§°',
                    industry VARCHAR(100) COMMENT 'æ‰€å±è¡Œä¸š',
                    industry_code VARCHAR(20) COMMENT 'è¡Œä¸šä»£ç ',
                    level VARCHAR(10) COMMENT 'è¡Œä¸šçº§åˆ«',
                    src VARCHAR(10) COMMENT 'æ•°æ®æº',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_ts_code (ts_code),
                    INDEX idx_industry (industry),
                    INDEX idx_industry_code (industry_code)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='è¡Œä¸šåˆ†ç±»è¡¨'
            """,
            
            'concept_classify': """
                CREATE TABLE IF NOT EXISTS concept_classify (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                    name VARCHAR(100) COMMENT 'è‚¡ç¥¨åç§°',
                    concept_name VARCHAR(100) COMMENT 'æ¦‚å¿µåç§°',
                    concept_code VARCHAR(20) COMMENT 'æ¦‚å¿µä»£ç ',
                    src VARCHAR(10) COMMENT 'æ•°æ®æº',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_ts_code (ts_code),
                    INDEX idx_concept (concept_name),
                    INDEX idx_concept_code (concept_code)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ¦‚å¿µåˆ†ç±»è¡¨'
            """,
            
            'area_classify': """
                CREATE TABLE IF NOT EXISTS area_classify (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                    name VARCHAR(100) COMMENT 'è‚¡ç¥¨åç§°',
                    area VARCHAR(50) COMMENT 'æ‰€å±åœ°åŸŸ',
                    province VARCHAR(50) COMMENT 'çœä»½',
                    city VARCHAR(50) COMMENT 'åŸå¸‚',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_ts_code (ts_code),
                    INDEX idx_area (area),
                    INDEX idx_province (province)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='åœ°åŸŸåˆ†ç±»è¡¨'
            """,
            
            'market_segment': """
                CREATE TABLE IF NOT EXISTS market_segment (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                    name VARCHAR(100) COMMENT 'è‚¡ç¥¨åç§°',
                    market VARCHAR(20) COMMENT 'å¸‚åœºç±»å‹',
                    segment VARCHAR(50) COMMENT 'å¸‚åœºæ¿å—',
                    is_hs VARCHAR(1) COMMENT 'æ˜¯å¦æ²ªæ·±æ¸¯é€š',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_ts_code (ts_code),
                    INDEX idx_market (market),
                    INDEX idx_segment (segment)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å¸‚åœºæ¿å—è¡¨'
            """
        }
        
        conn = self.get_db_connection()
        try:
            with conn.cursor() as cursor:
                for table_name, sql in tables_sql.items():
                    cursor.execute(sql)
                    logger.info(f"âœ… åˆ›å»º/æ£€æŸ¥è¡¨: {table_name}")
                conn.commit()
                logger.info("ğŸ¯ æ‰€æœ‰æ¿å—ç›¸å…³è¡¨åˆ›å»ºå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def fetch_industry_classify(self):
        """æŠ“å–è¡Œä¸šåˆ†ç±»æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–è¡Œä¸šåˆ†ç±»æ•°æ®...")
        
        try:
            # è·å–è¡Œä¸šåˆ†ç±»
            df = self.ts_pro.stock_company(
                exchange='',
                fields='ts_code,chairman,manager,secretary,reg_capital,setup_date,province'
            )
            
            if df.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°è¡Œä¸šåˆ†ç±»æ•°æ®")
                return 0
            
            # è·å–è¯¦ç»†è¡Œä¸šä¿¡æ¯
            industry_df = self.ts_pro.hs_const(hs_type='A')
            
            conn = self.get_db_connection()
            try:
                with conn.cursor() as cursor:
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM industry_classify")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO industry_classify 
                        (ts_code, symbol, name, industry, industry_code, level, src)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                    """
                    
                    records = []
                    for _, row in industry_df.iterrows():
                        records.append((
                            row.get('ts_code', ''),
                            row.get('symbol', ''),
                            row.get('name', ''),
                            row.get('industry', ''),
                            row.get('industry_code', ''),
                            row.get('level', ''),
                            'tushare'
                        ))
                    
                    cursor.executemany(insert_sql, records)
                    conn.commit()
                    
                    record_count = len(records)
                    self.stats['total_records'] += record_count
                    logger.info(f"âœ… è¡Œä¸šåˆ†ç±»æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                    return record_count
                    
            except Exception as e:
                logger.error(f"âŒ è¡Œä¸šåˆ†ç±»æ•°æ®æ’å…¥å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–è¡Œä¸šåˆ†ç±»æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_industry_classify',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def fetch_concept_classify(self):
        """æŠ“å–æ¦‚å¿µåˆ†ç±»æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–æ¦‚å¿µåˆ†ç±»æ•°æ®...")
        
        try:
            # è·å–æ¦‚å¿µåˆ†ç±»
            df = self.ts_pro.concept()
            
            if df.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°æ¦‚å¿µåˆ†ç±»æ•°æ®")
                return 0
            
            # è·å–æ¦‚å¿µæˆåˆ†è‚¡
            concept_detail_records = []
            
            for _, concept_row in df.iterrows():
                try:
                    concept_code = concept_row['code']
                    concept_name = concept_row['name']
                    
                    # è·å–è¯¥æ¦‚å¿µçš„æˆåˆ†è‚¡
                    detail_df = self.ts_pro.concept_detail(id=concept_code)
                    time.sleep(0.2)  # APIé™é¢‘
                    
                    for _, detail_row in detail_df.iterrows():
                        concept_detail_records.append((
                            detail_row.get('ts_code', ''),
                            detail_row.get('name', ''),
                            concept_name,
                            concept_code,
                            'tushare'
                        ))
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–æ¦‚å¿µ{concept_name}æˆåˆ†è‚¡å¤±è´¥: {e}")
                    continue
            
            conn = self.get_db_connection()
            try:
                with conn.cursor() as cursor:
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM concept_classify")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO concept_classify 
                        (ts_code, name, concept_name, concept_code, src)
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    cursor.executemany(insert_sql, concept_detail_records)
                    conn.commit()
                    
                    record_count = len(concept_detail_records)
                    self.stats['total_records'] += record_count
                    logger.info(f"âœ… æ¦‚å¿µåˆ†ç±»æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                    return record_count
                    
            except Exception as e:
                logger.error(f"âŒ æ¦‚å¿µåˆ†ç±»æ•°æ®æ’å…¥å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–æ¦‚å¿µåˆ†ç±»æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_concept_classify',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def fetch_area_classify(self):
        """æŠ“å–åœ°åŸŸåˆ†ç±»æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–åœ°åŸŸåˆ†ç±»æ•°æ®...")
        
        try:
            # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…é‡å¤è°ƒç”¨API
            conn = self.get_db_connection()
            
            try:
                with conn.cursor() as cursor:
                    # ä»stock_basicè¡¨è·å–åœ°åŸŸä¿¡æ¯
                    cursor.execute("""
                        SELECT ts_code, name, area, industry 
                        FROM stock_basic 
                        WHERE list_status = 'L'
                    """)
                    
                    stock_data = cursor.fetchall()
                    
                    if not stock_data:
                        logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                        return 0
                    
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM area_classify")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO area_classify 
                        (ts_code, name, area, province, city)
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    records = []
                    for row in stock_data:
                        ts_code, name, area, industry = row
                        records.append((
                            ts_code,
                            name,
                            area or '',  # åœ°åŸŸ
                            area or '',  # çœä»½ (ä½¿ç”¨areaå­—æ®µ)
                            ''  # åŸå¸‚ (æš‚æ—¶ä¸ºç©º)
                        ))
                    
                    if records:
                        cursor.executemany(insert_sql, records)
                        conn.commit()
                        
                        record_count = len(records)
                        self.stats['total_records'] += record_count
                        logger.info(f"âœ… åœ°åŸŸåˆ†ç±»æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                        return record_count
                    else:
                        logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥")
                        return 0
                    
            except Exception as e:
                logger.error(f"âŒ åœ°åŸŸåˆ†ç±»æ•°æ®æ’å…¥å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–åœ°åŸŸåˆ†ç±»æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_area_classify',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def fetch_market_segment(self):
        """æŠ“å–å¸‚åœºæ¿å—æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–å¸‚åœºæ¿å—æ•°æ®...")
        
        try:
            # è·å–æ²ªæ·±æ¸¯é€šæˆåˆ†è‚¡
            hsgt_df = self.ts_pro.hs_const(hs_type='SH')  # æ²ªè‚¡é€š
            hk_df = self.ts_pro.hs_const(hs_type='SZ')    # æ·±è‚¡é€š
            
            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            basic_df = self.ts_pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,symbol,name,area,industry,market,list_date'
            )
            
            conn = self.get_db_connection()
            try:
                with conn.cursor() as cursor:
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM market_segment")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO market_segment 
                        (ts_code, name, market, segment, is_hs)
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    records = []
                    hsgt_codes = set(hsgt_df['ts_code'].tolist() + hk_df['ts_code'].tolist())
                    
                    for _, row in basic_df.iterrows():
                        ts_code = row.get('ts_code', '')
                        is_hs = 'Y' if ts_code in hsgt_codes else 'N'
                        
                        records.append((
                            ts_code,
                            row.get('name', ''),
                            row.get('market', ''),
                            row.get('industry', ''),  # ä½¿ç”¨è¡Œä¸šä½œä¸ºæ¿å—
                            is_hs
                        ))
                    
                    cursor.executemany(insert_sql, records)
                    conn.commit()
                    
                    record_count = len(records)
                    self.stats['total_records'] += record_count
                    logger.info(f"âœ… å¸‚åœºæ¿å—æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                    return record_count
                    
            except Exception as e:
                logger.error(f"âŒ å¸‚åœºæ¿å—æ•°æ®æ’å…¥å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¸‚åœºæ¿å—æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_market_segment',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        try:
            os.makedirs('logs', exist_ok=True)
            with open(self.progress_file, 'w') as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def run(self):
        """æ‰§è¡Œæ¿å—æ•°æ®æŠ“å–"""
        logger.info("ğŸ¯ å¼€å§‹æ¿å—æ•°æ®æŠ“å–ä»»åŠ¡...")
        
        try:
            # åˆ›å»ºæ•°æ®è¡¨
            self.create_tables()
            
            # æŒ‰ä¼˜å…ˆçº§æŠ“å–æ•°æ®
            tasks = [
                ('è¡Œä¸šåˆ†ç±»', self.fetch_industry_classify),
                ('æ¦‚å¿µåˆ†ç±»', self.fetch_concept_classify),
                ('åœ°åŸŸåˆ†ç±»', self.fetch_area_classify),
                ('å¸‚åœºæ¿å—', self.fetch_market_segment)
            ]
            
            for task_name, task_func in tasks:
                logger.info(f"ğŸ“Š æ­£åœ¨å¤„ç†: {task_name}")
                
                try:
                    record_count = task_func()
                    self.stats['processed_tables'] += 1
                    logger.info(f"âœ… {task_name}å®Œæˆ: {record_count}æ¡è®°å½•")
                    
                    # ä¿å­˜è¿›åº¦
                    self.save_progress()
                    
                    # APIé™é¢‘
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ {task_name}å¤±è´¥: {e}")
                    self.stats['failed_operations'].append({
                        'task': task_name,
                        'error': str(e),
                        'time': datetime.now().isoformat()
                    })
            
            # æœ€ç»ˆç»Ÿè®¡
            self.stats['end_time'] = datetime.now().isoformat()
            self.stats['duration'] = (datetime.now() - datetime.fromisoformat(self.stats['start_time'])).total_seconds()
            
            logger.info("ğŸ‰ æ¿å—æ•°æ®æŠ“å–ä»»åŠ¡å®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†è¡¨æ•°: {self.stats['processed_tables']}")
            logger.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {self.stats['total_records']:,}")
            logger.info(f"â±ï¸ è€—æ—¶: {self.stats['duration']:.2f}ç§’")
            
            if self.stats['failed_operations']:
                logger.warning(f"âš ï¸ å¤±è´¥æ“ä½œæ•°: {len(self.stats['failed_operations'])}")
            
            self.save_progress()
            
        except Exception as e:
            logger.error(f"âŒ æ¿å—æ•°æ®æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            self.stats['error'] = str(e)
            self.save_progress()
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        fetcher = SectorDataFetcher()
        fetcher.run()
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    return 0

if __name__ == "__main__":
    exit(main())