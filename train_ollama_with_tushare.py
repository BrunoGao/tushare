#!/usr/bin/env python3
"""
TuShareæ•°æ® + Ollamaå¤§æ¨¡å‹è®­ç»ƒä¸»è„šæœ¬
å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
"""

import os
import sys
import logging
from datetime import datetime, timedelta
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm.tushare_data_extractor import TuShareDataExtractor
from llm.llm_training_data_generator import LLMTrainingDataGenerator
from llm.ollama_trainer import OllamaTrainer, create_test_cases

class TuShareOllamaTrainingPipeline:
    """TuShare + Ollamaå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
    
    def __init__(self, tushare_token: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
        
        Args:
            tushare_token: TuShare Pro API token
        """
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.data_extractor = TuShareDataExtractor(tushare_token)
        self.data_generator = LLMTrainingDataGenerator()
        self.ollama_trainer = OllamaTrainer()
        
        # é…ç½®
        self.config = {
            'stock_limit': 50,  # è‚¡ç¥¨æ•°é‡é™åˆ¶
            'days_back': 180,   # å†å²æ•°æ®å¤©æ•°
            'max_training_examples': 500,  # æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°
            'base_model': 'lingjingwanxiang:70b',
            'output_dir': 'data'
        }
        
        self.logger.info("ğŸš€ TuShare + Ollamaè®­ç»ƒæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def step1_extract_stock_data(self) -> str:
        """æ­¥éª¤1: æå–è‚¡ç¥¨æ•°æ®"""
        self.logger.info("ğŸ“Š æ­¥éª¤1: æå–TuShareè‚¡ç¥¨æ•°æ®")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = self.data_extractor.get_stock_list(limit=self.config['stock_limit'])
        if stocks.empty:
            raise ValueError("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        
        # æå–è‚¡ç¥¨ä»£ç 
        if 'ts_code' in stocks.columns:
            stock_codes = stocks['ts_code'].tolist()
        else:
            # å¤„ç†å…è´¹APIçš„æƒ…å†µ
            stocks.reset_index(inplace=True)
            stock_codes = [f"{code}.SZ" if code.startswith('0') or code.startswith('3') 
                          else f"{code}.SH" for code in stocks['code'].tolist()]
        
        self.logger.info(f"   é€‰æ‹©è‚¡ç¥¨: {len(stock_codes)}åª")
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=self.config['days_back'])).strftime('%Y%m%d')
        
        self.logger.info(f"   æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        
        # æå–ç»¼åˆæ•°æ®é›†
        dataset = self.data_extractor.extract_comprehensive_dataset(
            stock_codes=stock_codes,
            start_date=start_date,
            end_date=end_date,
            include_financial=False,  # æš‚æ—¶ä¸åŒ…å«è´¢åŠ¡æ•°æ®
            include_news=False       # æš‚æ—¶ä¸åŒ…å«æ–°é—»æ•°æ®
        )
        
        # ä¿å­˜æ•°æ®é›†
        dataset_dir = f"{self.config['output_dir']}/tushare_dataset"
        self.data_extractor.save_dataset(dataset, dataset_dir)
        
        # è¿”å›æ—¥çº¿æ•°æ®æ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        daily_data_file = f"{dataset_dir}/daily_data_{timestamp}.csv"
        
        self.logger.info("âœ… æ­¥éª¤1å®Œæˆ: è‚¡ç¥¨æ•°æ®æå–æˆåŠŸ")
        return daily_data_file
    
    def step2_generate_training_data(self, stock_data_file: str) -> str:
        """æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        self.logger.info("ğŸ§  æ­¥éª¤2: ç”Ÿæˆå¤§æ¨¡å‹è®­ç»ƒæ•°æ®")
        
        # è¯»å–è‚¡ç¥¨æ•°æ®
        import pandas as pd
        stock_data = pd.read_csv(stock_data_file)
        self.logger.info(f"   åŠ è½½è‚¡ç¥¨æ•°æ®: {len(stock_data)}æ¡è®°å½•")
        
        # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        training_examples = self.data_generator.create_training_examples(
            stock_data, 
            max_examples=self.config['max_training_examples']
        )
        
        if not training_examples:
            raise ValueError("æœªèƒ½ç”Ÿæˆè®­ç»ƒæ ·æœ¬")
        
        self.logger.info(f"   ç”Ÿæˆè®­ç»ƒæ ·æœ¬: {len(training_examples)}ä¸ª")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_dir = f"{self.config['output_dir']}/llm_training"
        os.makedirs(training_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        training_file = f"{training_dir}/stock_training_data_{timestamp}.jsonl"
        
        self.data_generator.save_training_data(training_examples, training_file, 'jsonl')
        
        self.logger.info("âœ… æ­¥éª¤2å®Œæˆ: è®­ç»ƒæ•°æ®ç”ŸæˆæˆåŠŸ")
        return training_file
    
    def step3_fine_tune_model(self, training_data_file: str) -> str:
        """æ­¥éª¤3: å¾®è°ƒæ¨¡å‹"""
        self.logger.info("ğŸ¯ æ­¥éª¤3: å¾®è°ƒOllamaæ¨¡å‹")
        
        # æ£€æŸ¥åŸºç¡€æ¨¡å‹
        base_model = self.config['base_model']
        if not self.ollama_trainer.check_model_exists(base_model):
            raise ValueError(f"åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: {base_model}")
        
        self.logger.info(f"   åŸºç¡€æ¨¡å‹: {base_model}")
        
        # åˆ›å»ºå¾®è°ƒæ¨¡å‹åç§°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        fine_tuned_model = f"lingjingwanxiang-stock-{timestamp}"
        
        self.logger.info(f"   å¾®è°ƒæ¨¡å‹: {fine_tuned_model}")
        
        # æ‰§è¡Œå¾®è°ƒ
        success = self.ollama_trainer.fine_tune_with_training_data(
            base_model=base_model,
            training_data_file=training_data_file,
            fine_tuned_model_name=fine_tuned_model
        )
        
        if not success:
            raise ValueError("æ¨¡å‹å¾®è°ƒå¤±è´¥")
        
        self.logger.info("âœ… æ­¥éª¤3å®Œæˆ: æ¨¡å‹å¾®è°ƒæˆåŠŸ")
        return fine_tuned_model
    
    def step4_test_model(self, model_name: str) -> dict:
        """æ­¥éª¤4: æµ‹è¯•æ¨¡å‹"""
        self.logger.info("ğŸ§ª æ­¥éª¤4: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹")
        
        # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        test_cases = create_test_cases()
        self.logger.info(f"   æµ‹è¯•ç”¨ä¾‹: {len(test_cases)}ä¸ª")
        
        # æ‰§è¡Œæµ‹è¯•
        test_results = self.ollama_trainer.test_fine_tuned_model(model_name, test_cases)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"{self.config['output_dir']}/llm_training/test_results_{timestamp}.json"
        self.ollama_trainer.save_test_results(test_results, results_file)
        
        self.logger.info("âœ… æ­¥éª¤4å®Œæˆ: æ¨¡å‹æµ‹è¯•æˆåŠŸ")
        return test_results
    
    def run_full_pipeline(self) -> dict:
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒæµæ°´çº¿")
        
        pipeline_results = {
            'start_time': datetime.now().isoformat(),
            'config': self.config,
            'steps': {},
            'final_model': None,
            'test_results': None
        }
        
        try:
            # æ­¥éª¤1: æå–è‚¡ç¥¨æ•°æ®
            stock_data_file = self.step1_extract_stock_data()
            pipeline_results['steps']['step1_data_extraction'] = {
                'status': 'success',
                'output_file': stock_data_file
            }
            
            # æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®
            training_data_file = self.step2_generate_training_data(stock_data_file)
            pipeline_results['steps']['step2_training_data'] = {
                'status': 'success',
                'output_file': training_data_file
            }
            
            # æ­¥éª¤3: å¾®è°ƒæ¨¡å‹
            fine_tuned_model = self.step3_fine_tune_model(training_data_file)
            pipeline_results['steps']['step3_fine_tuning'] = {
                'status': 'success',
                'model_name': fine_tuned_model
            }
            pipeline_results['final_model'] = fine_tuned_model
            
            # æ­¥éª¤4: æµ‹è¯•æ¨¡å‹
            test_results = self.step4_test_model(fine_tuned_model)
            pipeline_results['steps']['step4_testing'] = {
                'status': 'success',
                'test_summary': test_results['summary']
            }
            pipeline_results['test_results'] = test_results
            
            pipeline_results['status'] = 'success'
            pipeline_results['end_time'] = datetime.now().isoformat()
            
            self.logger.info("ğŸ‰ å®Œæ•´è®­ç»ƒæµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
            
        except Exception as e:
            pipeline_results['status'] = 'failed'
            pipeline_results['error'] = str(e)
            pipeline_results['end_time'] = datetime.now().isoformat()
            
            self.logger.error(f"âŒ è®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
            raise
        
        return pipeline_results
    
    def save_pipeline_results(self, results: dict):
        """ä¿å­˜æµæ°´çº¿ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"{self.config['output_dir']}/pipeline_results_{timestamp}.json"
        
        os.makedirs(os.path.dirname(results_file), exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… æµæ°´çº¿ç»“æœå·²ä¿å­˜: {results_file}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('logs/tushare_ollama_training.log')
        ]
    )
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    logger = logging.getLogger(__name__)
    
    print("ğŸš€ TuShare + Ollamaå¤§æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # è·å–TuShare tokenï¼ˆå¯é€‰ï¼‰
        tushare_token = os.getenv('TUSHARE_TOKEN')
        if not tushare_token:
            print("âš ï¸  æœªè®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨å…è´¹API")
        
        # åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
        pipeline = TuShareOllamaTrainingPipeline(tushare_token)
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   è‚¡ç¥¨æ•°é‡: {pipeline.config['stock_limit']}åª")
        print(f"   å†å²å¤©æ•°: {pipeline.config['days_back']}å¤©")
        print(f"   è®­ç»ƒæ ·æœ¬: {pipeline.config['max_training_examples']}ä¸ª")
        print(f"   åŸºç¡€æ¨¡å‹: {pipeline.config['base_model']}")
        print()
        
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        results = pipeline.run_full_pipeline()
        
        # ä¿å­˜ç»“æœ
        pipeline.save_pipeline_results(results)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹: {results['final_model']}")
        
        if results['test_results']:
            summary = results['test_results']['summary']
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
            print(f"   æµ‹è¯•ç”¨ä¾‹: {summary['total_tests']}ä¸ª")
            print(f"   æˆåŠŸç‡: {summary['success_rate']:.2%}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {summary['average_response_time']:.2f}ç§’")
        
        print(f"\nğŸ¯ ä½¿ç”¨æ–¹æ³•:")
        print(f"   ollama run {results['final_model']}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")


if __name__ == "__main__":
    main()