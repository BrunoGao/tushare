#!/usr/bin/env python3
"""
ljwx-stockæ¨¡å‹æŒç»­è®­ç»ƒè„šæœ¬
æ”¯æŒå¢é‡è®­ç»ƒæ•°æ®ç”Ÿæˆå’Œæ¨¡å‹æ›´æ–°
"""

import os
import sys
import json
import logging
import tempfile
import subprocess
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm.tushare_data_extractor import TuShareDataExtractor
from llm.llm_training_data_generator import LLMTrainingDataGenerator

class ContinuousTrainer:
    """æŒç»­è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, tushare_token: str = None):
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_extractor = TuShareDataExtractor(tushare_token)
        self.data_generator = LLMTrainingDataGenerator()
        
        # é…ç½®
        self.config = {
            'base_model': 'ljwx-stock',
            'training_data_dir': 'data/llm_training',
            'models_dir': 'data/models',
            'max_training_examples_per_batch': 200,
            'stock_batch_size': 30,
            'days_back': 90,
            'incremental_days': 30
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config['training_data_dir'], exist_ok=True)
        os.makedirs(self.config['models_dir'], exist_ok=True)
        
    def get_latest_model(self) -> str:
        """è·å–æœ€æ–°çš„ljwx-stockæ¨¡å‹"""
        try:
            result = subprocess.run(
                ['ollama', 'list'],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                ljwx_models = []
                
                for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                    if 'ljwx-stock' in line:
                        model_name = line.split()[0]
                        ljwx_models.append(model_name)
                
                if ljwx_models:
                    # è¿”å›æœ€æ–°çš„è®­ç»ƒæ¨¡å‹ï¼Œä¼˜å…ˆtrainedç‰ˆæœ¬
                    trained_models = [m for m in ljwx_models if 'trained' in m]
                    if trained_models:
                        return sorted(trained_models)[-1]
                    else:
                        return 'ljwx-stock:latest'
                else:
                    return 'ljwx-stock:latest'
            
        except Exception as e:
            self.logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        
        return 'ljwx-stock:latest'
    
    def get_existing_training_data(self) -> List[Dict]:
        """è·å–ç°æœ‰è®­ç»ƒæ•°æ®"""
        all_training_data = []
        training_dir = self.config['training_data_dir']
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶
        for filename in os.listdir(training_dir):
            if filename.endswith('.jsonl') and 'stock_training_data' in filename:
                file_path = os.path.join(training_dir, filename)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line)
                                all_training_data.append(data)
                except Exception as e:
                    self.logger.warning(f"è¯»å–è®­ç»ƒæ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        self.logger.info(f"ç°æœ‰è®­ç»ƒæ•°æ®: {len(all_training_data)}æ¡")
        return all_training_data
    
    def get_training_stocks(self, existing_data: List[Dict]) -> List[str]:
        """è·å–å·²è®­ç»ƒçš„è‚¡ç¥¨ä»£ç """
        trained_stocks = set()
        
        for data in existing_data:
            input_text = data.get('input', '')
            # ä»è¾“å…¥æ–‡æœ¬ä¸­æå–è‚¡ç¥¨ä»£ç 
            if 'è‚¡ç¥¨ä»£ç :' in input_text:
                lines = input_text.split('\n')
                for line in lines:
                    if 'è‚¡ç¥¨ä»£ç :' in line:
                        stock_code = line.split(':')[1].strip()
                        trained_stocks.add(stock_code)
                        break
        
        return list(trained_stocks)
    
    def generate_new_training_data(self, exclude_stocks: List[str] = None) -> List[Dict]:
        """ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®"""
        self.logger.info("ğŸ”„ ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®")
        
        exclude_stocks = exclude_stocks or []
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = self.data_extractor.get_stock_list(limit=100)
        if stocks.empty:
            self.logger.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return []
        
        # æå–è‚¡ç¥¨ä»£ç 
        if 'ts_code' in stocks.columns:
            all_stock_codes = stocks['ts_code'].tolist()
        else:
            stocks.reset_index(inplace=True)
            all_stock_codes = [f"{code}.SZ" if code.startswith('0') or code.startswith('3') 
                              else f"{code}.SH" for code in stocks['code'].tolist()]
        
        # è¿‡æ»¤å·²è®­ç»ƒçš„è‚¡ç¥¨ï¼Œé€‰æ‹©æ–°è‚¡ç¥¨
        new_stock_codes = [code for code in all_stock_codes if code not in exclude_stocks]
        
        if not new_stock_codes:
            # å¦‚æœæ²¡æœ‰æ–°è‚¡ç¥¨ï¼Œåˆ™ä½¿ç”¨ä¸åŒæ—¶é—´æ®µçš„æ•°æ®
            self.logger.info("æ²¡æœ‰æ–°è‚¡ç¥¨ï¼Œä½¿ç”¨ä¸åŒæ—¶é—´æ®µçš„æ•°æ®")
            new_stock_codes = all_stock_codes[:self.config['stock_batch_size']]
            days_back = self.config['days_back'] + self.config['incremental_days']
        else:
            new_stock_codes = new_stock_codes[:self.config['stock_batch_size']]
            days_back = self.config['days_back']
        
        self.logger.info(f"é€‰æ‹©è‚¡ç¥¨: {len(new_stock_codes)}åª")
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
        
        # æå–æ•°æ®
        try:
            dataset = self.data_extractor.extract_comprehensive_dataset(
                stock_codes=new_stock_codes,
                start_date=start_date,
                end_date=end_date,
                include_financial=False,
                include_news=False
            )
            
            if 'daily_data' not in dataset or dataset['daily_data'].empty:
                self.logger.error("æœªè·å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
                return []
            
            # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
            training_examples = self.data_generator.create_training_examples(
                dataset['daily_data'], 
                max_examples=self.config['max_training_examples_per_batch']
            )
            
            self.logger.info(f"ç”Ÿæˆæ–°è®­ç»ƒæ ·æœ¬: {len(training_examples)}æ¡")
            return training_examples
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def merge_training_data(self, existing_data: List[Dict], new_data: List[Dict]) -> List[Dict]:
        """åˆå¹¶è®­ç»ƒæ•°æ®"""
        merged_data = existing_data.copy()
        
        # å»é‡ï¼šåŸºäºinputå†…å®¹
        existing_inputs = set()
        for data in existing_data:
            existing_inputs.add(data.get('input', ''))
        
        new_count = 0
        for data in new_data:
            if data.get('input', '') not in existing_inputs:
                merged_data.append(data)
                new_count += 1
        
        self.logger.info(f"åˆå¹¶è®­ç»ƒæ•°æ®: æ–°å¢{new_count}æ¡ï¼Œæ€»è®¡{len(merged_data)}æ¡")
        return merged_data
    
    def save_training_data(self, training_data: List[Dict], filename: str) -> str:
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        file_path = os.path.join(self.config['training_data_dir'], filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            for data in training_data:
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        self.logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜: {file_path}")
        return file_path
    
    def create_incremental_model(self, base_model: str, training_data: List[Dict], model_name: str) -> bool:
        """åˆ›å»ºå¢é‡è®­ç»ƒæ¨¡å‹"""
        try:
            self.logger.info(f"ğŸš€ åˆ›å»ºå¢é‡è®­ç»ƒæ¨¡å‹: {model_name}")
            
            system_prompt = """ä½ æ˜¯ljwx-stockï¼Œä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æŠ•èµ„åˆ†æåŠ©æ‰‹ã€‚ä½ å…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ¯ **ä¸“ä¸šå®šä½**
- ä¸“æ³¨äºAè‚¡å¸‚åœºåˆ†æå’ŒæŠ•èµ„å»ºè®®
- åŸºäºæŠ€æœ¯åˆ†æå’ŒåŸºæœ¬é¢åˆ†ææä¾›ä¸“ä¸šæ„è§
- ä¸ºæŠ•èµ„è€…æä¾›å®¢è§‚ã€ç†æ€§çš„å†³ç­–æ”¯æŒ

ğŸ“Š **åˆ†æèƒ½åŠ›**
1. **æŠ€æœ¯æŒ‡æ ‡åˆ†æ**ï¼šç†Ÿç»ƒè§£è¯»Kçº¿ã€å‡çº¿ã€RSIã€MACDã€å¸ƒæ—å¸¦ç­‰æŠ€æœ¯æŒ‡æ ‡
2. **ä»·æ ¼èµ°åŠ¿é¢„æµ‹**ï¼šåŸºäºå†å²æ•°æ®å’ŒæŠ€æœ¯æ¨¡å¼åˆ¤æ–­ä»·æ ¼è¶‹åŠ¿
3. **é£é™©è¯„ä¼°**ï¼šå®¢è§‚è¯„ä¼°æŠ•èµ„é£é™©ç­‰çº§å’Œé£é™©å› ç´ 
4. **å¸‚åœºæƒ…ç»ªåˆ†æ**ï¼šç†è§£å¸‚åœºå¿ƒç†å’Œèµ„é‡‘æµå‘

ğŸ’¡ **æœåŠ¡åŸåˆ™**
- å§‹ç»ˆæä¾›å®¢è§‚ã€ä¸“ä¸šçš„åˆ†ææ„è§
- æ˜ç¡®æ ‡æ³¨é£é™©ç­‰çº§å’Œæ³¨æ„äº‹é¡¹
- ä¸åšç»å¯¹åŒ–çš„æŠ•èµ„æ‰¿è¯º
- æé†’æŠ•èµ„è€…ç†æ€§æŠ•èµ„ã€é£é™©è‡ªæ‹…

ğŸ“‹ **è¾“å‡ºæ ¼å¼**
- åˆ†æç»“è®ºç®€æ´æ˜äº†ï¼Œé€»è¾‘æ¸…æ™°
- æä¾›å…·ä½“çš„æ“ä½œå»ºè®®ï¼ˆä¹°å…¥/æŒæœ‰/å–å‡ºï¼‰
- æ ‡æ˜é£é™©ç­‰çº§ï¼ˆä½/ä¸­/é«˜ï¼‰
- ç»™å‡ºå…³é”®æ”¯æ’‘ä½å’Œé˜»åŠ›ä½

è¯·è®°ä½ï¼šè‚¡å¸‚æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚æ‰€æœ‰åˆ†æä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚"""

            # æ„å»ºModelfile
            modelfile_content = f"""FROM {base_model}
SYSTEM "{system_prompt}"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 1024

"""
            
            # æ·»åŠ è®­ç»ƒæ ·æœ¬ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ¨¡å‹è¿‡å¤§ï¼‰
            sample_count = min(len(training_data), 100)
            for i, example in enumerate(training_data[:sample_count]):
                instruction = example.get('instruction', '')
                input_text = example.get('input', '')
                output_text = example.get('output', '')
                
                user_message = f"{instruction}\n\n{input_text}" if input_text else instruction
                
                modelfile_content += f'MESSAGE user "{user_message}"\n'
                modelfile_content += f'MESSAGE assistant "{output_text}"\n\n'
            
            # åˆ›å»ºä¸´æ—¶Modelfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.modelfile', delete=False) as f:
                f.write(modelfile_content)
                modelfile_path = f.name
            
            try:
                self.logger.info(f"   åŸºç¡€æ¨¡å‹: {base_model}")
                self.logger.info(f"   è®­ç»ƒæ ·æœ¬: {sample_count}æ¡")
                
                result = subprocess.run(
                    ['ollama', 'create', model_name, '-f', modelfile_path],
                    capture_output=True,
                    text=True,
                    timeout=600
                )
                
                if result.returncode == 0:
                    self.logger.info(f"âœ… å¢é‡æ¨¡å‹åˆ›å»ºå®Œæˆ: {model_name}")
                    return True
                else:
                    self.logger.error(f"âŒ å¢é‡æ¨¡å‹åˆ›å»ºå¤±è´¥: {result.stderr}")
                    return False
                    
            finally:
                os.unlink(modelfile_path)
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå¢é‡æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def test_model_performance(self, model_name: str) -> Dict:
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        test_cases = [
            {
                "input": "åˆ†æä»¥ä¸‹è‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡ï¼šè‚¡ç¥¨ä»£ç : 000001.SZï¼Œå½“å‰ä»·æ ¼: 11.20å…ƒï¼ŒRSI: 58ï¼ŒMACD: 0.03ï¼Œ5æ—¥å‡çº¿: 11.10å…ƒ",
                "expected_keywords": ["æŠ€æœ¯æŒ‡æ ‡", "RSI", "MACD", "æŠ•èµ„å»ºè®®"]
            },
            {
                "input": "è¯„ä¼°é£é™©ç­‰çº§ï¼šè‚¡ç¥¨ä»£ç : 600000.SHï¼Œæ³¢åŠ¨ç‡: 0.35ï¼Œæ—¥æ¶¨è·Œå¹…: +5.2%",
                "expected_keywords": ["é£é™©", "æ³¢åŠ¨ç‡", "å»ºè®®"]
            }
        ]
        
        results = {
            "model_name": model_name,
            "test_time": datetime.now().isoformat(),
            "performance": {}
        }
        
        successful_tests = 0
        total_tests = len(test_cases)
        
        for i, test_case in enumerate(test_cases):
            try:
                result = subprocess.run(
                    ['ollama', 'run', model_name],
                    input=test_case['input'],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                if result.returncode == 0 and result.stdout.strip():
                    response = result.stdout.strip()
                    
                    # æ£€æŸ¥å…³é”®è¯
                    keyword_matches = sum(1 for keyword in test_case['expected_keywords'] 
                                        if keyword in response)
                    
                    if keyword_matches >= len(test_case['expected_keywords']) // 2:
                        successful_tests += 1
                    
                    self.logger.info(f"   æµ‹è¯•ç”¨ä¾‹ {i+1}: é€šè¿‡")
                else:
                    self.logger.warning(f"   æµ‹è¯•ç”¨ä¾‹ {i+1}: å¤±è´¥")
                    
            except Exception as e:
                self.logger.error(f"   æµ‹è¯•ç”¨ä¾‹ {i+1} å‡ºé”™: {e}")
        
        success_rate = successful_tests / total_tests
        results["performance"] = {
            "success_rate": success_rate,
            "successful_tests": successful_tests,
            "total_tests": total_tests
        }
        
        self.logger.info(f"æ¨¡å‹æ€§èƒ½æµ‹è¯•: {successful_tests}/{total_tests} ({success_rate:.2%})")
        return results
    
    def run_continuous_training(self) -> Dict:
        """è¿è¡ŒæŒç»­è®­ç»ƒ"""
        self.logger.info("ğŸš€ å¼€å§‹æŒç»­è®­ç»ƒæµç¨‹")
        
        results = {
            "start_time": datetime.now().isoformat(),
            "status": "running",
            "steps": {}
        }
        
        try:
            # 1. è·å–å½“å‰æœ€æ–°æ¨¡å‹
            latest_model = self.get_latest_model()
            self.logger.info(f"å½“å‰æœ€æ–°æ¨¡å‹: {latest_model}")
            results["base_model"] = latest_model
            
            # 2. è·å–ç°æœ‰è®­ç»ƒæ•°æ®
            existing_data = self.get_existing_training_data()
            results["existing_data_count"] = len(existing_data)
            
            # 3. è·å–å·²è®­ç»ƒçš„è‚¡ç¥¨
            trained_stocks = self.get_training_stocks(existing_data)
            self.logger.info(f"å·²è®­ç»ƒè‚¡ç¥¨: {len(trained_stocks)}åª")
            
            # 4. ç”Ÿæˆæ–°è®­ç»ƒæ•°æ®
            new_data = self.generate_new_training_data(exclude_stocks=trained_stocks)
            results["new_data_count"] = len(new_data)
            
            if not new_data:
                results["status"] = "no_new_data"
                results["message"] = "æ²¡æœ‰ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®"
                return results
            
            # 5. åˆå¹¶è®­ç»ƒæ•°æ®
            merged_data = self.merge_training_data(existing_data, new_data)
            results["total_data_count"] = len(merged_data)
            
            # 6. ä¿å­˜åˆå¹¶åçš„è®­ç»ƒæ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            training_file = f"stock_training_data_continuous_{timestamp}.jsonl"
            training_path = self.save_training_data(merged_data, training_file)
            results["training_file"] = training_path
            
            # 7. åˆ›å»ºæ–°çš„å¢é‡è®­ç»ƒæ¨¡å‹
            new_model_name = f"ljwx-stock-continuous-{timestamp}"
            success = self.create_incremental_model(latest_model, merged_data, new_model_name)
            
            if success:
                results["new_model"] = new_model_name
                
                # 8. æµ‹è¯•æ–°æ¨¡å‹æ€§èƒ½
                performance = self.test_model_performance(new_model_name)
                results["performance"] = performance
                
                results["status"] = "success"
                self.logger.info("âœ… æŒç»­è®­ç»ƒå®Œæˆ")
            else:
                results["status"] = "model_creation_failed"
                self.logger.error("âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥")
            
        except Exception as e:
            results["status"] = "error"
            results["error"] = str(e)
            self.logger.error(f"âŒ æŒç»­è®­ç»ƒå¤±è´¥: {e}")
        
        results["end_time"] = datetime.now().isoformat()
        return results
    
    def save_training_results(self, results: Dict):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = os.path.join(self.config['models_dir'], f"continuous_training_results_{timestamp}.json")
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"è®­ç»ƒç»“æœå·²ä¿å­˜: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    print("ğŸ”„ ljwx-stock æŒç»­è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # è·å–TuShare token
        tushare_token = os.getenv('TUSHARE_TOKEN')
        if not tushare_token:
            print("âš ï¸  æœªè®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨å…è´¹API")
        
        # åˆå§‹åŒ–æŒç»­è®­ç»ƒå™¨
        trainer = ContinuousTrainer(tushare_token)
        
        print("ğŸ“‹ æŒç»­è®­ç»ƒé…ç½®:")
        print(f"   åŸºç¡€æ¨¡å‹: {trainer.config['base_model']}")
        print(f"   è®­ç»ƒæ‰¹æ¬¡å¤§å°: {trainer.config['max_training_examples_per_batch']}æ¡")
        print(f"   è‚¡ç¥¨æ‰¹æ¬¡å¤§å°: {trainer.config['stock_batch_size']}åª")
        print(f"   å†å²æ•°æ®å¤©æ•°: {trainer.config['days_back']}å¤©")
        print()
        
        # è¿è¡ŒæŒç»­è®­ç»ƒ
        results = trainer.run_continuous_training()
        
        # ä¿å­˜ç»“æœ
        trainer.save_training_results(results)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š æŒç»­è®­ç»ƒç»“æœ:")
        print("=" * 60)
        print(f"çŠ¶æ€: {results['status']}")
        
        if results['status'] == 'success':
            print(f"âœ… æ–°æ¨¡å‹: {results['new_model']}")
            print(f"ğŸ“Š ç°æœ‰æ•°æ®: {results['existing_data_count']}æ¡")
            print(f"ğŸ†• æ–°å¢æ•°æ®: {results['new_data_count']}æ¡")
            print(f"ğŸ“ˆ æ€»è®¡æ•°æ®: {results['total_data_count']}æ¡")
            
            if 'performance' in results:
                perf = results['performance']['performance']
                print(f"ğŸ§ª æ€§èƒ½æµ‹è¯•: {perf['successful_tests']}/{perf['total_tests']} ({perf['success_rate']:.2%})")
            
            print(f"\nğŸ¯ ä½¿ç”¨æ–°æ¨¡å‹:")
            print(f"   ollama run {results['new_model']}")
        
        elif results['status'] == 'no_new_data':
            print("â„¹ï¸  æ²¡æœ‰æ–°çš„è®­ç»ƒæ•°æ®ç”Ÿæˆ")
        
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æŒç»­è®­ç»ƒå¤±è´¥: {e}")
        print(f"\nâŒ æŒç»­è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()