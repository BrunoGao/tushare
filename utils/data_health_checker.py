#!/usr/bin/env python3
"""
æ•°æ®å¥åº·æ£€æŸ¥å™¨ - å¢å¼ºç‰ˆ
è‡ªåŠ¨ç›‘æ§æ•°æ®å®Œæ•´æ€§ï¼Œæ£€æµ‹æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¹¶è§¦å‘è‡ªåŠ¨ä¿®å¤
æ”¯æŒå¤šç»´åº¦æ•°æ®è´¨é‡è¯„ä¼°å’Œæ™ºèƒ½ä¿®å¤ç­–ç•¥
"""
import logging
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import time
import threading
import asyncio
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import text
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from utils.db_helper import db
from scripts.fetch_comprehensive_data import ComprehensiveDataFetcher

logger = logging.getLogger(__name__)

class DataHealthChecker:
    """æ•°æ®å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.fetcher = ComprehensiveDataFetcher()
        self.health_report = {}
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.lock = threading.Lock()
        
        # æ‰©å±•å…³é”®è¡¨åˆ—è¡¨
        self.critical_tables = [
            'stock_basic', 't_stock_basic', 'stock_indicators',
            't_financial_indicators', 't_money_flow', 'recommend_result',
            't_daily_basic', 't_concept', 't_concept_detail', 't_index_basic'
        ]
        
        # å¢å¼ºæ•°æ®è´¨é‡æ ‡å‡†
        self.quality_standards = {
            'stock_basic': {
                'min_records': 4000,
                'required_fields': ['ts_code', 'name', 'industry', 'market'],
                'data_types': {'ts_code': 'str', 'name': 'str'},
                'null_tolerance': 0.01  # å…è®¸1%çš„ç©ºå€¼
            },
            't_stock_basic': {
                'min_records': 4000,
                'required_fields': ['ts_code', 'name', 'industry', 'market'],
                'data_types': {'ts_code': 'str', 'name': 'str'},
                'null_tolerance': 0.01
            },
            'stock_indicators': {
                'min_records': 100000,
                'required_fields': ['ts_code', 'trade_date', 'close', 'volume'],
                'data_types': {'close': 'float', 'volume': 'int'},
                'null_tolerance': 0.05
            },
            't_financial_indicators': {
                'min_records': 50000,
                'required_fields': ['ts_code', 'end_date', 'total_revenue'],
                'data_types': {'total_revenue': 'float'},
                'null_tolerance': 0.1
            },
            't_money_flow': {
                'min_records': 10000,
                'required_fields': ['ts_code', 'trade_date', 'net_mf_amount'],
                'data_types': {'net_mf_amount': 'float'},
                'null_tolerance': 0.05
            },
            'recommend_result': {
                'min_records': 10,
                'required_fields': ['ts_code', 'score', 'strategy'],
                'data_types': {'score': 'float'},
                'null_tolerance': 0.0
            },
            't_daily_basic': {
                'min_records': 50000,
                'required_fields': ['ts_code', 'trade_date', 'pe', 'pb'],
                'data_types': {'pe': 'float', 'pb': 'float'},
                'null_tolerance': 0.1
            },
            't_concept': {
                'min_records': 100,
                'required_fields': ['concept_code', 'concept_name'],
                'data_types': {'concept_code': 'str'},
                'null_tolerance': 0.0
            }
        }
        
        # æ•°æ®è´¨é‡æ£€æŸ¥è§„åˆ™
        self.quality_rules = {
            'duplicate_check': True,
            'outlier_detection': True,
            'consistency_check': True,
            'completeness_check': True,
            'timeliness_check': True
        }
        
    def check_table_health(self, table_name: str) -> Dict:
        """æ£€æŸ¥å•ä¸ªè¡¨çš„å¥åº·çŠ¶æ€ - å¢å¼ºç‰ˆ"""
        try:
            with db.engine.connect() as conn:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                table_exists = conn.execute(text(f"SHOW TABLES LIKE '{table_name}'")).fetchone()
                if not table_exists:
                    return {
                        'table': table_name,
                        'status': 'MISSING',
                        'record_count': 0,
                        'issues': ['è¡¨ä¸å­˜åœ¨'],
                        'health_score': 0,
                        'quality_metrics': {}
                    }
                
                # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
                record_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).scalar()
                
                # å¤šç»´åº¦è´¨é‡æ£€æŸ¥
                quality_metrics = self._comprehensive_quality_check(table_name, conn)
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                missing_fields = self._check_required_fields(table_name, conn)
                
                # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
                data_freshness = self._check_data_freshness(table_name, conn)
                
                # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                completeness_score = self._check_data_completeness(table_name, conn)
                
                # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
                consistency_score = self._check_data_consistency(table_name, conn)
                
                # è®¡ç®—ç»¼åˆå¥åº·è¯„åˆ†
                health_score = self._calculate_enhanced_health_score(
                    table_name, record_count, missing_fields, data_freshness,
                    completeness_score, consistency_score, quality_metrics
                )
                
                # ç”Ÿæˆè¯¦ç»†é—®é¢˜åˆ—è¡¨
                issues = self._generate_detailed_issues(
                    table_name, record_count, missing_fields, data_freshness,
                    completeness_score, consistency_score, quality_metrics
                )
                
                status = self._determine_health_status(health_score, issues)
                
                return {
                    'table': table_name,
                    'status': status,
                    'record_count': record_count,
                    'missing_fields': missing_fields,
                    'data_freshness': data_freshness,
                    'completeness_score': completeness_score,
                    'consistency_score': consistency_score,
                    'quality_metrics': quality_metrics,
                    'health_score': health_score,
                    'issues': issues,
                    'check_timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥è¡¨ {table_name} å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            return {
                'table': table_name,
                'status': 'ERROR',
                'record_count': 0,
                'issues': [f'æ£€æŸ¥å¤±è´¥: {str(e)}'],
                'health_score': 0,
                'quality_metrics': {},
                'error': str(e)
            }
    
    def _check_data_freshness(self, table_name: str, conn) -> Dict:
        """æ£€æŸ¥æ•°æ®æ–°é²œåº¦"""
        try:
            # å°è¯•ä¸åŒçš„æ—¥æœŸå­—æ®µ
            date_fields = ['trade_date', 'ann_date', 'end_date', 'created_at', 'updated_at']
            
            for field in date_fields:
                try:
                    result = conn.execute(text(f"SELECT MAX({field}) FROM {table_name}")).scalar()
                    if result:
                        if isinstance(result, str):
                            latest_date = pd.to_datetime(result).date()
                        else:
                            latest_date = result.date() if hasattr(result, 'date') else result
                        
                        days_old = (datetime.now().date() - latest_date).days
                        return {
                            'latest_date': latest_date.isoformat(),
                            'days_old': days_old,
                            'date_field': field
                        }
                except:
                    continue
            
            return {'latest_date': None, 'days_old': 999, 'date_field': None}
            
        except Exception as e:
            return {'latest_date': None, 'days_old': 999, 'date_field': None, 'error': str(e)}
    
    def _calculate_health_score(self, table_name: str, record_count: int, missing_fields: List, data_freshness: Dict) -> int:
        """è®¡ç®—å¥åº·è¯„åˆ† (0-100)"""
        score = 100
        
        # è®°å½•æ•°é‡è¯„åˆ† (40åˆ†)
        if table_name in self.quality_standards:
            min_records = self.quality_standards[table_name]['min_records']
            if record_count == 0:
                score -= 40
            elif record_count < min_records * 0.5:
                score -= 30
            elif record_count < min_records:
                score -= 20
        
        # å­—æ®µå®Œæ•´æ€§è¯„åˆ† (30åˆ†)
        if missing_fields:
            score -= len(missing_fields) * 10
        
        # æ•°æ®æ–°é²œåº¦è¯„åˆ† (30åˆ†)
        days_old = data_freshness.get('days_old', 999)
        if days_old > 30:
            score -= 30
        elif days_old > 7:
            score -= 20
        elif days_old > 3:
            score -= 10
        
        return max(0, score)
    
    def check_system_health(self) -> Dict:
        """æ£€æŸ¥æ•´ä¸ªç³»ç»Ÿçš„å¥åº·çŠ¶æ€"""
        logger.info("ğŸ” å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        system_health = {
            'check_time': datetime.now().isoformat(),
            'tables': {},
            'overall_status': 'HEALTHY',
            'critical_issues': [],
            'warnings': [],
            'recommendations': []
        }
        
        critical_issues = []
        warnings = []
        total_score = 0
        
        # æ£€æŸ¥å…³é”®è¡¨
        for table_name in self.critical_tables:
            table_health = self.check_table_health(table_name)
            system_health['tables'][table_name] = table_health
            
            if table_health['status'] == 'CRITICAL' or table_health['status'] == 'MISSING':
                critical_issues.extend(table_health['issues'])
            elif table_health['status'] == 'WARNING':
                warnings.extend(table_health['issues'])
            
            total_score += table_health['health_score']
        
        # è®¡ç®—æ•´ä½“å¥åº·è¯„åˆ†
        avg_score = total_score / len(self.critical_tables) if self.critical_tables else 0
        
        if avg_score >= 80:
            system_health['overall_status'] = 'HEALTHY'
        elif avg_score >= 60:
            system_health['overall_status'] = 'WARNING'
        else:
            system_health['overall_status'] = 'CRITICAL'
        
        system_health['overall_score'] = round(avg_score, 2)
        system_health['critical_issues'] = critical_issues
        system_health['warnings'] = warnings
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        system_health['recommendations'] = self._generate_recommendations(system_health)
        
        logger.info(f"âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥å®Œæˆï¼Œæ•´ä½“è¯„åˆ†: {avg_score:.1f}")
        return system_health
    
    def _generate_recommendations(self, health_report: Dict) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        for table_name, table_health in health_report['tables'].items():
            if table_health['status'] in ['CRITICAL', 'MISSING']:
                if table_health['record_count'] == 0:
                    recommendations.append(f"æ‰§è¡Œæ•°æ®åˆå§‹åŒ–: python3 scripts/fetch_comprehensive_data.py --category basic")
                
                if 'stock_basic' in table_name and table_health['record_count'] < 1000:
                    recommendations.append(f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: python3 scripts/fetch_stock_basic.py")
                
                if 'financial' in table_name and table_health['record_count'] < 10000:
                    recommendations.append(f"è·å–è´¢åŠ¡æ•°æ®: python3 run.py fetch-comprehensive --category financial")
                
                if 'money_flow' in table_name and table_health['record_count'] < 5000:
                    recommendations.append(f"è·å–èµ„é‡‘æµæ•°æ®: python3 run.py fetch-comprehensive --category money_flow")
        
        # é€šç”¨å»ºè®®
        if health_report['overall_score'] < 70:
            recommendations.append("è¿è¡Œå®Œæ•´æ•°æ®è·å–: python3 run.py fetch-comprehensive --mode full")
            recommendations.append("å¯åŠ¨æ•°æ®è°ƒåº¦å™¨: python3 utils/data_scheduler.py start")
        
        return recommendations
    
    def auto_repair(self, health_report: Dict) -> Dict:
        """è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜"""
        logger.info("ğŸ”§ å¼€å§‹è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜...")
        
        repair_results = {
            'start_time': datetime.now().isoformat(),
            'actions_taken': [],
            'success_count': 0,
            'failure_count': 0,
            'details': []
        }
        
        try:
            # æ‰§è¡Œæ¨èçš„ä¿®å¤æ“ä½œ
            for recommendation in health_report.get('recommendations', []):
                try:
                    if 'fetch_stock_basic' in recommendation:
                        logger.info("ğŸ“Š æ‰§è¡Œè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è·å–...")
                        result = self.fetcher.fetch_basic_info_data('incremental')
                        repair_results['actions_taken'].append('è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è·å–')
                        repair_results['success_count'] += 1
                        repair_results['details'].append(f"è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è·å–: {result}")
                        
                    elif 'fetch-comprehensive' in recommendation and 'financial' in recommendation:
                        logger.info("ğŸ’¼ æ‰§è¡Œè´¢åŠ¡æ•°æ®è·å–...")
                        result = self.fetcher.fetch_financial_data('incremental')
                        repair_results['actions_taken'].append('è´¢åŠ¡æ•°æ®è·å–')
                        repair_results['success_count'] += 1
                        repair_results['details'].append(f"è´¢åŠ¡æ•°æ®è·å–: {result}")
                        
                    elif 'fetch-comprehensive' in recommendation and 'money_flow' in recommendation:
                        logger.info("ğŸ’¸ æ‰§è¡Œèµ„é‡‘æµæ•°æ®è·å–...")
                        result = self.fetcher.fetch_money_flow_data('incremental')
                        repair_results['actions_taken'].append('èµ„é‡‘æµæ•°æ®è·å–')
                        repair_results['success_count'] += 1
                        repair_results['details'].append(f"èµ„é‡‘æµæ•°æ®è·å–: {result}")
                        
                    time.sleep(2)  # é¿å…APIé¢‘ç‡é™åˆ¶
                    
                except Exception as e:
                    logger.error(f"ä¿®å¤æ“ä½œå¤±è´¥: {e}")
                    repair_results['failure_count'] += 1
                    repair_results['details'].append(f"ä¿®å¤å¤±è´¥: {str(e)}")
        
        except Exception as e:
            logger.error(f"è‡ªåŠ¨ä¿®å¤è¿‡ç¨‹å¼‚å¸¸: {e}")
            repair_results['details'].append(f"ä¿®å¤è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        
        repair_results['end_time'] = datetime.now().isoformat()
        logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤å®Œæˆï¼ŒæˆåŠŸ: {repair_results['success_count']}, å¤±è´¥: {repair_results['failure_count']}")
        
        return repair_results
    
    def generate_health_report(self, save_to_file: bool = True) -> str:
        """ç”Ÿæˆå¥åº·æ£€æŸ¥æŠ¥å‘Š"""
        health_data = self.check_system_health()
        
        report = f"""
# ğŸ“Š æ•°æ®å¥åº·æ£€æŸ¥æŠ¥å‘Š

**æ£€æŸ¥æ—¶é—´**: {health_data['check_time']}
**æ•´ä½“çŠ¶æ€**: {health_data['overall_status']} ({health_data['overall_score']}/100)

## ğŸ“‹ è¡¨å¥åº·çŠ¶æ€

| è¡¨å | çŠ¶æ€ | è®°å½•æ•° | å¥åº·è¯„åˆ† | ä¸»è¦é—®é¢˜ |
|------|------|--------|----------|----------|
"""
        
        for table_name, table_health in health_data['tables'].items():
            issues = '; '.join(table_health['issues'][:2]) if table_health['issues'] else 'æ— '
            report += f"| {table_name} | {table_health['status']} | {table_health['record_count']:,} | {table_health['health_score']}/100 | {issues} |\n"
        
        if health_data['critical_issues']:
            report += f"\n## ğŸš¨ ä¸¥é‡é—®é¢˜\n"
            for issue in health_data['critical_issues']:
                report += f"- {issue}\n"
        
        if health_data['warnings']:
            report += f"\n## âš ï¸ è­¦å‘Š\n"
            for warning in health_data['warnings']:
                report += f"- {warning}\n"
        
        if health_data['recommendations']:
            report += f"\n## ğŸ”§ ä¿®å¤å»ºè®®\n"
            for rec in health_data['recommendations']:
                report += f"- {rec}\n"
        
        report += f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        
        if save_to_file:
            report_file = f"logs/health_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            os.makedirs('logs', exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ å¥åº·æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def monitor_continuous(self, interval_minutes: int = 30):
        """æŒç»­ç›‘æ§æ¨¡å¼"""
        logger.info(f"ğŸ”„ å¯åŠ¨æŒç»­ç›‘æ§æ¨¡å¼ï¼Œæ£€æŸ¥é—´éš”: {interval_minutes}åˆ†é’Ÿ")
        
        while True:
            try:
                health_data = self.check_system_health()
                
                # å¦‚æœå‘ç°ä¸¥é‡é—®é¢˜ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
                if health_data['overall_status'] == 'CRITICAL':
                    logger.warning("ğŸš¨ æ£€æµ‹åˆ°ä¸¥é‡é—®é¢˜ï¼Œå¯åŠ¨è‡ªåŠ¨ä¿®å¤...")
                    repair_result = self.auto_repair(health_data)
                    logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤å®Œæˆ: {repair_result}")
                
                # è®°å½•ç›‘æ§ç»“æœ
                self._log_monitoring_result(health_data)
                
                time.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                logger.error(f"âŒ ç›‘æ§è¿‡ç¨‹å¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­
    
    def _log_monitoring_result(self, health_data: Dict):
        """è®°å½•ç›‘æ§ç»“æœåˆ°æ•°æ®åº“"""
        try:
            with db.engine.connect() as conn:
                # åˆ›å»ºç›‘æ§æ—¥å¿—è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                conn.execute(text("""
                    CREATE TABLE IF NOT EXISTS data_health_log (
                        id BIGINT AUTO_INCREMENT PRIMARY KEY,
                        check_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        overall_status VARCHAR(20),
                        overall_score DECIMAL(5,2),
                        critical_issues_count INT,
                        warnings_count INT,
                        details JSON,
                        INDEX idx_check_time (check_time),
                        INDEX idx_status (overall_status)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ•°æ®å¥åº·ç›‘æ§æ—¥å¿—'
                """))
                
                # æ’å…¥ç›‘æ§è®°å½•
                conn.execute(text("""
                    INSERT INTO data_health_log 
                    (overall_status, overall_score, critical_issues_count, warnings_count, details)
                    VALUES (:status, :score, :critical_count, :warning_count, :details)
                """), {
                    'status': health_data['overall_status'],
                    'score': health_data['overall_score'],
                    'critical_count': len(health_data['critical_issues']),
                    'warning_count': len(health_data['warnings']),
                    'details': json.dumps(health_data, ensure_ascii=False, default=str)
                })
                conn.commit()
                
        except Exception as e:
            logger.error(f"è®°å½•ç›‘æ§ç»“æœå¤±è´¥: {e}")
    
    def _comprehensive_quality_check(self, table_name: str, conn) -> Dict:
        """ç»¼åˆæ•°æ®è´¨é‡æ£€æŸ¥"""
        metrics = {}
        
        try:
            # é‡å¤æ•°æ®æ£€æŸ¥
            if self.quality_rules['duplicate_check']:
                metrics['duplicate_rate'] = self._check_duplicate_rate(table_name, conn)
            
            # ç©ºå€¼ç‡æ£€æŸ¥
            metrics['null_rates'] = self._check_null_rates(table_name, conn)
            
            # æ•°æ®ç±»å‹ä¸€è‡´æ€§
            metrics['type_consistency'] = self._check_type_consistency(table_name, conn)
            
            # æ•°æ®åˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥
            if self.quality_rules['outlier_detection']:
                metrics['outlier_detection'] = self._detect_outliers(table_name, conn)
            
            # æ•°æ®å¢é•¿è¶‹åŠ¿
            metrics['growth_trend'] = self._analyze_growth_trend(table_name, conn)
            
        except Exception as e:
            logger.warning(f"è´¨é‡æ£€æŸ¥å¼‚å¸¸ {table_name}: {e}")
            metrics['error'] = str(e)
        
        return metrics
    
    def _check_required_fields(self, table_name: str, conn) -> List[str]:
        """æ£€æŸ¥å¿…éœ€å­—æ®µ"""
        missing_fields = []
        
        if table_name in self.quality_standards:
            required_fields = self.quality_standards[table_name]['required_fields']
            for field in required_fields:
                try:
                    conn.execute(text(f"SELECT {field} FROM {table_name} LIMIT 1"))
                except:
                    missing_fields.append(field)
        
        return missing_fields
    
    def _check_data_completeness(self, table_name: str, conn) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§è¯„åˆ†"""
        try:
            if table_name not in self.quality_standards:
                return 100.0
            
            required_fields = self.quality_standards[table_name]['required_fields']
            total_score = 0
            
            for field in required_fields:
                try:
                    # è®¡ç®—éç©ºç‡
                    total_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).scalar()
                    non_null_count = conn.execute(text(f"SELECT COUNT({field}) FROM {table_name}")).scalar()
                    
                    if total_count > 0:
                        completeness = (non_null_count / total_count) * 100
                        total_score += completeness
                except:
                    total_score += 0
            
            return total_score / len(required_fields) if required_fields else 100.0
            
        except Exception as e:
            logger.warning(f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
            return 0.0
    
    def _check_data_consistency(self, table_name: str, conn) -> float:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§è¯„åˆ†"""
        try:
            consistency_score = 100.0
            
            # æ£€æŸ¥ts_codeæ ¼å¼ä¸€è‡´æ€§
            if 'ts_code' in self.quality_standards.get(table_name, {}).get('required_fields', []):
                try:
                    invalid_codes = conn.execute(text(f"""
                        SELECT COUNT(*) FROM {table_name}
                        WHERE ts_code NOT REGEXP '^[0-9]{{6}}\\.(SH|SZ)$'
                    """)).scalar()
                    
                    total_codes = conn.execute(text(f"SELECT COUNT(*) FROM {table_name} WHERE ts_code IS NOT NULL")).scalar()
                    
                    if total_codes > 0:
                        consistency_score -= (invalid_codes / total_codes) * 30
                except:
                    pass
            
            # æ£€æŸ¥æ—¥æœŸæ ¼å¼ä¸€è‡´æ€§
            date_fields = ['trade_date', 'end_date', 'ann_date']
            for field in date_fields:
                try:
                    invalid_dates = conn.execute(text(f"""
                        SELECT COUNT(*) FROM {table_name}
                        WHERE {field} IS NOT NULL AND {field} NOT REGEXP '^[0-9]{{8}}$'
                    """)).scalar()
                    
                    total_dates = conn.execute(text(f"SELECT COUNT(*) FROM {table_name} WHERE {field} IS NOT NULL")).scalar()
                    
                    if total_dates > 0:
                        consistency_score -= (invalid_dates / total_dates) * 10
                except:
                    continue
            
            return max(0.0, consistency_score)
            
        except Exception as e:
            logger.warning(f"ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
            return 50.0
    
    def _check_duplicate_rate(self, table_name: str, conn) -> float:
        """æ£€æŸ¥é‡å¤æ•°æ®ç‡"""
        try:
            total_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).scalar()
            
            # æ ¹æ®è¡¨ç±»å‹é€‰æ‹©å»é‡å­—æ®µ
            if 'ts_code' in self.quality_standards.get(table_name, {}).get('required_fields', []):
                if 'trade_date' in self.quality_standards.get(table_name, {}).get('required_fields', []):
                    unique_count = conn.execute(text(f"SELECT COUNT(DISTINCT ts_code, trade_date) FROM {table_name}")).scalar()
                else:
                    unique_count = conn.execute(text(f"SELECT COUNT(DISTINCT ts_code) FROM {table_name}")).scalar()
            else:
                unique_count = total_count
            
            if total_count > 0:
                return ((total_count - unique_count) / total_count) * 100
            return 0.0
            
        except Exception as e:
            logger.warning(f"é‡å¤ç‡æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
            return 0.0
    
    def _check_null_rates(self, table_name: str, conn) -> Dict:
        """æ£€æŸ¥å„å­—æ®µç©ºå€¼ç‡"""
        null_rates = {}
        
        try:
            if table_name in self.quality_standards:
                required_fields = self.quality_standards[table_name]['required_fields']
                total_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).scalar()
                
                for field in required_fields:
                    try:
                        null_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name} WHERE {field} IS NULL")).scalar()
                        null_rates[field] = (null_count / total_count) * 100 if total_count > 0 else 0
                    except:
                        null_rates[field] = 100.0
        
        except Exception as e:
            logger.warning(f"ç©ºå€¼ç‡æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
        
        return null_rates
    
    def _check_type_consistency(self, table_name: str, conn) -> Dict:
        """æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§"""
        type_consistency = {}
        
        try:
            if table_name in self.quality_standards:
                data_types = self.quality_standards[table_name].get('data_types', {})
                
                for field, expected_type in data_types.items():
                    try:
                        # ç®€å•çš„ç±»å‹æ£€æŸ¥
                        if expected_type == 'float':
                            invalid_count = conn.execute(text(f"""
                                SELECT COUNT(*) FROM {table_name}
                                WHERE {field} IS NOT NULL AND {field} NOT REGEXP '^-?[0-9]+(\\.[0-9]+)?$'
                            """)).scalar()
                        elif expected_type == 'int':
                            invalid_count = conn.execute(text(f"""
                                SELECT COUNT(*) FROM {table_name}
                                WHERE {field} IS NOT NULL AND {field} NOT REGEXP '^-?[0-9]+$'
                            """)).scalar()
                        else:
                            invalid_count = 0
                        
                        total_count = conn.execute(text(f"SELECT COUNT(*) FROM {table_name} WHERE {field} IS NOT NULL")).scalar()
                        
                        type_consistency[field] = {
                            'expected_type': expected_type,
                            'invalid_count': invalid_count,
                            'consistency_rate': ((total_count - invalid_count) / total_count * 100) if total_count > 0 else 100
                        }
                    except:
                        type_consistency[field] = {'error': 'check_failed'}
        
        except Exception as e:
            logger.warning(f"ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
        
        return type_consistency
    
    def _detect_outliers(self, table_name: str, conn) -> Dict:
        """æ£€æµ‹æ•°æ®å¼‚å¸¸å€¼"""
        outliers = {}
        
        try:
            # æ£€æŸ¥æ•°å€¼å­—æ®µçš„å¼‚å¸¸å€¼
            numeric_fields = ['close', 'volume', 'amount', 'pe', 'pb', 'total_revenue']
            
            for field in numeric_fields:
                try:
                    # ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
                    result = conn.execute(text(f"""
                        SELECT
                            COUNT(*) as total,
                            AVG({field}) as mean_val,
                            STD({field}) as std_val,
                            MIN({field}) as min_val,
                            MAX({field}) as max_val
                        FROM {table_name}
                        WHERE {field} IS NOT NULL AND {field} > 0
                    """)).fetchone()
                    
                    if result and result.total > 0:
                        outliers[field] = {
                            'total_records': result.total,
                            'mean': float(result.mean_val) if result.mean_val else 0,
                            'std': float(result.std_val) if result.std_val else 0,
                            'min': float(result.min_val) if result.min_val else 0,
                            'max': float(result.max_val) if result.max_val else 0
                        }
                except:
                    continue
        
        except Exception as e:
            logger.warning(f"å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥ {table_name}: {e}")
        
        return outliers
    
    def _analyze_growth_trend(self, table_name: str, conn) -> Dict:
        """åˆ†ææ•°æ®å¢é•¿è¶‹åŠ¿"""
        try:
            # æ£€æŸ¥æœ€è¿‘7å¤©çš„æ•°æ®å¢é•¿
            if 'trade_date' in self.quality_standards.get(table_name, {}).get('required_fields', []):
                result = conn.execute(text(f"""
                    SELECT
                        DATE(STR_TO_DATE(trade_date, '%Y%m%d')) as date,
                        COUNT(*) as daily_count
                    FROM {table_name}
                    WHERE trade_date >= DATE_FORMAT(DATE_SUB(NOW(), INTERVAL 7 DAY), '%Y%m%d')
                    GROUP BY DATE(STR_TO_DATE(trade_date, '%Y%m%d'))
                    ORDER BY date DESC
                    LIMIT 7
                """)).fetchall()
                
                if result:
                    daily_counts = [row.daily_count for row in result]
                    return {
                        'recent_days': len(daily_counts),
                        'daily_counts': daily_counts,
                        'avg_daily_growth': sum(daily_counts) / len(daily_counts),
                        'trend': 'increasing' if len(daily_counts) > 1 and daily_counts[0] > daily_counts[-1] else 'stable'
                    }
            
            return {'status': 'no_date_field'}
            
        except Exception as e:
            logger.warning(f"å¢é•¿è¶‹åŠ¿åˆ†æå¤±è´¥ {table_name}: {e}")
            return {'error': str(e)}
    
    def _calculate_enhanced_health_score(self, table_name: str, record_count: int, missing_fields: List,
                                       data_freshness: Dict, completeness_score: float,
                                       consistency_score: float, quality_metrics: Dict) -> int:
        """è®¡ç®—å¢å¼ºçš„å¥åº·è¯„åˆ†"""
        score = 100
        
        # è®°å½•æ•°é‡è¯„åˆ† (25åˆ†)
        if table_name in self.quality_standards:
            min_records = self.quality_standards[table_name]['min_records']
            if record_count == 0:
                score -= 25
            elif record_count < min_records * 0.5:
                score -= 20
            elif record_count < min_records:
                score -= 10
        
        # å­—æ®µå®Œæ•´æ€§è¯„åˆ† (20åˆ†)
        if missing_fields:
            score -= len(missing_fields) * 5
        
        # æ•°æ®æ–°é²œåº¦è¯„åˆ† (20åˆ†)
        days_old = data_freshness.get('days_old', 999)
        if days_old > 30:
            score -= 20
        elif days_old > 7:
            score -= 15
        elif days_old > 3:
            score -= 5
        
        # æ•°æ®å®Œæ•´æ€§è¯„åˆ† (15åˆ†)
        score -= (100 - completeness_score) * 0.15
        
        # æ•°æ®ä¸€è‡´æ€§è¯„åˆ† (15åˆ†)
        score -= (100 - consistency_score) * 0.15
        
        # è´¨é‡æŒ‡æ ‡è¯„åˆ† (5åˆ†)
        duplicate_rate = quality_metrics.get('duplicate_rate', 0)
        if duplicate_rate > 10:
            score -= 5
        elif duplicate_rate > 5:
            score -= 3
        elif duplicate_rate > 1:
            score -= 1
        
        return max(0, int(score))
    
    def _generate_detailed_issues(self, table_name: str, record_count: int, missing_fields: List,
                                data_freshness: Dict, completeness_score: float,
                                consistency_score: float, quality_metrics: Dict) -> List[str]:
        """ç”Ÿæˆè¯¦ç»†é—®é¢˜åˆ—è¡¨"""
        issues = []
        
        # è®°å½•æ•°é‡é—®é¢˜
        if table_name in self.quality_standards:
            min_records = self.quality_standards[table_name]['min_records']
            if record_count < min_records:
                issues.append(f"è®°å½•æ•°ä¸è¶³: {record_count:,} < {min_records:,}")
        
        # å­—æ®µç¼ºå¤±é—®é¢˜
        if missing_fields:
            issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
        
        # æ•°æ®æ–°é²œåº¦é—®é¢˜
        days_old = data_freshness.get('days_old', 999)
        if days_old > 7:
            issues.append(f"æ•°æ®è¿‡æœŸ: {days_old}å¤©å‰")
        
        # å®Œæ•´æ€§é—®é¢˜
        if completeness_score < 90:
            issues.append(f"æ•°æ®å®Œæ•´æ€§ä¸è¶³: {completeness_score:.1f}%")
        
        # ä¸€è‡´æ€§é—®é¢˜
        if consistency_score < 90:
            issues.append(f"æ•°æ®ä¸€è‡´æ€§é—®é¢˜: {consistency_score:.1f}%")
        
        # é‡å¤æ•°æ®é—®é¢˜
        duplicate_rate = quality_metrics.get('duplicate_rate', 0)
        if duplicate_rate > 5:
            issues.append(f"é‡å¤æ•°æ®ç‡è¿‡é«˜: {duplicate_rate:.1f}%")
        
        # ç©ºå€¼ç‡é—®é¢˜
        null_rates = quality_metrics.get('null_rates', {})
        for field, rate in null_rates.items():
            tolerance = self.quality_standards.get(table_name, {}).get('null_tolerance', 0.05) * 100
            if rate > tolerance:
                issues.append(f"å­—æ®µ{field}ç©ºå€¼ç‡è¿‡é«˜: {rate:.1f}% > {tolerance}%")
        
        return issues
    
    def _determine_health_status(self, health_score: int, issues: List[str]) -> str:
        """ç¡®å®šå¥åº·çŠ¶æ€"""
        if health_score >= 85:
            return 'HEALTHY'
        elif health_score >= 70:
            return 'WARNING'
        else:
            return 'CRITICAL'

# åˆ›å»ºå…¨å±€å®ä¾‹
data_health_checker = DataHealthChecker()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®å¥åº·æ£€æŸ¥å™¨')
    parser.add_argument('command', choices=['check', 'repair', 'report', 'monitor'], 
                       help='æ“ä½œå‘½ä»¤')
    parser.add_argument('--interval', type=int, default=30,
                       help='ç›‘æ§é—´éš”(åˆ†é’Ÿ)')
    parser.add_argument('--auto-repair', action='store_true',
                       help='è‡ªåŠ¨ä¿®å¤é—®é¢˜')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('logs/data_health.log', encoding='utf-8')
        ]
    )
    
    try:
        if args.command == 'check':
            health_data = data_health_checker.check_system_health()
            print(json.dumps(health_data, indent=2, ensure_ascii=False, default=str))
            
            if args.auto_repair and health_data['overall_status'] in ['CRITICAL', 'WARNING']:
                repair_result = data_health_checker.auto_repair(health_data)
                print("ä¿®å¤ç»“æœ:", json.dumps(repair_result, indent=2, ensure_ascii=False, default=str))
                
        elif args.command == 'repair':
            health_data = data_health_checker.check_system_health()
            repair_result = data_health_checker.auto_repair(health_data)
            print("ä¿®å¤ç»“æœ:", json.dumps(repair_result, indent=2, ensure_ascii=False, default=str))
            
        elif args.command == 'report':
            report = data_health_checker.generate_health_report()
            print(report)
            
        elif args.command == 'monitor':
            data_health_checker.monitor_continuous(args.interval)
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()