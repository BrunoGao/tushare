"""
M2 Ultra ç¡¬ä»¶ä¼˜åŒ–é…ç½®ç®¡ç†å™¨
é’ˆå¯¹Apple M2 UltraèŠ¯ç‰‡çš„ä¸“é¡¹ä¼˜åŒ–é…ç½®
"""

import os
import psutil
import platform
import subprocess
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class M2UltraConfig:
    """M2 Ultraä¼˜åŒ–é…ç½®ç®¡ç†å™¨"""
    
    # M2 Ultraç¡¬ä»¶è§„æ ¼
    CPU_CORES = 24
    GPU_CORES = 76
    MEMORY_GB = 192
    ARCHITECTURE = "arm64"
    
    def __init__(self):
        self.is_m2_ultra = self._detect_m2_ultra()
        self.available_memory = self._get_available_memory_gb()
        self.cpu_count = os.cpu_count()
        
        if self.is_m2_ultra:
            logger.info(f"âœ… æ£€æµ‹åˆ°M2 UltraèŠ¯ç‰‡ï¼Œå¯ç”¨ä¼˜åŒ–é…ç½®")
            self._setup_environment()
        else:
            logger.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°M2 Ultraï¼Œä½¿ç”¨é€šç”¨é…ç½®")
    
    def _detect_m2_ultra(self) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºM2 UltraèŠ¯ç‰‡"""
        try:
            if platform.system() != "Darwin":
                return False
                
            result = subprocess.run(
                ["system_profiler", "SPHardwareDataType"], 
                capture_output=True, text=True
            )
            
            output = result.stdout
            return "M2 Ultra" in output and "192 GB" in output
            
        except Exception as e:
            logger.error(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _get_available_memory_gb(self) -> float:
        """è·å–å¯ç”¨å†…å­˜ï¼ˆGBï¼‰"""
        try:
            memory = psutil.virtual_memory()
            return memory.available / (1024**3)
        except Exception:
            return 16.0  # é»˜è®¤å€¼
    
    def _setup_environment(self):
        """è®¾ç½®M2 Ultraä¼˜åŒ–ç¯å¢ƒå˜é‡"""
        env_vars = {
            'OMP_NUM_THREADS': str(self.CPU_CORES),
            'MKL_NUM_THREADS': str(self.CPU_CORES),
            'NUMEXPR_NUM_THREADS': str(self.CPU_CORES),
            'VECLIB_MAXIMUM_THREADS': str(self.CPU_CORES),
            'ACCELERATE_NEW_LAPACK': '1',  # ä½¿ç”¨Apple Accelerate
            'PYTORCH_ENABLE_MPS_FALLBACK': '1',  # MPSå›é€€æ”¯æŒ
        }
        
        for key, value in env_vars.items():
            os.environ[key] = value
            logger.info(f"è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
    
    def get_optimal_config(self, task_type: str) -> Dict[str, Any]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–æœ€ä¼˜é…ç½®"""
        
        base_config = {
            'cpu_cores': self.cpu_count,
            'memory_gb': self.available_memory,
            'is_m2_ultra': self.is_m2_ultra
        }
        
        if not self.is_m2_ultra:
            return self._get_fallback_config(task_type, base_config)
        
        configs = {
            'data_processing': {
                'max_workers': min(20, self.CPU_CORES - 4),  # ä¿ç•™4æ ¸ç»™ç³»ç»Ÿ
                'chunk_size': 50000,  # å¤§å†…å­˜å…è®¸æ›´å¤§chunk
                'memory_limit_gb': min(64, self.available_memory * 0.4),
                'use_multiprocessing': True,
                'prefetch_factor': 4,
                'pin_memory': True,
                **base_config
            },
            
            'traditional_ml': {
                'xgboost_threads': self.CPU_CORES,
                'lightgbm_threads': self.CPU_CORES,
                'sklearn_jobs': self.CPU_CORES,
                'memory_limit_gb': min(128, self.available_memory * 0.7),
                'tree_method': 'hist',  # é€‚åˆå¤§å†…å­˜
                'max_bin': 1024,  # åˆ©ç”¨å¤§å†…å­˜æé«˜ç²¾åº¦
                'cache_enabled': True,
                'cache_size_gb': min(32, self.available_memory * 0.2),
                **base_config
            },
            
            'deep_learning': {
                'device': 'mps' if self._check_mps_available() else 'cpu',
                'batch_size_multiplier': 8 if self.available_memory > 100 else 4,
                'num_workers': 16,
                'pin_memory': True,
                'persistent_workers': True,
                'prefetch_factor': 4,
                'memory_limit_gb': min(96, self.available_memory * 0.5),
                'use_mixed_precision': True,  # MPSæ”¯æŒæ··åˆç²¾åº¦
                **base_config
            },
            
            'model_optimization': {
                'optuna_n_jobs': min(16, self.CPU_CORES - 8),
                'n_trials': 200,  # å¤§å†…å­˜å…è®¸æ›´å¤štrials
                'memory_limit_gb': min(80, self.available_memory * 0.4),
                'parallel_backend': 'multiprocessing',
                **base_config
            }
        }
        
        return configs.get(task_type, base_config)
    
    def _check_mps_available(self) -> bool:
        """æ£€æŸ¥MPSæ˜¯å¦å¯ç”¨"""
        try:
            import torch
            return torch.backends.mps.is_available()
        except ImportError:
            return False
    
    def _get_fallback_config(self, task_type: str, base_config: Dict) -> Dict[str, Any]:
        """éM2 Ultraçš„å›é€€é…ç½®"""
        fallback_configs = {
            'data_processing': {
                'max_workers': min(8, self.cpu_count - 2),
                'chunk_size': 10000,
                'memory_limit_gb': min(16, self.available_memory * 0.5),
                **base_config
            },
            'traditional_ml': {
                'xgboost_threads': self.cpu_count,
                'lightgbm_threads': self.cpu_count,
                'sklearn_jobs': self.cpu_count,
                'memory_limit_gb': min(32, self.available_memory * 0.6),
                'tree_method': 'approx',
                'max_bin': 256,
                **base_config
            },
            'deep_learning': {
                'device': 'cpu',
                'batch_size_multiplier': 2,
                'num_workers': 4,
                'pin_memory': False,
                'memory_limit_gb': min(24, self.available_memory * 0.4),
                **base_config
            }
        }
        
        return fallback_configs.get(task_type, base_config)
    
    def get_xgboost_config(self) -> Dict[str, Any]:
        """è·å–XGBoostä¼˜åŒ–é…ç½®"""
        if not self.is_m2_ultra:
            return {
                'nthread': self.cpu_count,
                'tree_method': 'approx',
                'max_bin': 256
            }
        
        return {
            'nthread': self.CPU_CORES,
            'tree_method': 'hist',  # M2 Ultraå¤§å†…å­˜ä¼˜åŒ–
            'max_bin': 1024,  # æ›´é«˜ç²¾åº¦
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'objective': 'multi:softprob',
            'eval_metric': 'mlogloss',
            'verbosity': 1,
            'use_label_encoder': False,
            # M2 Ultraç‰¹å®šä¼˜åŒ–
            'max_delta_step': 1,
            'scale_pos_weight': 1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0
        }
    
    def get_lightgbm_config(self) -> Dict[str, Any]:
        """è·å–LightGBMä¼˜åŒ–é…ç½®"""
        if not self.is_m2_ultra:
            return {
                'num_threads': self.cpu_count,
                'max_bin': 255,
                'device_type': 'cpu'
            }
        
        return {
            'num_threads': self.CPU_CORES,
            'device_type': 'cpu',  # Apple Siliconä¼˜åŒ–
            'max_bin': 1024,  # M2 Ultraå¤§å†…å­˜ä¼˜åŒ–
            'bagging_fraction': 0.8,
            'feature_fraction': 0.8,
            'objective': 'multiclass',
            'metric': 'multi_logloss',
            'verbose': 1,
            'force_col_wise': True,  # å†…å­˜ä¼˜åŒ–
            'histogram_pool_size': 512,  # å¤§å†…å­˜ä¼˜åŒ–
            'max_depth': -1,
            'learning_rate': 0.1,
            'num_leaves': 31,
            'min_data_in_leaf': 20,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1
        }
    
    def get_pytorch_config(self) -> Dict[str, Any]:
        """è·å–PyTorchä¼˜åŒ–é…ç½®"""
        config = {
            'device': 'cpu',
            'num_workers': 4,
            'batch_size_base': 32,
            'pin_memory': False,
            'persistent_workers': False
        }
        
        if self.is_m2_ultra and self._check_mps_available():
            config.update({
                'device': 'mps',
                'num_workers': 16,
                'batch_size_base': 256,  # M2 Ultraå¤§å†…å­˜ä¼˜åŒ–
                'pin_memory': True,
                'persistent_workers': True,
                'prefetch_factor': 4,
                'enable_mixed_precision': True,
                # MPSç‰¹å®šé…ç½®
                'mps_sync_allocator': True,
                'mps_metal_sync': True
            })
        
        return config
    
    def get_memory_config(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç®¡ç†é…ç½®"""
        if not self.is_m2_ultra:
            return {
                'cache_size_gb': min(4, self.available_memory * 0.2),
                'max_memory_usage': 0.6,
                'enable_memory_mapping': False
            }
        
        return {
            'cache_size_gb': min(32, self.available_memory * 0.2),  # 192GBçš„20%
            'max_memory_usage': 0.8,  # M2 Ultraå¯ä»¥ä½¿ç”¨æ›´å¤šå†…å­˜
            'enable_memory_mapping': True,  # å¤§æ–‡ä»¶å†…å­˜æ˜ å°„
            'prefetch_size_gb': 8,  # é¢„è¯»å–å¤§å°
            'compress_cache': False,  # å¤§å†…å­˜ä¸éœ€è¦å‹ç¼©
            'memory_pool_size_gb': 16,  # å†…å­˜æ± å¤§å°
        }
    
    def get_parallel_config(self) -> Dict[str, Any]:
        """è·å–å¹¶è¡Œå¤„ç†é…ç½®"""
        if not self.is_m2_ultra:
            return {
                'max_workers': min(4, self.cpu_count - 1),
                'backend': 'threading',
                'chunk_size': 1000
            }
        
        return {
            'max_workers': min(20, self.CPU_CORES - 4),  # ä¿ç•™4æ ¸ç»™ç³»ç»Ÿ
            'backend': 'multiprocessing',  # M2 Ultraå¤šè¿›ç¨‹æ€§èƒ½æ›´å¥½
            'chunk_size': 10000,  # å¤§å†…å­˜å…è®¸æ›´å¤§chunk
            'maxtasksperchild': 1000,  # é˜²æ­¢å†…å­˜æ³„æ¼
            'context': 'spawn',  # macOSæ¨è
        }
    
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ–¥ï¸  M2 Ultra ç³»ç»Ÿä¿¡æ¯")
        print("="*60)
        print(f"èŠ¯ç‰‡ç±»å‹: {'M2 Ultra' if self.is_m2_ultra else 'å…¶ä»–'}")
        print(f"CPUæ ¸å¿ƒæ•°: {self.cpu_count}")
        print(f"å¯ç”¨å†…å­˜: {self.available_memory:.1f} GB")
        print(f"ç³»ç»Ÿæ¶æ„: {platform.machine()}")
        
        if self._check_mps_available():
            print(f"MPSæ”¯æŒ: âœ… å¯ç”¨")
        else:
            print(f"MPSæ”¯æŒ: âŒ ä¸å¯ç”¨")
        
        print("="*60)

# å…¨å±€é…ç½®å®ä¾‹
m2_ultra_config = M2UltraConfig()

def get_optimal_config(task_type: str) -> Dict[str, Any]:
    """è·å–ä»»åŠ¡ç±»å‹çš„æœ€ä¼˜é…ç½®"""
    return m2_ultra_config.get_optimal_config(task_type)

def is_m2_ultra() -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºM2 Ultra"""
    return m2_ultra_config.is_m2_ultra

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    config = M2UltraConfig()
    config.print_system_info()
    
    print("\nğŸ“Š å„ä»»åŠ¡ç±»å‹ä¼˜åŒ–é…ç½®:")
    for task in ['data_processing', 'traditional_ml', 'deep_learning']:
        print(f"\n{task}:")
        task_config = config.get_optimal_config(task)
        for key, value in task_config.items():
            print(f"  {key}: {value}")