#!/usr/bin/env python3
"""
Redisç¼“å­˜ç®¡ç†å™¨
æä¾›æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œå¤§å¹…æå‡ç³»ç»Ÿå“åº”é€Ÿåº¦
æ”¯æŒå¤šå±‚ç¼“å­˜ã€è‡ªåŠ¨å¤±æ•ˆã€é¢„çƒ­ç­‰åŠŸèƒ½
"""
import redis
import json
import pickle
import logging
import time
import hashlib
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union, Callable
from functools import wraps
import threading
from concurrent.futures import ThreadPoolExecutor
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config

logger = logging.getLogger(__name__)

class RedisCacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, host='localhost', port=6379, db=0, password=None):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=host,
                port=port,
                db=db,
                password=password,
                decode_responses=False,  # ä¿æŒäºŒè¿›åˆ¶æ¨¡å¼ä»¥æ”¯æŒpickle
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True,
                health_check_interval=30
            )
            
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info("âœ… Redisè¿æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            # ä½¿ç”¨å†…å­˜ç¼“å­˜ä½œä¸ºé™çº§æ–¹æ¡ˆ
            self.redis_client = None
            self._memory_cache = {}
            self._memory_cache_ttl = {}
        
        # ç¼“å­˜é…ç½®
        self.cache_config = {
            # æ¨èç»“æœç¼“å­˜ (5åˆ†é’Ÿ)
            'recommendations': {'ttl': 300, 'prefix': 'rec'},
            # æŠ€æœ¯æŒ‡æ ‡ç¼“å­˜ (1åˆ†é’Ÿ)
            'indicators': {'ttl': 60, 'prefix': 'ind'},
            # å®æ—¶æ•°æ®ç¼“å­˜ (30ç§’)
            'realtime': {'ttl': 30, 'prefix': 'rt'},
            # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ç¼“å­˜ (1å°æ—¶)
            'stock_basic': {'ttl': 3600, 'prefix': 'sb'},
            # è´¢åŠ¡æ•°æ®ç¼“å­˜ (30åˆ†é’Ÿ)
            'financial': {'ttl': 1800, 'prefix': 'fin'},
            # èµ„é‡‘æµæ•°æ®ç¼“å­˜ (2åˆ†é’Ÿ)
            'money_flow': {'ttl': 120, 'prefix': 'mf'},
            # ç”¨æˆ·ä¼šè¯ç¼“å­˜ (24å°æ—¶)
            'session': {'ttl': 86400, 'prefix': 'sess'},
            # ç³»ç»ŸçŠ¶æ€ç¼“å­˜ (5åˆ†é’Ÿ)
            'system_status': {'ttl': 300, 'prefix': 'sys'}
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'errors': 0
        }
        
        # çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥æ“ä½œ
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.lock = threading.Lock()
    
    def _generate_key(self, category: str, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        prefix = self.cache_config.get(category, {}).get('prefix', 'cache')
        return f"{prefix}:{key}"
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        try:
            if isinstance(data, (dict, list)):
                return json.dumps(data, ensure_ascii=False, default=str).encode('utf-8')
            else:
                return pickle.dumps(data)
        except Exception as e:
            logger.error(f"æ•°æ®åºåˆ—åŒ–å¤±è´¥: {e}")
            return pickle.dumps(data)
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        try:
            # å°è¯•JSONååºåˆ—åŒ–
            return json.loads(data.decode('utf-8'))
        except:
            try:
                # å°è¯•pickleååºåˆ—åŒ–
                return pickle.loads(data)
            except Exception as e:
                logger.error(f"æ•°æ®ååºåˆ—åŒ–å¤±è´¥: {e}")
                return None
    
    def get(self, category: str, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_key(category, key)
        
        try:
            if self.redis_client:
                # Redisç¼“å­˜
                data = self.redis_client.get(cache_key)
                if data:
                    with self.lock:
                        self.stats['hits'] += 1
                    return self._deserialize_data(data)
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                if cache_key in self._memory_cache:
                    # æ£€æŸ¥TTL
                    if cache_key in self._memory_cache_ttl:
                        if time.time() > self._memory_cache_ttl[cache_key]:
                            del self._memory_cache[cache_key]
                            del self._memory_cache_ttl[cache_key]
                        else:
                            with self.lock:
                                self.stats['hits'] += 1
                            return self._memory_cache[cache_key]
            
            with self.lock:
                self.stats['misses'] += 1
            return None
            
        except Exception as e:
            logger.error(f"ç¼“å­˜è·å–å¤±è´¥ {cache_key}: {e}")
            with self.lock:
                self.stats['errors'] += 1
            return None
    
    def set(self, category: str, key: str, data: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_key(category, key)
        
        if ttl is None:
            ttl = self.cache_config.get(category, {}).get('ttl', 300)
        
        try:
            serialized_data = self._serialize_data(data)
            
            if self.redis_client:
                # Redisç¼“å­˜
                result = self.redis_client.setex(cache_key, ttl, serialized_data)
                if result:
                    with self.lock:
                        self.stats['sets'] += 1
                    return True
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                self._memory_cache[cache_key] = data
                self._memory_cache_ttl[cache_key] = time.time() + ttl
                with self.lock:
                    self.stats['sets'] += 1
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥ {cache_key}: {e}")
            with self.lock:
                self.stats['errors'] += 1
            return False
    
    def delete(self, category: str, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_key(category, key)
        
        try:
            if self.redis_client:
                result = self.redis_client.delete(cache_key)
                if result:
                    with self.lock:
                        self.stats['deletes'] += 1
                    return True
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                if cache_key in self._memory_cache:
                    del self._memory_cache[cache_key]
                    if cache_key in self._memory_cache_ttl:
                        del self._memory_cache_ttl[cache_key]
                    with self.lock:
                        self.stats['deletes'] += 1
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ é™¤å¤±è´¥ {cache_key}: {e}")
            with self.lock:
                self.stats['errors'] += 1
            return False
    
    def clear_category(self, category: str) -> int:
        """æ¸…ç©ºæŒ‡å®šç±»åˆ«çš„æ‰€æœ‰ç¼“å­˜"""
        try:
            prefix = self.cache_config.get(category, {}).get('prefix', 'cache')
            pattern = f"{prefix}:*"
            
            if self.redis_client:
                keys = self.redis_client.keys(pattern)
                if keys:
                    deleted = self.redis_client.delete(*keys)
                    with self.lock:
                        self.stats['deletes'] += deleted
                    return deleted
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                deleted = 0
                keys_to_delete = [k for k in self._memory_cache.keys() if k.startswith(f"{prefix}:")]
                for key in keys_to_delete:
                    del self._memory_cache[key]
                    if key in self._memory_cache_ttl:
                        del self._memory_cache_ttl[key]
                    deleted += 1
                
                with self.lock:
                    self.stats['deletes'] += deleted
                return deleted
            
            return 0
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜ç±»åˆ«å¤±è´¥ {category}: {e}")
            with self.lock:
                self.stats['errors'] += 1
            return 0
    
    def exists(self, category: str, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_key = self._generate_key(category, key)
        
        try:
            if self.redis_client:
                return bool(self.redis_client.exists(cache_key))
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                if cache_key in self._memory_cache:
                    # æ£€æŸ¥TTL
                    if cache_key in self._memory_cache_ttl:
                        if time.time() > self._memory_cache_ttl[cache_key]:
                            del self._memory_cache[cache_key]
                            del self._memory_cache_ttl[cache_key]
                            return False
                    return True
                return False
                
        except Exception as e:
            logger.error(f"ç¼“å­˜å­˜åœ¨æ€§æ£€æŸ¥å¤±è´¥ {cache_key}: {e}")
            return False
    
    def get_ttl(self, category: str, key: str) -> int:
        """è·å–ç¼“å­˜å‰©ä½™TTL"""
        cache_key = self._generate_key(category, key)
        
        try:
            if self.redis_client:
                return self.redis_client.ttl(cache_key)
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                if cache_key in self._memory_cache_ttl:
                    remaining = self._memory_cache_ttl[cache_key] - time.time()
                    return int(max(0, remaining))
                return -1
                
        except Exception as e:
            logger.error(f"TTLè·å–å¤±è´¥ {cache_key}: {e}")
            return -1
    
    def extend_ttl(self, category: str, key: str, additional_seconds: int) -> bool:
        """å»¶é•¿ç¼“å­˜TTL"""
        cache_key = self._generate_key(category, key)
        
        try:
            if self.redis_client:
                current_ttl = self.redis_client.ttl(cache_key)
                if current_ttl > 0:
                    return bool(self.redis_client.expire(cache_key, current_ttl + additional_seconds))
            else:
                # å†…å­˜ç¼“å­˜é™çº§
                if cache_key in self._memory_cache_ttl:
                    self._memory_cache_ttl[cache_key] += additional_seconds
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"TTLå»¶é•¿å¤±è´¥ {cache_key}: {e}")
            return False
    
    def get_or_set(self, category: str, key: str, fetch_func: Callable, ttl: Optional[int] = None) -> Any:
        """è·å–ç¼“å­˜ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ"""
        # å…ˆå°è¯•è·å–ç¼“å­˜
        cached_data = self.get(category, key)
        if cached_data is not None:
            return cached_data
        
        # ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ‰§è¡Œå‡½æ•°è·å–æ•°æ®
        try:
            data = fetch_func()
            if data is not None:
                self.set(category, key, data, ttl)
            return data
        except Exception as e:
            logger.error(f"æ•°æ®è·å–å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def batch_get(self, category: str, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–ç¼“å­˜"""
        results = {}
        
        if self.redis_client:
            # Redisæ‰¹é‡è·å–
            cache_keys = [self._generate_key(category, key) for key in keys]
            try:
                values = self.redis_client.mget(cache_keys)
                for i, value in enumerate(values):
                    if value:
                        results[keys[i]] = self._deserialize_data(value)
                        with self.lock:
                            self.stats['hits'] += 1
                    else:
                        with self.lock:
                            self.stats['misses'] += 1
            except Exception as e:
                logger.error(f"æ‰¹é‡è·å–å¤±è´¥: {e}")
        else:
            # å†…å­˜ç¼“å­˜é™çº§
            for key in keys:
                result = self.get(category, key)
                if result is not None:
                    results[key] = result
        
        return results
    
    def batch_set(self, category: str, data_dict: Dict[str, Any], ttl: Optional[int] = None) -> int:
        """æ‰¹é‡è®¾ç½®ç¼“å­˜"""
        if ttl is None:
            ttl = self.cache_config.get(category, {}).get('ttl', 300)
        
        success_count = 0
        
        if self.redis_client:
            # Redisæ‰¹é‡è®¾ç½®
            try:
                pipe = self.redis_client.pipeline()
                for key, data in data_dict.items():
                    cache_key = self._generate_key(category, key)
                    serialized_data = self._serialize_data(data)
                    pipe.setex(cache_key, ttl, serialized_data)
                
                results = pipe.execute()
                success_count = sum(1 for result in results if result)
                
                with self.lock:
                    self.stats['sets'] += success_count
                    
            except Exception as e:
                logger.error(f"æ‰¹é‡è®¾ç½®å¤±è´¥: {e}")
        else:
            # å†…å­˜ç¼“å­˜é™çº§
            for key, data in data_dict.items():
                if self.set(category, key, data, ttl):
                    success_count += 1
        
        return success_count
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            stats = self.stats.copy()
        
        # è®¡ç®—å‘½ä¸­ç‡
        total_requests = stats['hits'] + stats['misses']
        hit_rate = (stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        stats['hit_rate'] = round(hit_rate, 2)
        stats['total_requests'] = total_requests
        
        # è·å–Redisä¿¡æ¯
        if self.redis_client:
            try:
                redis_info = self.redis_client.info()
                stats['redis_info'] = {
                    'used_memory': redis_info.get('used_memory_human', 'N/A'),
                    'connected_clients': redis_info.get('connected_clients', 0),
                    'total_commands_processed': redis_info.get('total_commands_processed', 0),
                    'keyspace_hits': redis_info.get('keyspace_hits', 0),
                    'keyspace_misses': redis_info.get('keyspace_misses', 0)
                }
            except:
                stats['redis_info'] = {'status': 'unavailable'}
        else:
            stats['memory_cache_size'] = len(self._memory_cache)
        
        return stats
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'issues': []
        }
        
        try:
            if self.redis_client:
                # æµ‹è¯•Redisè¿æ¥
                start_time = time.time()
                self.redis_client.ping()
                response_time = (time.time() - start_time) * 1000
                
                health['redis_status'] = 'connected'
                health['response_time_ms'] = round(response_time, 2)
                
                # æ£€æŸ¥å“åº”æ—¶é—´
                if response_time > 100:
                    health['issues'].append(f'Rediså“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}ms')
                    health['status'] = 'warning'
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                redis_info = self.redis_client.info()
                used_memory_mb = redis_info.get('used_memory', 0) / 1024 / 1024
                if used_memory_mb > 1000:  # è¶…è¿‡1GB
                    health['issues'].append(f'Rediså†…å­˜ä½¿ç”¨è¿‡é«˜: {used_memory_mb:.1f}MB')
                    health['status'] = 'warning'
                
            else:
                health['redis_status'] = 'disconnected'
                health['fallback_mode'] = 'memory_cache'
                health['memory_cache_size'] = len(self._memory_cache)
                health['issues'].append('Redisä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜é™çº§')
                health['status'] = 'warning'
            
            # æ£€æŸ¥å‘½ä¸­ç‡
            stats = self.get_stats()
            if stats['hit_rate'] < 50 and stats['total_requests'] > 100:
                health['issues'].append(f'ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {stats["hit_rate"]}%')
                health['status'] = 'warning'
            
            if health['issues']:
                health['status'] = 'warning' if health['status'] != 'critical' else 'critical'
            
        except Exception as e:
            health['status'] = 'critical'
            health['error'] = str(e)
            health['issues'].append(f'å¥åº·æ£€æŸ¥å¤±è´¥: {e}')
        
        return health
    
    def preload_cache(self, preload_config: Dict[str, Callable]) -> Dict[str, int]:
        """é¢„çƒ­ç¼“å­˜"""
        logger.info("ğŸ”¥ å¼€å§‹ç¼“å­˜é¢„çƒ­...")
        results = {}
        
        for category, fetch_func in preload_config.items():
            try:
                logger.info(f"é¢„çƒ­ç¼“å­˜ç±»åˆ«: {category}")
                data = fetch_func()
                
                if isinstance(data, dict):
                    # æ‰¹é‡æ•°æ®
                    success_count = self.batch_set(category, data)
                    results[category] = success_count
                    logger.info(f"âœ… {category} é¢„çƒ­å®Œæˆ: {success_count}æ¡")
                else:
                    # å•æ¡æ•°æ®
                    if self.set(category, 'default', data):
                        results[category] = 1
                        logger.info(f"âœ… {category} é¢„çƒ­å®Œæˆ: 1æ¡")
                    else:
                        results[category] = 0
                        logger.warning(f"âš ï¸ {category} é¢„çƒ­å¤±è´¥")
                        
            except Exception as e:
                logger.error(f"âŒ {category} é¢„çƒ­å¼‚å¸¸: {e}")
                results[category] = 0
        
        logger.info(f"ğŸ”¥ ç¼“å­˜é¢„çƒ­å®Œæˆ: {results}")
        return results
    
    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸçš„å†…å­˜ç¼“å­˜ï¼ˆä»…åœ¨é™çº§æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰"""
        if self.redis_client:
            return 0  # Redisè‡ªåŠ¨å¤„ç†è¿‡æœŸ
        
        current_time = time.time()
        expired_keys = []
        
        for key, expire_time in self._memory_cache_ttl.items():
            if current_time > expire_time:
                expired_keys.append(key)
        
        for key in expired_keys:
            if key in self._memory_cache:
                del self._memory_cache[key]
            del self._memory_cache_ttl[key]
        
        if expired_keys:
            logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)}æ¡")
        
        return len(expired_keys)

# ç¼“å­˜è£…é¥°å™¨
def cached(category: str, key_func: Optional[Callable] = None, ttl: Optional[int] = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # ä½¿ç”¨å‡½æ•°åå’Œå‚æ•°ç”Ÿæˆé”®
                key_parts = [func.__name__]
                key_parts.extend(str(arg) for arg in args)
                key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_manager.get(category, cache_key)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            if result is not None:
                cache_manager.set(category, cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

# åˆ›å»ºå…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
try:
    # å°è¯•ä»é…ç½®æ–‡ä»¶è·å–Redisé…ç½®
    redis_config = getattr(config, 'REDIS_CONFIG', {
        'host': 'localhost',
        'port': 6379,
        'db': 0,
        'password': None
    })
    cache_manager = RedisCacheManager(**redis_config)
except Exception as e:
    logger.warning(f"ä½¿ç”¨é»˜è®¤Redisé…ç½®: {e}")
    cache_manager = RedisCacheManager()

def main():
    """æµ‹è¯•å’Œç®¡ç†åŠŸèƒ½"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Redisç¼“å­˜ç®¡ç†å™¨')
    parser.add_argument('command', choices=['test', 'stats', 'health', 'clear', 'preload'], 
                       help='æ“ä½œå‘½ä»¤')
    parser.add_argument('--category', type=str, help='ç¼“å­˜ç±»åˆ«')
    
    args = parser.parse_args()
    
    if args.command == 'test':
        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("ğŸ§ª æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
        
        # è®¾ç½®æµ‹è¯•æ•°æ®
        test_data = {'test': 'data', 'timestamp': datetime.now().isoformat()}
        cache_manager.set('test', 'sample', test_data, 60)
        
        # è·å–æµ‹è¯•æ•°æ®
        retrieved = cache_manager.get('test', 'sample')
        print(f"è®¾ç½®æ•°æ®: {test_data}")
        print(f"è·å–æ•°æ®: {retrieved}")
        print(f"æ•°æ®ä¸€è‡´: {test_data == retrieved}")
        
    elif args.command == 'stats':
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = cache_manager.get_stats()
        print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
            
    elif args.command == 'health':
        # å¥åº·æ£€æŸ¥
        health = cache_manager.health_check()
        print("ğŸ¥ ç¼“å­˜å¥åº·çŠ¶æ€:")
        print(json.dumps(health, indent=2, ensure_ascii=False, default=str))
        
    elif args.command == 'clear':
        # æ¸…ç©ºç¼“å­˜
        if args.category:
            deleted = cache_manager.clear_category(args.category)
            print(f"ğŸ§¹ æ¸…ç©ºç¼“å­˜ç±»åˆ« {args.category}: {deleted}æ¡")
        else:
            print("è¯·æŒ‡å®šè¦æ¸…ç©ºçš„ç¼“å­˜ç±»åˆ« --category")
            
    elif args.command == 'preload':
        # é¢„çƒ­ç¼“å­˜ç¤ºä¾‹
        def sample_preload():
            return {'sample_key': 'sample_value', 'timestamp': datetime.now().isoformat()}
        
        preload_config = {'test': sample_preload}
        results = cache_manager.preload_cache(preload_config)
        print(f"ğŸ”¥ ç¼“å­˜é¢„çƒ­ç»“æœ: {results}")

if __name__ == "__main__":
    main()