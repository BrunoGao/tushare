#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–è„šæœ¬
ä¼˜åŒ–æ€§èƒ½ï¼Œé¿å…è¶…æ—¶é—®é¢˜
"""

import os
import sys
import time
import json
import pandas as pd
import pymysql
from datetime import datetime
from typing import Dict, List, Optional
import logging
import tushare as ts

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import Config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/sector_simple_fetch.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleSectorDataFetcher:
    """ç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.ts_pro = ts.pro_api(Config.TS_TOKEN)
        self.db_config = {
            'host': Config.DB_HOST,
            'port': Config.DB_PORT,
            'user': Config.DB_USER,
            'password': Config.DB_PASSWORD,
            'database': Config.DB_NAME,
            'charset': 'utf8mb4'
        }
        self.progress_file = 'logs/sector_simple_progress.json'
        self.stats = {
            'start_time': datetime.now().isoformat(),
            'processed_tables': 0,
            'total_records': 0,
            'failed_operations': []
        }
        
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return pymysql.connect(**self.db_config)
    
    def create_tables(self):
        """åˆ›å»ºæ‰€éœ€çš„æ•°æ®è¡¨"""
        tables_sql = {
            'industry_classify_simple': """
                CREATE TABLE IF NOT EXISTS industry_classify_simple (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ts_code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç ',
                    name VARCHAR(100) COMMENT 'è‚¡ç¥¨åç§°',
                    industry VARCHAR(100) COMMENT 'æ‰€å±è¡Œä¸š',
                    area VARCHAR(50) COMMENT 'åœ°åŸŸ',
                    market VARCHAR(20) COMMENT 'å¸‚åœº',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE KEY uk_ts_code (ts_code),
                    INDEX idx_industry (industry),
                    INDEX idx_area (area)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ç®€åŒ–è¡Œä¸šåˆ†ç±»è¡¨'
            """,
            
            'concept_classify_simple': """
                CREATE TABLE IF NOT EXISTS concept_classify_simple (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    concept_code VARCHAR(20) NOT NULL COMMENT 'æ¦‚å¿µä»£ç ',
                    concept_name VARCHAR(100) NOT NULL COMMENT 'æ¦‚å¿µåç§°',
                    stock_count INT DEFAULT 0 COMMENT 'æˆåˆ†è‚¡æ•°é‡',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE KEY uk_concept_code (concept_code),
                    INDEX idx_concept_name (concept_name)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ç®€åŒ–æ¦‚å¿µåˆ†ç±»è¡¨'
            """
        }
        
        conn = self.get_db_connection()
        try:
            with conn.cursor() as cursor:
                for table_name, sql in tables_sql.items():
                    cursor.execute(sql)
                    logger.info(f"âœ… åˆ›å»º/æ£€æŸ¥è¡¨: {table_name}")
                conn.commit()
                logger.info("ğŸ¯ æ‰€æœ‰æ¿å—ç›¸å…³è¡¨åˆ›å»ºå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def fetch_industry_classify_simple(self):
        """ç®€åŒ–ç‰ˆè¡Œä¸šåˆ†ç±»æ•°æ®æŠ“å–"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–è¡Œä¸šåˆ†ç±»æ•°æ®(ç®€åŒ–ç‰ˆ)...")
        
        try:
            conn = self.get_db_connection()
            
            try:
                with conn.cursor() as cursor:
                    # ç›´æ¥ä»stock_basicè¡¨è·å–æ•°æ®
                    cursor.execute("""
                        SELECT ts_code, name, industry, area, market 
                        FROM stock_basic 
                        WHERE ts_code IS NOT NULL AND name IS NOT NULL
                    """)
                    
                    stock_data = cursor.fetchall()
                    
                    if not stock_data:
                        logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                        return 0
                    
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM industry_classify_simple")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO industry_classify_simple 
                        (ts_code, name, industry, area, market)
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    records = []
                    for row in stock_data:
                        ts_code, name, industry, area, market = row
                        records.append((
                            ts_code,
                            name,
                            industry or '',
                            area or '',
                            market or ''
                        ))
                    
                    if records:
                        cursor.executemany(insert_sql, records)
                        conn.commit()
                        
                        record_count = len(records)
                        self.stats['total_records'] += record_count
                        logger.info(f"âœ… è¡Œä¸šåˆ†ç±»æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                        return record_count
                    else:
                        logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥")
                        return 0
                    
            except Exception as e:
                logger.error(f"âŒ è¡Œä¸šåˆ†ç±»æ•°æ®å¤„ç†å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–è¡Œä¸šåˆ†ç±»æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_industry_classify_simple',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def fetch_concept_classify_simple(self):
        """ç®€åŒ–ç‰ˆæ¦‚å¿µåˆ†ç±»æ•°æ®æŠ“å–"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–æ¦‚å¿µåˆ†ç±»æ•°æ®(ç®€åŒ–ç‰ˆ)...")
        
        try:
            # åªè·å–æ¦‚å¿µåˆ—è¡¨ï¼Œä¸è·å–æˆåˆ†è‚¡è¯¦æƒ…
            df = self.ts_pro.concept()
            
            if df.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°æ¦‚å¿µåˆ†ç±»æ•°æ®")
                return 0
            
            conn = self.get_db_connection()
            try:
                with conn.cursor() as cursor:
                    # æ¸…ç©ºç°æœ‰æ•°æ®
                    cursor.execute("DELETE FROM concept_classify_simple")
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®
                    insert_sql = """
                        INSERT INTO concept_classify_simple 
                        (concept_code, concept_name, stock_count)
                        VALUES (%s, %s, %s)
                    """
                    
                    records = []
                    for _, row in df.iterrows():
                        records.append((
                            row.get('code', ''),
                            row.get('name', ''),
                            0  # æš‚æ—¶è®¾ä¸º0ï¼Œåç»­å¯ä»¥å•ç‹¬æ›´æ–°
                        ))
                    
                    cursor.executemany(insert_sql, records)
                    conn.commit()
                    
                    record_count = len(records)
                    self.stats['total_records'] += record_count
                    logger.info(f"âœ… æ¦‚å¿µåˆ†ç±»æ•°æ®æŠ“å–å®Œæˆ: {record_count}æ¡è®°å½•")
                    return record_count
                    
            except Exception as e:
                logger.error(f"âŒ æ¦‚å¿µåˆ†ç±»æ•°æ®æ’å…¥å¤±è´¥: {e}")
                conn.rollback()
                return 0
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å–æ¦‚å¿µåˆ†ç±»æ•°æ®å¤±è´¥: {e}")
            self.stats['failed_operations'].append({
                'operation': 'fetch_concept_classify_simple',
                'error': str(e),
                'time': datetime.now().isoformat()
            })
            return 0
    
    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        try:
            os.makedirs('logs', exist_ok=True)
            with open(self.progress_file, 'w') as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def run(self):
        """æ‰§è¡Œç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–"""
        logger.info("ğŸ¯ å¼€å§‹ç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–ä»»åŠ¡...")
        
        try:
            # åˆ›å»ºæ•°æ®è¡¨
            self.create_tables()
            
            # æŒ‰ä¼˜å…ˆçº§æŠ“å–æ•°æ®
            tasks = [
                ('è¡Œä¸šåˆ†ç±»(ç®€åŒ–)', self.fetch_industry_classify_simple),
                ('æ¦‚å¿µåˆ†ç±»(ç®€åŒ–)', self.fetch_concept_classify_simple)
            ]
            
            for task_name, task_func in tasks:
                logger.info(f"ğŸ“Š æ­£åœ¨å¤„ç†: {task_name}")
                
                try:
                    record_count = task_func()
                    self.stats['processed_tables'] += 1
                    logger.info(f"âœ… {task_name}å®Œæˆ: {record_count}æ¡è®°å½•")
                    
                    # ä¿å­˜è¿›åº¦
                    self.save_progress()
                    
                    # APIé™é¢‘
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ {task_name}å¤±è´¥: {e}")
                    self.stats['failed_operations'].append({
                        'task': task_name,
                        'error': str(e),
                        'time': datetime.now().isoformat()
                    })
            
            # æœ€ç»ˆç»Ÿè®¡
            self.stats['end_time'] = datetime.now().isoformat()
            self.stats['duration'] = (datetime.now() - datetime.fromisoformat(self.stats['start_time'])).total_seconds()
            
            logger.info("ğŸ‰ ç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–ä»»åŠ¡å®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†è¡¨æ•°: {self.stats['processed_tables']}")
            logger.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {self.stats['total_records']:,}")
            logger.info(f"â±ï¸ è€—æ—¶: {self.stats['duration']:.2f}ç§’")
            
            if self.stats['failed_operations']:
                logger.warning(f"âš ï¸ å¤±è´¥æ“ä½œæ•°: {len(self.stats['failed_operations'])}")
            
            self.save_progress()
            
        except Exception as e:
            logger.error(f"âŒ ç®€åŒ–ç‰ˆæ¿å—æ•°æ®æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            self.stats['error'] = str(e)
            self.save_progress()
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        fetcher = SimpleSectorDataFetcher()
        fetcher.run()
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    return 0

if __name__ == "__main__":
    exit(main())