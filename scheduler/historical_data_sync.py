#!/usr/bin/env python3
"""
å†å²æ•°æ®å…¨é‡åŒæ­¥ç³»ç»Ÿ - è·å–æ‰€æœ‰è‚¡ç¥¨è¿‡å»5å¹´å®Œæ•´æ•°æ®
åŒ…å«ï¼šæ—¥çº¿æ•°æ®ã€æŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬æŒ‡æ ‡ã€æ¶¨è·Œåœã€åœå¤ç‰Œç­‰å…¨å¥—æ•°æ®
"""

import sys
import os
import time
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import tushare as ts
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from database.db_manager import DatabaseManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/historical_sync.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class HistoricalDataSync:
    """å†å²æ•°æ®å…¨é‡åŒæ­¥å™¨ - 5å¹´å®Œæ•´æ•°æ®"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.ts_api = ts.pro_api(config.TS_TOKEN)
        self.stats_lock = Lock()
        
        # æ—¶é—´èŒƒå›´ - è¿‡å»5å¹´
        self.end_date = datetime.now().strftime('%Y%m%d')
        self.start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y%m%d')
        
        # åŒæ­¥ç»Ÿè®¡
        self.stats = {
            'total_stocks': 0,
            'completed_stocks': 0,
            'failed_stocks': 0,
            'total_records': 0,
            'start_time': None
        }
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs('logs', exist_ok=True)
        
        logger.info(f"ğŸ“… æ•°æ®åŒæ­¥æ—¶é—´èŒƒå›´: {self.start_date} - {self.end_date}")
    
    def sync_all_historical_data(self):
        """åŒæ­¥æ‰€æœ‰å†å²æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹å†å²æ•°æ®å…¨é‡åŒæ­¥ (5å¹´)")
        self.stats['start_time'] = datetime.now()
        
        try:
            # 1. æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            self._update_stock_basic()
            
            # 2. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            all_stocks = self._get_all_stocks()
            if not all_stocks:
                logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return False
            
            self.stats['total_stocks'] = len(all_stocks)
            logger.info(f"ğŸ“Š éœ€è¦åŒæ­¥ {len(all_stocks)} åªè‚¡ç¥¨çš„5å¹´å†å²æ•°æ®")
            
            # 3. åˆ†é˜¶æ®µåŒæ­¥ä¸åŒç±»å‹çš„æ•°æ®
            stages = [
                ("æ—¥çº¿åŸºç¡€æ•°æ®", self._sync_daily_data_batch, all_stocks),
                ("æŠ€æœ¯æŒ‡æ ‡æ•°æ®", self._sync_technical_indicators_batch, all_stocks),
                ("åŸºæœ¬æŒ‡æ ‡æ•°æ®", self._sync_basic_indicators_batch, None),
                ("æ¶¨è·Œåœæ•°æ®", self._sync_limit_data_batch, None),
                ("åœå¤ç‰Œæ•°æ®", self._sync_suspend_data_batch, None),
                ("åˆ†çº¢é€è‚¡æ•°æ®", self._sync_dividend_data_batch, None)
            ]
            
            for stage_name, stage_func, stock_list in stages:
                logger.info(f"ğŸ”„ å¼€å§‹é˜¶æ®µ: {stage_name}")
                try:
                    if stock_list:
                        success = stage_func(stock_list)
                    else:
                        success = stage_func()
                    
                    if success:
                        logger.info(f"âœ… {stage_name} åŒæ­¥å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ {stage_name} åŒæ­¥éƒ¨åˆ†å¤±è´¥")
                        
                except Exception as e:
                    logger.error(f"âŒ {stage_name} åŒæ­¥å¼‚å¸¸: {e}")
                
                # é˜¶æ®µé—´ä¼‘æ¯
                time.sleep(5)
            
            # 4. è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            self._print_final_stats()
            return True
            
        except Exception as e:
            logger.error(f"âŒ å†å²æ•°æ®åŒæ­¥å¼‚å¸¸: {e}")
            return False
    
    def _update_stock_basic(self):
        """æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        logger.info("ğŸ“‹ æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
        
        try:
            # è·å–æ‰€æœ‰è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_basic = self.ts_api.stock_basic(exchange='', list_status='L')
            
            if not stock_basic.empty:
                # è¿‡æ»¤Aè‚¡
                a_stocks = stock_basic[
                    (stock_basic['ts_code'].str.endswith('.SZ')) | 
                    (stock_basic['ts_code'].str.endswith('.SH'))
                ]
                
                success = self.db.insert_dataframe('stock_basic', a_stocks, replace=True)
                if success:
                    logger.info(f"âœ… è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {len(a_stocks)} åªAè‚¡")
                else:
                    logger.error("âŒ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ›´æ–°å¤±è´¥")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
    
    def _get_all_stocks(self):
        """è·å–æ‰€æœ‰Aè‚¡ä»£ç """
        try:
            sql = """
            SELECT ts_code, name, list_date, industry 
            FROM stock_basic 
            WHERE (ts_code LIKE '%%.SZ' OR ts_code LIKE '%%.SH')
            ORDER BY ts_code
            """
            df = self.db.fetch_data(sql)
            
            if not df.empty:
                return df['ts_code'].tolist()
            else:
                return []
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _sync_daily_data_batch(self, stock_list):
        """æ‰¹é‡åŒæ­¥æ—¥çº¿æ•°æ®"""
        logger.info(f"ğŸ“ˆ å¼€å§‹åŒæ­¥ {len(stock_list)} åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®")
        
        success_count = 0
        batch_size = 20
        
        for i in range(0, len(stock_list), batch_size):
            batch = stock_list[i:i + batch_size]
            
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_stock = {
                    executor.submit(self._sync_single_stock_daily, ts_code): ts_code 
                    for ts_code in batch
                }
                
                for future in tqdm(as_completed(future_to_stock), 
                                 total=len(batch), 
                                 desc=f"æ—¥çº¿æ•°æ®æ‰¹æ¬¡ {i//batch_size + 1}"):
                    ts_code = future_to_stock[future]
                    try:
                        success = future.result(timeout=60)
                        if success:
                            success_count += 1
                            with self.stats_lock:
                                self.stats['completed_stocks'] += 1
                    except Exception as e:
                        logger.error(f"âŒ {ts_code} æ—¥çº¿æ•°æ®åŒæ­¥å¤±è´¥: {e}")
                        with self.stats_lock:
                            self.stats['failed_stocks'] += 1
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            time.sleep(3)
            logger.info(f"ğŸ“Š å·²å®Œæˆ {min(i + batch_size, len(stock_list))}/{len(stock_list)} åªè‚¡ç¥¨")
        
        logger.info(f"âœ… æ—¥çº¿æ•°æ®åŒæ­¥å®Œæˆ: {success_count}/{len(stock_list)}")
        return success_count > len(stock_list) * 0.8  # 80%æˆåŠŸç‡è®¤ä¸ºæˆåŠŸ
    
    def _sync_single_stock_daily(self, ts_code):
        """åŒæ­¥å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®"""
        try:
            # è·å–5å¹´æ—¥çº¿æ•°æ®
            df = self.ts_api.daily(
                ts_code=ts_code,
                start_date=self.start_date,
                end_date=self.end_date
            )
            
            if not df.empty:
                # æ·»åŠ è®¡ç®—å­—æ®µ
                df['change'] = df['close'] - df['pre_close']
                df['change_pct'] = df['pct_chg']
                
                # æŒ‰æœˆä»½åˆ†ç»„ï¼Œæ’å…¥ä¸åŒçš„åˆ†è¡¨
                success_count = 0
                grouped = df.groupby(df['trade_date'].str[:6])
                
                for year_month, month_data in grouped:
                    table_name = f'stock_daily_{year_month}'
                    
                    # ç¡®ä¿è¡¨å­˜åœ¨
                    self._ensure_monthly_table_exists(table_name)
                    
                    success = self.db.insert_dataframe(table_name, month_data, replace=False)
                    if success:
                        success_count += 1
                        with self.stats_lock:
                            self.stats['total_records'] += len(month_data)
                
                return success_count > 0
            else:
                return True  # æ— æ•°æ®ä¹Ÿç®—æˆåŠŸ
                
        except Exception as e:
            logger.debug(f"âŒ {ts_code} æ—¥çº¿æ•°æ®è·å–å¤±è´¥: {e}")
            return False
    
    def _ensure_monthly_table_exists(self, table_name):
        """ç¡®ä¿æœˆåº¦åˆ†è¡¨å­˜åœ¨"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_sql = f"""
            SELECT COUNT(*) as count 
            FROM information_schema.tables 
            WHERE table_schema = 'ljwx_stock' AND table_name = '{table_name}'
            """
            
            result = self.db.fetch_data(check_sql)
            if result.iloc[0]['count'] == 0:
                # åˆ›å»ºæ–°çš„æœˆåº¦è¡¨
                create_sql = f"""
                CREATE TABLE {table_name} LIKE stock_daily_202507
                """
                self.db.execute_sql(create_sql)
                logger.info(f"ğŸ“… åˆ›å»ºæœˆåº¦è¡¨: {table_name}")
                
        except Exception as e:
            logger.debug(f"æœˆåº¦è¡¨æ£€æŸ¥å¤±è´¥ {table_name}: {e}")
    
    def _sync_technical_indicators_batch(self, stock_list):
        """æ‰¹é‡åŒæ­¥æŠ€æœ¯æŒ‡æ ‡"""
        logger.info("ğŸ“Š å¼€å§‹è®¡ç®—å’ŒåŒæ­¥æŠ€æœ¯æŒ‡æ ‡")
        
        try:
            # è·å–æ‰€æœ‰æ—¥çº¿æ•°æ®è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            all_daily_data = self._get_all_daily_data_for_indicators()
            
            if all_daily_data.empty:
                logger.warning("âš ï¸ æ— æ—¥çº¿æ•°æ®ç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡")
                return False
            
            logger.info(f"ğŸ“ˆ åŸºäº {len(all_daily_data)} æ¡æ—¥çº¿æ•°æ®è®¡ç®—æŠ€æœ¯æŒ‡æ ‡")
            
            # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—æŒ‡æ ‡
            all_indicators = []
            
            for ts_code, group_data in tqdm(all_daily_data.groupby('ts_code'), 
                                          desc="è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"):
                if len(group_data) < 30:  # è‡³å°‘éœ€è¦30å¤©æ•°æ®
                    continue
                
                try:
                    indicators = self._calculate_complete_technical_indicators(ts_code, group_data)
                    if not indicators.empty:
                        all_indicators.append(indicators)
                except Exception as e:
                    logger.debug(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥ {ts_code}: {e}")
                    continue
            
            if all_indicators:
                combined_indicators = pd.concat(all_indicators, ignore_index=True)
                combined_indicators['created_at'] = datetime.now()
                
                # åˆ†æ‰¹æ’å…¥é¿å…å†…å­˜é—®é¢˜
                batch_size = 10000
                success_count = 0
                
                for i in range(0, len(combined_indicators), batch_size):
                    batch_data = combined_indicators[i:i + batch_size]
                    success = self.db.insert_dataframe('technical_indicators', batch_data, replace=True)
                    if success:
                        success_count += len(batch_data)
                
                logger.info(f"âœ… æŠ€æœ¯æŒ‡æ ‡: {success_count:,} æ¡è®°å½•")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æŠ€æœ¯æŒ‡æ ‡åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _get_all_daily_data_for_indicators(self):
        """è·å–æ‰€æœ‰æ—¥çº¿æ•°æ®ç”¨äºæŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
        try:
            # è·å–è¿‘2å¹´çš„æ•°æ®ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼ˆéœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
            tables_to_query = []
            for i in range(24):  # è¿‡å»24ä¸ªæœˆ
                date = datetime.now() - timedelta(days=30*i)
                table_name = f"stock_daily_{date.strftime('%Y%m')}"
                tables_to_query.append(table_name)
            
            all_data = []
            for table_name in tables_to_query:
                try:
                    sql = f"""
                    SELECT ts_code, trade_date, open, high, low, close, vol, amount
                    FROM {table_name}
                    ORDER BY ts_code, trade_date
                    """
                    data = self.db.fetch_data(sql)
                    if not data.empty:
                        all_data.append(data)
                except:
                    continue
            
            if all_data:
                return pd.concat(all_data, ignore_index=True)
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _calculate_complete_technical_indicators(self, ts_code, data):
        """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡ï¼ŒåŒ…æ‹¬MACDå’ŒKDJ"""
        try:
            # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
            data = data.sort_values('trade_date').reset_index(drop=True)
            
            if len(data) < 30:
                return pd.DataFrame()
            
            # å‡†å¤‡ä»·æ ¼æ•°æ®
            close = data['close'].values
            high = data['high'].values
            low = data['low'].values
            volume = data['vol'].values
            
            indicators_list = []
            
            # å¯¹æ¯ä¸€å¤©è®¡ç®—æŒ‡æ ‡
            for i in range(26, len(data)):  # ä»ç¬¬26å¤©å¼€å§‹ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—MACD
                trade_date = data.iloc[i]['trade_date']
                
                # åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
                indicators = {
                    'ts_code': ts_code,
                    'trade_date': trade_date,
                    'close': close[i],
                    
                    # ç§»åŠ¨å¹³å‡çº¿
                    'ma5': np.mean(close[max(0, i-4):i+1]) if i >= 4 else None,
                    'ma10': np.mean(close[max(0, i-9):i+1]) if i >= 9 else None,
                    'ma20': np.mean(close[max(0, i-19):i+1]) if i >= 19 else None,
                    'ma60': np.mean(close[max(0, i-59):i+1]) if i >= 59 else None,
                    
                    # RSIç›¸å¯¹å¼ºå¼±æŒ‡æ ‡
                    'rsi6': self._calculate_rsi(close[max(0, i-6):i+1]) if i >= 6 else None,
                    'rsi12': self._calculate_rsi(close[max(0, i-12):i+1]) if i >= 12 else None,
                    'rsi24': self._calculate_rsi(close[max(0, i-24):i+1]) if i >= 24 else None,
                    
                    # æˆäº¤é‡æŒ‡æ ‡
                    'vol_ratio': volume[i] / np.mean(volume[max(0, i-4):i+1]) if i >= 4 and np.mean(volume[max(0, i-4):i+1]) > 0 else None,
                }
                
                # MACDæŒ‡æ ‡è®¡ç®—
                if i >= 25:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—MACD
                    macd_data = self._calculate_macd(close[:i+1])
                    indicators.update(macd_data)
                
                # KDJæŒ‡æ ‡è®¡ç®—
                if i >= 8:  # KDJéœ€è¦9å¤©æ•°æ®
                    kdj_data = self._calculate_kdj(high[max(0, i-8):i+1], 
                                                 low[max(0, i-8):i+1], 
                                                 close[max(0, i-8):i+1])
                    indicators.update(kdj_data)
                
                # å¸ƒæ—å¸¦æŒ‡æ ‡
                if i >= 19:
                    boll_data = self._calculate_bollinger_bands(close[max(0, i-19):i+1])
                    indicators.update(boll_data)
                
                indicators_list.append(indicators)
            
            return pd.DataFrame(indicators_list)
            
        except Exception as e:
            logger.debug(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥ {ts_code}: {e}")
            return pd.DataFrame()
    
    def _calculate_rsi(self, prices):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        try:
            if len(prices) < 2:
                return None
                
            deltas = np.diff(prices)
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            
            if len(gains) == 0:
                return 50
                
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            
            if avg_loss == 0:
                return 100
                
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            
            return round(rsi, 2)
        except:
            return None
    
    def _calculate_macd(self, prices):
        """è®¡ç®—MACDæŒ‡æ ‡"""
        try:
            if len(prices) < 26:
                return {'macd_dif': None, 'macd_dea': None, 'macd_macd': None}
            
            # è®¡ç®—EMA
            ema12 = self._calculate_ema(prices, 12)
            ema26 = self._calculate_ema(prices, 26)
            
            # DIFçº¿
            dif = ema12 - ema26
            
            # DEAçº¿ï¼ˆDIFçš„9æ—¥EMAï¼‰
            if len(prices) >= 34:  # 26 + 9 - 1
                dif_series = []
                for i in range(25, len(prices)):
                    temp_ema12 = self._calculate_ema(prices[:i+1], 12)
                    temp_ema26 = self._calculate_ema(prices[:i+1], 26)
                    dif_series.append(temp_ema12 - temp_ema26)
                
                dea = self._calculate_ema(np.array(dif_series), 9)
                macd = 2 * (dif - dea)
                
                return {
                    'macd_dif': round(dif, 4),
                    'macd_dea': round(dea, 4),
                    'macd_macd': round(macd, 4)
                }
            
            return {
                'macd_dif': round(dif, 4),
                'macd_dea': None,
                'macd_macd': None
            }
            
        except:
            return {'macd_dif': None, 'macd_dea': None, 'macd_macd': None}
    
    def _calculate_ema(self, prices, period):
        """è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
        try:
            alpha = 2 / (period + 1)
            ema = prices[0]
            
            for price in prices[1:]:
                ema = alpha * price + (1 - alpha) * ema
                
            return ema
        except:
            return None
    
    def _calculate_kdj(self, high, low, close):
        """è®¡ç®—KDJæŒ‡æ ‡"""
        try:
            if len(high) < 9:
                return {'kdj_k': None, 'kdj_d': None, 'kdj_j': None}
            
            # è®¡ç®—RSV
            lowest_low = np.min(low)
            highest_high = np.max(high)
            
            if highest_high == lowest_low:
                rsv = 50
            else:
                rsv = (close[-1] - lowest_low) / (highest_high - lowest_low) * 100
            
            # ç®€åŒ–çš„KDJè®¡ç®—
            k = rsv * 0.33 + 50 * 0.67  # ç®€åŒ–å¤„ç†
            d = k * 0.33 + 50 * 0.67
            j = 3 * k - 2 * d
            
            return {
                'kdj_k': round(k, 2),
                'kdj_d': round(d, 2),
                'kdj_j': round(j, 2)
            }
            
        except:
            return {'kdj_k': None, 'kdj_d': None, 'kdj_j': None}
    
    def _calculate_bollinger_bands(self, prices):
        """è®¡ç®—å¸ƒæ—å¸¦æŒ‡æ ‡"""
        try:
            if len(prices) < 20:
                return {'boll_upper': None, 'boll_mid': None, 'boll_lower': None}
            
            mid = np.mean(prices)
            std = np.std(prices)
            
            return {
                'boll_upper': round(mid + 2 * std, 2),
                'boll_mid': round(mid, 2),
                'boll_lower': round(mid - 2 * std, 2)
            }
            
        except:
            return {'boll_upper': None, 'boll_mid': None, 'boll_lower': None}
    
    def _sync_basic_indicators_batch(self):
        """æ‰¹é‡åŒæ­¥åŸºæœ¬æŒ‡æ ‡æ•°æ®ï¼ˆPEã€PBç­‰ï¼‰"""
        logger.info("ğŸ“Š å¼€å§‹åŒæ­¥å†å²åŸºæœ¬æŒ‡æ ‡æ•°æ®")
        
        try:
            # è·å–è¿‡å»1å¹´çš„åŸºæœ¬æŒ‡æ ‡æ•°æ®
            total_records = 0
            
            # æŒ‰æœˆè·å–æ•°æ®
            for i in range(60):  # è¿‡å»5å¹´ï¼Œæ¯æœˆè·å–ä¸€æ¬¡
                target_date = datetime.now() - timedelta(days=30*i)
                trade_date = target_date.strftime('%Y%m%d')
                
                try:
                    # è·å–è¯¥æ—¥æœŸçš„åŸºæœ¬æŒ‡æ ‡
                    basic_df = self.ts_api.daily_basic(trade_date=trade_date)
                    
                    if not basic_df.empty:
                        basic_df['created_at'] = datetime.now()
                        success = self.db.insert_dataframe('stock_indicators', basic_df, replace=False)
                        if success:
                            total_records += len(basic_df)
                            logger.info(f"ğŸ“… {trade_date} åŸºæœ¬æŒ‡æ ‡: {len(basic_df)} æ¡è®°å½•")
                    
                    time.sleep(0.5)  # APIé™åˆ¶
                    
                except Exception as e:
                    if "æ²¡æœ‰æ•°æ®" not in str(e):
                        logger.debug(f"åŸºæœ¬æŒ‡æ ‡ {trade_date}: {e}")
                    continue
            
            logger.info(f"âœ… å†å²åŸºæœ¬æŒ‡æ ‡åŒæ­¥å®Œæˆ: {total_records:,} æ¡è®°å½•")
            return total_records > 0
            
        except Exception as e:
            logger.error(f"å†å²åŸºæœ¬æŒ‡æ ‡åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _sync_limit_data_batch(self):
        """æ‰¹é‡åŒæ­¥æ¶¨è·Œåœå†å²æ•°æ®"""
        logger.info("ğŸ“Š å¼€å§‹åŒæ­¥å†å²æ¶¨è·Œåœæ•°æ®")
        
        try:
            total_records = 0
            
            # è·å–è¿‡å»1å¹´çš„æ¶¨è·Œåœæ•°æ®
            for i in range(365):
                trade_date = (datetime.now() - timedelta(days=i)).strftime('%Y%m%d')
                
                # è·³è¿‡å‘¨æœ«
                date_obj = datetime.strptime(trade_date, '%Y%m%d')
                if date_obj.weekday() >= 5:
                    continue
                
                try:
                    all_limit_data = []
                    
                    # è·å–æ¶¨åœè‚¡
                    limit_up = self.ts_api.limit_list(trade_date=trade_date, limit_type='U')
                    if not limit_up.empty:
                        limit_up['limit_type'] = 'UP'
                        limit_up['trade_date'] = trade_date
                        all_limit_data.append(limit_up)
                    
                    time.sleep(0.2)
                    
                    # è·å–è·Œåœè‚¡
                    limit_down = self.ts_api.limit_list(trade_date=trade_date, limit_type='D')
                    if not limit_down.empty:
                        limit_down['limit_type'] = 'DOWN'
                        limit_down['trade_date'] = trade_date
                        all_limit_data.append(limit_down)
                    
                    if all_limit_data:
                        combined_df = pd.concat(all_limit_data, ignore_index=True)
                        combined_df['created_at'] = datetime.now()
                        
                        success = self.db.insert_dataframe('t_limit_price', combined_df, replace=False)
                        if success:
                            total_records += len(combined_df)
                    
                    time.sleep(0.2)
                    
                except Exception as e:
                    if "æ²¡æœ‰æ•°æ®" not in str(e):
                        logger.debug(f"æ¶¨è·Œåœ {trade_date}: {e}")
                    continue
                
                # æ¯100å¤©è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if i % 100 == 0:
                    logger.info(f"ğŸ“… æ¶¨è·Œåœæ•°æ®è¿›åº¦: {i}/365 å¤©, å·²è·å– {total_records} æ¡è®°å½•")
            
            logger.info(f"âœ… å†å²æ¶¨è·Œåœæ•°æ®: {total_records:,} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"å†å²æ¶¨è·Œåœæ•°æ®åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _sync_suspend_data_batch(self):
        """æ‰¹é‡åŒæ­¥åœå¤ç‰Œå†å²æ•°æ®"""
        logger.info("ğŸ“Š å¼€å§‹åŒæ­¥å†å²åœå¤ç‰Œæ•°æ®")
        
        try:
            # è·å–è¿‡å»5å¹´çš„åœå¤ç‰Œæ•°æ®
            suspend_df = self.ts_api.suspend_d(
                start_date=self.start_date, 
                end_date=self.end_date
            )
            
            if not suspend_df.empty:
                suspend_df['created_at'] = datetime.now()
                success = self.db.insert_dataframe('t_suspend', suspend_df, replace=False)
                if success:
                    logger.info(f"âœ… å†å²åœå¤ç‰Œæ•°æ®: {len(suspend_df):,} æ¡è®°å½•")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"å†å²åœå¤ç‰Œæ•°æ®åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _sync_dividend_data_batch(self):
        """æ‰¹é‡åŒæ­¥åˆ†çº¢é€è‚¡å†å²æ•°æ®"""
        logger.info("ğŸ“Š å¼€å§‹åŒæ­¥å†å²åˆ†çº¢é€è‚¡æ•°æ®")
        
        try:
            # è·å–è¿‡å»5å¹´çš„åˆ†çº¢æ•°æ®
            dividend_df = self.ts_api.dividend(
                start_date=self.start_date,
                end_date=self.end_date
            )
            
            if not dividend_df.empty:
                dividend_df['created_at'] = datetime.now()
                success = self.db.insert_dataframe('t_dividend', dividend_df, replace=False)
                if success:
                    logger.info(f"âœ… å†å²åˆ†çº¢æ•°æ®: {len(dividend_df):,} æ¡è®°å½•")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"å†å²åˆ†çº¢æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ å†å²æ•°æ®å…¨é‡åŒæ­¥å®Œæˆï¼ï¼ˆ5å¹´æ•°æ®ï¼‰")
        logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {self.stats['total_stocks']}")
        logger.info(f"âœ… æˆåŠŸåŒæ­¥: {self.stats['completed_stocks']}")
        logger.info(f"âŒ å¤±è´¥æ•°é‡: {self.stats['failed_stocks']}")
        logger.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {self.stats['total_records']:,}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        logger.info(f"ğŸš€ åŒæ­¥é€Ÿåº¦: {self.stats['total_records'] / duration.total_seconds():.2f} è®°å½•/ç§’")
        
        # æ•°æ®åº“ç»Ÿè®¡
        self._print_database_summary()
    
    def _print_database_summary(self):
        """æ‰“å°æ•°æ®åº“æ±‡æ€»ä¿¡æ¯"""
        try:
            tables_info = [
                ('è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯', 'stock_basic'),
                ('æŠ€æœ¯æŒ‡æ ‡', 'technical_indicators'),
                ('åŸºæœ¬æŒ‡æ ‡', 'stock_indicators'),
                ('æ¶¨è·Œåœ', 't_limit_price'),
                ('åœå¤ç‰Œ', 't_suspend'),
                ('åˆ†çº¢é€è‚¡', 't_dividend')
            ]
            
            logger.info("ğŸ“Š æ•°æ®åº“æ±‡æ€»ç»Ÿè®¡:")
            total_records = 0
            
            for name, table in tables_info:
                try:
                    count = self.db.fetch_data(f'SELECT COUNT(*) as count FROM {table}').iloc[0]['count']
                    logger.info(f"   {name}: {count:,} æ¡è®°å½•")
                    total_records += count
                except:
                    logger.info(f"   {name}: è¡¨ä¸å­˜åœ¨æˆ–æ— æ•°æ®")
            
            # ç»Ÿè®¡æ‰€æœ‰æœˆåº¦åˆ†è¡¨
            monthly_tables = self.db.fetch_data("""
                SELECT table_name, table_rows
                FROM information_schema.tables 
                WHERE table_schema = 'ljwx_stock' 
                AND table_name LIKE 'stock_daily_%'
                ORDER BY table_name
            """)
            
            if not monthly_tables.empty:
                monthly_total = monthly_tables['table_rows'].sum()
                logger.info(f"   æ—¥çº¿æ•°æ® ({len(monthly_tables)}ä¸ªæœˆè¡¨): {monthly_total:,} æ¡è®°å½•")
                total_records += monthly_total
            
            logger.info(f"ğŸ“ˆ æ•°æ®åº“æ€»è®¡: {total_records:,} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å†å²æ•°æ®å…¨é‡åŒæ­¥ç³»ç»Ÿ (5å¹´)')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    syncer = HistoricalDataSync()
    
    try:
        if args.test:
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ - ä»…åŒæ­¥å°‘é‡æ•°æ®")
            # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹å¯ä»¥é™åˆ¶æ•°æ®èŒƒå›´
            syncer.start_date = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
        
        success = syncer.sync_all_historical_data()
        
        if success:
            logger.info("ğŸ‰ å†å²æ•°æ®å…¨é‡åŒæ­¥æˆåŠŸå®Œæˆï¼")
        else:
            logger.warning("âš ï¸ å†å²æ•°æ®åŒæ­¥æœªå®Œå…¨æˆåŠŸ")
            
    except KeyboardInterrupt:
        logger.info("â¸ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()