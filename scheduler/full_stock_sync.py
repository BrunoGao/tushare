#!/usr/bin/env python3
"""
å…¨é‡è‚¡ç¥¨æ•°æ®åŒæ­¥ç³»ç»Ÿ
æ™ºèƒ½åˆ†æ‰¹æ›´æ–°æ‰€æœ‰Aè‚¡æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè¿›åº¦ç›‘æ§
"""

import sys
import os
import time
import json
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import tushare as ts
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from database.db_manager import DatabaseManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/full_sync.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FullStockDataSync:
    """å…¨é‡è‚¡ç¥¨æ•°æ®åŒæ­¥å™¨"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.ts_api = ts.pro_api(config.TS_TOKEN)
        self.progress_file = 'logs/sync_progress.json'
        self.stats_lock = Lock()
        
        # åŒæ­¥ç»Ÿè®¡
        self.stats = {
            'total_stocks': 0,
            'completed_stocks': 0,
            'failed_stocks': 0,
            'start_time': None,
            'current_batch': 0,
            'total_batches': 0,
            'failed_list': []
        }
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs('logs', exist_ok=True)
    
    def sync_all_stocks(self, days: int = 30, batch_size: int = 50, max_workers: int = 5):
        """åŒæ­¥æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹å…¨é‡è‚¡ç¥¨æ•°æ®åŒæ­¥")
        self.stats['start_time'] = datetime.now()
        
        try:
            # 1. æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            self._update_stock_basic()
            
            # 2. è·å–æ‰€æœ‰Aè‚¡ä»£ç 
            all_stocks = self._get_all_a_stocks()
            if not all_stocks:
                logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return False
            
            self.stats['total_stocks'] = len(all_stocks)
            self.stats['total_batches'] = (len(all_stocks) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“Š æ€»å…±éœ€è¦åŒæ­¥ {len(all_stocks)} åªAè‚¡")
            logger.info(f"ğŸ“¦ åˆ† {self.stats['total_batches']} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} åª")
            
            # 3. æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„è¿›åº¦
            resume_from = self._load_progress()
            if resume_from > 0:
                logger.info(f"ğŸ”„ ä»ç¬¬ {resume_from + 1} æ‰¹ç»§ç»­åŒæ­¥")
                all_stocks = all_stocks[resume_from * batch_size:]
                self.stats['current_batch'] = resume_from
                self.stats['completed_stocks'] = resume_from * batch_size
            
            # 4. åˆ†æ‰¹å¹¶è¡ŒåŒæ­¥
            self._batch_sync_stocks(all_stocks, days, batch_size, max_workers)
            
            # 5. è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            self._print_final_stats()
            
            # 6. æ¸…ç†è¿›åº¦æ–‡ä»¶
            self._clear_progress()
            
            return True
            
        except KeyboardInterrupt:
            logger.info("â¸ï¸ ç”¨æˆ·ä¸­æ–­åŒæ­¥ï¼Œä¿å­˜è¿›åº¦...")
            self._save_progress()
            return False
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å¼‚å¸¸: {e}")
            self._save_progress()
            return False
    
    def _update_stock_basic(self):
        """æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        logger.info("ğŸ“‹ æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
        
        try:
            # è·å–æ‰€æœ‰Aè‚¡åŸºæœ¬ä¿¡æ¯
            stock_basic = self.ts_api.stock_basic(exchange='', list_status='L')
            
            if not stock_basic.empty:
                # è¿‡æ»¤Aè‚¡ï¼ˆæ’é™¤æ¸¯è‚¡ã€ç¾è‚¡ç­‰ï¼‰
                a_stocks = stock_basic[
                    (stock_basic['ts_code'].str.endswith('.SZ')) | 
                    (stock_basic['ts_code'].str.endswith('.SH'))
                ]
                
                # ä½¿ç”¨REPLACEæ¨¡å¼æ›´æ–°
                success = self.db.insert_dataframe('stock_basic', a_stocks, replace=True)
                if success:
                    logger.info(f"âœ… è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ›´æ–°å®Œæˆ: {len(a_stocks)} åªAè‚¡")
                else:
                    logger.error("âŒ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ›´æ–°å¤±è´¥")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
    
    def _get_all_a_stocks(self):
        """è·å–æ‰€æœ‰Aè‚¡ä»£ç """
        try:
            sql = """
            SELECT ts_code, name, industry, market
            FROM stock_basic 
            WHERE (ts_code LIKE '%%.SZ' OR ts_code LIKE '%%.SH')
              AND ts_code NOT LIKE '688%%'
            ORDER BY ts_code
            """
            df = self.db.fetch_data(sql)
            
            if not df.empty:
                stocks = df['ts_code'].tolist()
                logger.info(f"ğŸ“ˆ è·å–åˆ° {len(stocks)} åªAè‚¡ä»£ç ")
                return stocks
            else:
                logger.error("âŒ æ•°æ®åº“ä¸­æ— è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                return []
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _batch_sync_stocks(self, all_stocks, days, batch_size, max_workers):
        """åˆ†æ‰¹åŒæ­¥è‚¡ç¥¨æ•°æ®"""
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(all_stocks), batch_size):
            batch = all_stocks[i:i + batch_size]
            batch_num = self.stats['current_batch'] + 1
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{self.stats['total_batches']} ({len(batch)} åªè‚¡ç¥¨)")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_stock = {
                    executor.submit(self._sync_single_stock, ts_code, start_date, end_date): ts_code 
                    for ts_code in batch
                }
                
                # ä½¿ç”¨tqdmæ˜¾ç¤ºæ‰¹æ¬¡å†…è¿›åº¦
                batch_progress = tqdm(
                    as_completed(future_to_stock), 
                    total=len(batch),
                    desc=f"Batch {batch_num}",
                    unit="stock"
                )
                
                batch_success = 0
                for future in batch_progress:
                    ts_code = future_to_stock[future]
                    try:
                        success = future.result(timeout=30)
                        if success:
                            batch_success += 1
                            with self.stats_lock:
                                self.stats['completed_stocks'] += 1
                        else:
                            with self.stats_lock:
                                self.stats['failed_stocks'] += 1
                                self.stats['failed_list'].append(ts_code)
                    except Exception as e:
                        logger.error(f"âŒ {ts_code}: {e}")
                        with self.stats_lock:
                            self.stats['failed_stocks'] += 1
                            self.stats['failed_list'].append(ts_code)
                
                batch_progress.close()
            
            # æ‰¹æ¬¡å®Œæˆç»Ÿè®¡
            success_rate = (batch_success / len(batch)) * 100
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {batch_success}/{len(batch)} æˆåŠŸ ({success_rate:.1f}%)")
            
            # æ›´æ–°è¿›åº¦
            self.stats['current_batch'] = batch_num
            if batch_num % 5 == 0:  # æ¯5æ‰¹ä¿å­˜ä¸€æ¬¡è¿›åº¦
                self._save_progress()
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…APIé™åˆ¶
            if batch_num < self.stats['total_batches']:
                time.sleep(2)
    
    def _get_table_name_for_date(self, trade_date):
        """æ ¹æ®äº¤æ˜“æ—¥æœŸè·å–å¯¹åº”çš„åˆ†è¡¨åç§°"""
        if isinstance(trade_date, str):
            if len(trade_date) == 8:  # YYYYMMDDæ ¼å¼
                year_month = trade_date[:6]
            else:  # YYYY-MM-DDæ ¼å¼
                year_month = trade_date.replace('-', '')[:6]
        else:
            year_month = trade_date.strftime('%Y%m')
        
        return f'stock_daily_{year_month}'
    
    def _sync_single_stock(self, ts_code, start_date, end_date):
        """åŒæ­¥å•åªè‚¡ç¥¨æ•°æ®"""
        try:
            # è·å–æ—¥çº¿æ•°æ®
            df = self.ts_api.daily(
                ts_code=ts_code, 
                start_date=start_date, 
                end_date=end_date
            )
            
            if not df.empty:
                # æ·»åŠ è®¡ç®—å­—æ®µ
                df['change'] = df['close'] - df['pre_close']
                df['change_pct'] = df['pct_chg']
                
                # æŒ‰æœˆä»½åˆ†ç»„æ•°æ®ï¼Œåˆ†åˆ«æ’å…¥å¯¹åº”è¡¨
                success_count = 0
                grouped = df.groupby(df['trade_date'].str[:6])  # æŒ‰å¹´æœˆåˆ†ç»„
                
                for year_month, month_data in grouped:
                    table_name = f'stock_daily_{year_month}'
                    success = self.db.insert_dataframe(table_name, month_data, replace=False)
                    if success:
                        success_count += 1
                
                return success_count > 0
            else:
                logger.debug(f"ğŸ“Š {ts_code}: æ— æ•°æ®")
                return True  # æ— æ•°æ®ä¹Ÿç®—æˆåŠŸ
                
        except Exception as e:
            logger.error(f"âŒ {ts_code}: {e}")
            return False
    
    def _save_progress(self):
        """ä¿å­˜åŒæ­¥è¿›åº¦"""
        try:
            progress_data = {
                'current_batch': self.stats['current_batch'],
                'completed_stocks': self.stats['completed_stocks'],
                'failed_stocks': self.stats['failed_stocks'],
                'failed_list': self.stats['failed_list'],
                'last_save_time': datetime.now().isoformat()
            }
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: å®Œæˆ {self.stats['completed_stocks']} åªè‚¡ç¥¨")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def _load_progress(self):
        """åŠ è½½åŒæ­¥è¿›åº¦"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                
                self.stats['completed_stocks'] = progress_data.get('completed_stocks', 0)
                self.stats['failed_stocks'] = progress_data.get('failed_stocks', 0)
                self.stats['failed_list'] = progress_data.get('failed_list', [])
                
                return progress_data.get('current_batch', 0)
            else:
                return 0
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return 0
    
    def _clear_progress(self):
        """æ¸…ç†è¿›åº¦æ–‡ä»¶"""
        try:
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ å…¨é‡è‚¡ç¥¨æ•°æ®åŒæ­¥å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {self.stats['total_stocks']}")
        logger.info(f"âœ… æˆåŠŸåŒæ­¥: {self.stats['completed_stocks']}")
        logger.info(f"âŒ å¤±è´¥æ•°é‡: {self.stats['failed_stocks']}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {self.stats['completed_stocks'] / duration.total_seconds():.2f} è‚¡ç¥¨/ç§’")
        
        if self.stats['failed_list']:
            logger.warning(f"âŒ å¤±è´¥è‚¡ç¥¨åˆ—è¡¨: {self.stats['failed_list'][:10]}...")
        
        # æ•°æ®åº“ç»Ÿè®¡
        self._print_database_stats()
    
    def _print_database_stats(self):
        """æ‰“å°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–å½“å‰æœˆçš„è¡¨å
            current_table = self._get_table_name_for_date(datetime.now().strftime('%Y%m%d'))
            
            # å½“å‰æœˆç»Ÿè®¡
            current_stats = self.db.fetch_data(f"""
                SELECT 
                    COUNT(DISTINCT ts_code) as total_stocks,
                    COUNT(*) as total_records,
                    MIN(trade_date) as earliest_date,
                    MAX(trade_date) as latest_date
                FROM {current_table}
            """)
            
            if not current_stats.empty:
                stats = current_stats.iloc[0]
                logger.info("ğŸ“ˆ å½“å‰æœˆæ•°æ®åº“ç»Ÿè®¡:")
                logger.info(f"   è‚¡ç¥¨æ€»æ•°: {stats['total_stocks']}")
                logger.info(f"   è®°å½•æ€»æ•°: {stats['total_records']:,}")
                logger.info(f"   æ—¥æœŸèŒƒå›´: {stats['earliest_date']} - {stats['latest_date']}")
                
            # ä»Šæ—¥æ›´æ–°ç»Ÿè®¡ - æ£€æŸ¥æœ€æ–°äº¤æ˜“æ—¥æœŸ
            today_stats = self.db.fetch_data(f"""
                SELECT COUNT(DISTINCT ts_code) as today_updates
                FROM {current_table}
                WHERE trade_date >= DATE_SUB(CURDATE(), INTERVAL 3 DAY)
            """)
            
            if not today_stats.empty:
                today_count = today_stats.iloc[0]['today_updates']
                logger.info(f"   ä»Šæ—¥æ›´æ–°: {today_count} åªè‚¡ç¥¨")
                
        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
    
    def retry_failed_stocks(self, days: int = 30):
        """é‡è¯•å¤±è´¥çš„è‚¡ç¥¨"""
        if not self.stats['failed_list']:
            logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„è‚¡ç¥¨éœ€è¦é‡è¯•")
            return
        
        logger.info(f"ğŸ”„ é‡è¯• {len(self.stats['failed_list'])} åªå¤±è´¥çš„è‚¡ç¥¨")
        
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
        
        retry_success = 0
        for ts_code in self.stats['failed_list']:
            success = self._sync_single_stock(ts_code, start_date, end_date)
            if success:
                retry_success += 1
                logger.info(f"âœ… é‡è¯•æˆåŠŸ: {ts_code}")
            else:
                logger.error(f"âŒ é‡è¯•å¤±è´¥: {ts_code}")
            
            time.sleep(0.2)  # é¿å…APIé™åˆ¶
        
        logger.info(f"ğŸ”„ é‡è¯•å®Œæˆ: {retry_success}/{len(self.stats['failed_list'])} æˆåŠŸ")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å…¨é‡è‚¡ç¥¨æ•°æ®åŒæ­¥ç³»ç»Ÿ')
    parser.add_argument('--days', type=int, default=30, help='åŒæ­¥å¤©æ•° (é»˜è®¤30å¤©)')
    parser.add_argument('--batch-size', type=int, default=50, help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤50)')
    parser.add_argument('--workers', type=int, default=5, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤5)')
    parser.add_argument('--retry', action='store_true', help='é‡è¯•å¤±è´¥çš„è‚¡ç¥¨')
    parser.add_argument('--resume', action='store_true', help='ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­')
    
    args = parser.parse_args()
    
    syncer = FullStockDataSync()
    
    try:
        if args.retry:
            syncer.retry_failed_stocks(args.days)
        else:
            success = syncer.sync_all_stocks(
                days=args.days,
                batch_size=args.batch_size,
                max_workers=args.workers
            )
            
            if success:
                logger.info("ğŸ‰ å…¨é‡åŒæ­¥æˆåŠŸå®Œæˆï¼")
            else:
                logger.warning("âš ï¸ åŒæ­¥æœªå®Œæˆï¼Œå¯ä½¿ç”¨ --resume å‚æ•°ç»§ç»­")
                
    except KeyboardInterrupt:
        logger.info("â¸ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()