"""
Ollamaå¤§æ¨¡å‹è®­ç»ƒå™¨
ç”¨äºå¯¹lingjingwanxiang:70bæ¨¡å‹è¿›è¡Œå¾®è°ƒè®­ç»ƒ
"""

import requests
import json
import logging
from typing import Dict, List, Optional, Any
import time
import os
from datetime import datetime
import subprocess
import asyncio
import aiohttp

class OllamaTrainer:
    """Ollamaæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–Ollamaè®­ç»ƒå™¨
        
        Args:
            base_url: OllamaæœåŠ¡åœ°å€
        """
        self.base_url = base_url
        self.logger = logging.getLogger(__name__)
        
        # æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
        self._check_ollama_service()
        
    def _check_ollama_service(self):
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                self.logger.info("âœ… OllamaæœåŠ¡è¿æ¥æˆåŠŸ")
                models = response.json().get('models', [])
                self.logger.info(f"   å¯ç”¨æ¨¡å‹: {len(models)}ä¸ª")
                for model in models:
                    if 'lingjingwanxiang' in model['name']:
                        self.logger.info(f"   ğŸ¯ æ‰¾åˆ°ç›®æ ‡æ¨¡å‹: {model['name']}")
            else:
                self.logger.error(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {response.status_code}")
        except Exception as e:
            self.logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
            
    def list_models(self) -> List[Dict]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                return response.json().get('models', [])
            return []
        except Exception as e:
            self.logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def check_model_exists(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
        models = self.list_models()
        return any(model['name'] == model_name for model in models)
    
    def create_model_from_template(self, 
                                 base_model: str,
                                 new_model_name: str, 
                                 model_file_content: str) -> bool:
        """
        ä»æ¨¡æ¿åˆ›å»ºæ–°æ¨¡å‹
        
        Args:
            base_model: åŸºç¡€æ¨¡å‹åç§°
            new_model_name: æ–°æ¨¡å‹åç§°
            model_file_content: Modelfileå†…å®¹
            
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        try:
            data = {
                "name": new_model_name,
                "modelfile": model_file_content
            }
            
            response = requests.post(
                f"{self.base_url}/api/create",
                json=data,
                stream=True
            )
            
            if response.status_code == 200:
                self.logger.info(f"âœ… å¼€å§‹åˆ›å»ºæ¨¡å‹: {new_model_name}")
                
                # å¤„ç†æµå¼å“åº”
                for line in response.iter_lines():
                    if line:
                        try:
                            progress = json.loads(line)
                            if 'status' in progress:
                                self.logger.info(f"   {progress['status']}")
                        except json.JSONDecodeError:
                            continue
                
                self.logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {new_model_name}")
                return True
            else:
                self.logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def generate_response(self, 
                         model: str, 
                         prompt: str, 
                         system_message: str = None,
                         temperature: float = 0.7,
                         top_p: float = 0.9,
                         max_tokens: int = 2048) -> Optional[str]:
        """
        ç”Ÿæˆæ¨¡å‹å“åº”
        
        Args:
            model: æ¨¡å‹åç§°
            prompt: è¾“å…¥æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        try:
            messages = []
            if system_message:
                messages.append({"role": "system", "content": system_message})
            messages.append({"role": "user", "content": prompt})
            
            data = {
                "model": model,
                "messages": messages,
                "options": {
                    "temperature": temperature,
                    "top_p": top_p,
                    "num_predict": max_tokens
                },
                "stream": False
            }
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=data,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('message', {}).get('content', '')
            else:
                self.logger.error(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
            return None
    
    def fine_tune_with_training_data(self, 
                                   base_model: str,
                                   training_data_file: str,
                                   fine_tuned_model_name: str,
                                   system_prompt: str = None) -> bool:
        """
        ä½¿ç”¨è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ
        
        Args:
            base_model: åŸºç¡€æ¨¡å‹åç§°
            training_data_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            fine_tuned_model_name: å¾®è°ƒåæ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤º
            
        Returns:
            æ˜¯å¦å¾®è°ƒæˆåŠŸ
        """
        try:
            # æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶
            if not os.path.exists(training_data_file):
                self.logger.error(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {training_data_file}")
                return False
            
            # è¯»å–è®­ç»ƒæ•°æ®
            training_examples = self._load_training_data(training_data_file)
            if not training_examples:
                self.logger.error("âŒ è®­ç»ƒæ•°æ®ä¸ºç©º")
                return False
            
            self.logger.info(f"ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®: {len(training_examples)}ä¸ªæ ·æœ¬")
            
            # åˆ›å»ºModelfile
            modelfile_content = self._create_modelfile_with_training_data(
                base_model, training_examples, system_prompt
            )
            
            # åˆ›å»ºå¾®è°ƒæ¨¡å‹
            return self.create_model_from_template(
                base_model, fine_tuned_model_name, modelfile_content
            )
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®è°ƒè¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def _load_training_data(self, file_path: str) -> List[Dict]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        training_examples = []
        
        try:
            if file_path.endswith('.jsonl'):
                # JSONLæ ¼å¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            example = json.loads(line)
                            training_examples.append(example)
            elif file_path.endswith('.json'):
                # JSONæ ¼å¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        training_examples = data
                    else:
                        training_examples = [data]
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            
        return training_examples
    
    def _create_modelfile_with_training_data(self, 
                                           base_model: str, 
                                           training_examples: List[Dict],
                                           system_prompt: str = None) -> str:
        """åˆ›å»ºåŒ…å«è®­ç»ƒæ•°æ®çš„Modelfile"""
        
        # é»˜è®¤ç³»ç»Ÿæç¤º
        if not system_prompt:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æŠ•èµ„åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„é‡‘èå¸‚åœºç»éªŒã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡å’Œä»·æ ¼èµ°åŠ¿
2. æä¾›ä¸“ä¸šçš„æŠ•èµ„å»ºè®®å’Œé£é™©è¯„ä¼°
3. åŸºäºå†å²æ•°æ®é¢„æµ‹ä»·æ ¼è¶‹åŠ¿
4. ç»™å‡ºæ˜ç¡®ã€å®ç”¨çš„äº¤æ˜“å»ºè®®

è¯·å§‹ç»ˆä¿æŒå®¢è§‚ã€ç†æ€§çš„åˆ†ææ€åº¦ï¼Œå¹¶æé†’ç”¨æˆ·æ³¨æ„æŠ•èµ„é£é™©ã€‚"""
        
        # æ„å»ºModelfile
        modelfile_lines = [
            f"FROM {base_model}",
            "",
            f"SYSTEM \"\"\"{system_prompt}\"\"\"",
            ""
        ]
        
        # æ·»åŠ è®­ç»ƒæ ·æœ¬ä½œä¸ºç¤ºä¾‹ï¼ˆé€‰æ‹©å‰20ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼‰
        selected_examples = training_examples[:20]
        
        for i, example in enumerate(selected_examples):
            instruction = example.get('instruction', '')
            input_text = example.get('input', '')
            output_text = example.get('output', '')
            
            if instruction and input_text and output_text:
                # ç»„åˆæŒ‡ä»¤å’Œè¾“å…¥
                user_message = f"{instruction}\n\n{input_text}"
                
                modelfile_lines.extend([
                    f"MESSAGE user \"\"\"{user_message}\"\"\"",
                    f"MESSAGE assistant \"\"\"{output_text}\"\"\"",
                    ""
                ])
        
        # æ·»åŠ å‚æ•°è®¾ç½®
        modelfile_lines.extend([
            "PARAMETER temperature 0.7",
            "PARAMETER top_p 0.9",
            "PARAMETER top_k 40",
            "PARAMETER repeat_penalty 1.1",
            "PARAMETER num_predict 2048"
        ])
        
        return "\n".join(modelfile_lines)
    
    def test_fine_tuned_model(self, model_name: str, test_cases: List[Dict]) -> Dict[str, Any]:
        """
        æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        results = {
            'model_name': model_name,
            'test_time': datetime.now().isoformat(),
            'test_cases': [],
            'summary': {}
        }
        
        successful_tests = 0
        total_response_time = 0
        
        for i, test_case in enumerate(test_cases):
            self.logger.info(f"ğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}")
            
            start_time = time.time()
            response = self.generate_response(
                model=model_name,
                prompt=test_case.get('prompt', ''),
                system_message=test_case.get('system_message')
            )
            response_time = time.time() - start_time
            
            test_result = {
                'input': test_case.get('prompt', ''),
                'expected_type': test_case.get('expected_type', ''),
                'response': response,
                'response_time': response_time,
                'success': response is not None and len(response) > 0
            }
            
            results['test_cases'].append(test_result)
            
            if test_result['success']:
                successful_tests += 1
                total_response_time += response_time
        
        # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
        results['summary'] = {
            'total_tests': len(test_cases),
            'successful_tests': successful_tests,
            'success_rate': successful_tests / len(test_cases) if test_cases else 0,
            'average_response_time': total_response_time / successful_tests if successful_tests > 0 else 0
        }
        
        self.logger.info(f"âœ… æµ‹è¯•å®Œæˆ: æˆåŠŸç‡ {results['summary']['success_rate']:.2%}")
        
        return results
    
    def save_test_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {output_file}")


def create_test_cases() -> List[Dict]:
    """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
    return [
        {
            'prompt': """åˆ†æä»¥ä¸‹è‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡å’Œä»·æ ¼èµ°åŠ¿ï¼Œå¹¶ç»™å‡ºæŠ•èµ„å»ºè®®ï¼š

è‚¡ç¥¨ä»£ç : 000001.SZ
äº¤æ˜“æ—¥æœŸ: 2024-06-15
å¼€ç›˜ä»·: 12.50
æœ€é«˜ä»·: 12.80
æœ€ä½ä»·: 12.30
æ”¶ç›˜ä»·: 12.75
æˆäº¤é‡: 8,500,000

æŠ€æœ¯æŒ‡æ ‡:
5æ—¥å‡çº¿: 12.60
20æ—¥å‡çº¿: 12.40
RSI: 65.50
MACD: 0.0820
æ³¢åŠ¨ç‡: 0.25""",
            'expected_type': 'investment_advice'
        },
        {
            'prompt': """æ ¹æ®å†å²ä»·æ ¼æ•°æ®ï¼Œé¢„æµ‹è¯¥è‚¡ç¥¨æœªæ¥çš„ä»·æ ¼è¶‹åŠ¿ï¼š

è‚¡ç¥¨ä»£ç : 600000.SH
å½“å‰ä»·æ ¼: 15.20
5æ—¥æ¶¨è·Œå¹…: +3.50%
20æ—¥æ¶¨è·Œå¹…: +8.20%
RSI: 72.0
MACD: 0.1200""",
            'expected_type': 'price_prediction'
        },
        {
            'prompt': """è¯„ä¼°æŠ•èµ„è¯¥è‚¡ç¥¨çš„é£é™©ç­‰çº§ï¼š

è‚¡ç¥¨ä»£ç : 002001.SZ
æ³¢åŠ¨ç‡: 0.45
RSI: 85.0
æ—¥æ¶¨è·Œå¹…: +9.80%
æˆäº¤é‡æ”¾å¤§: 300%""",
            'expected_type': 'risk_assessment'
        }
    ]


def main():
    """ä¸»å‡½æ•° - å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = OllamaTrainer()
    
    # æ£€æŸ¥åŸºç¡€æ¨¡å‹
    base_model = "lingjingwanxiang:70b"
    if not trainer.check_model_exists(base_model):
        logging.error(f"âŒ åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: {base_model}")
        return
    
    logging.info(f"âœ… æ‰¾åˆ°åŸºç¡€æ¨¡å‹: {base_model}")
    
    # æŸ¥æ‰¾è®­ç»ƒæ•°æ®æ–‡ä»¶
    training_data_dir = "data/llm_training"
    if os.path.exists(training_data_dir):
        jsonl_files = [f for f in os.listdir(training_data_dir) if f.endswith('.jsonl')]
        if jsonl_files:
            # ä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ•°æ®æ–‡ä»¶
            latest_file = sorted(jsonl_files)[-1]
            training_data_file = os.path.join(training_data_dir, latest_file)
            logging.info(f"ğŸ“‚ ä½¿ç”¨è®­ç»ƒæ•°æ®: {training_data_file}")
        else:
            logging.error("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return
    else:
        logging.error("âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºå¾®è°ƒæ¨¡å‹åç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    fine_tuned_model_name = f"lingjingwanxiang-stock-{timestamp}"
    
    # æ‰§è¡Œå¾®è°ƒ
    logging.info(f"ğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹: {fine_tuned_model_name}")
    success = trainer.fine_tune_with_training_data(
        base_model=base_model,
        training_data_file=training_data_file,
        fine_tuned_model_name=fine_tuned_model_name
    )
    
    if success:
        logging.info(f"âœ… æ¨¡å‹å¾®è°ƒå®Œæˆ: {fine_tuned_model_name}")
        
        # æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
        logging.info("ğŸ§ª å¼€å§‹æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
        test_cases = create_test_cases()
        
        test_results = trainer.test_fine_tuned_model(fine_tuned_model_name, test_cases)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_file = f"data/llm_training/test_results_{timestamp}.json"
        trainer.save_test_results(test_results, results_file)
        
        # æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦
        summary = test_results['summary']
        print(f"\nğŸ¯ å¾®è°ƒæ¨¡å‹æµ‹è¯•ç»“æœ:")
        print(f"   æ¨¡å‹åç§°: {fine_tuned_model_name}")
        print(f"   æµ‹è¯•ç”¨ä¾‹: {summary['total_tests']}ä¸ª")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {summary['average_response_time']:.2f}ç§’")
        print(f"\nâœ… å®Œæ•´è®­ç»ƒæµç¨‹å®Œæˆï¼")
        
    else:
        logging.error("âŒ æ¨¡å‹å¾®è°ƒå¤±è´¥")


if __name__ == "__main__":
    main()